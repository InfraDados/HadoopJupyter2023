{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop - multi-node cluster setup \n",
    "![Hadoop](https://hadoop.apache.org/elephant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hadoop base image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker container\n",
    "\n",
    "- Ubuntu 20.04 (https://ubuntu.com/)\n",
    "- Docker (https://www.docker.com/)\n",
    "    - container based virtualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a9f89cb975f3e4bce7a3c84518d0b80f844c4474c237947b50b8c85d63beddc3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE          COMMAND       CREATED        STATUS                  PORTS     NAMES\n",
      "a9f89cb975f3   ubuntu:20.04   \"/bin/bash\"   1 second ago   Up Less than a second             hadoopimg\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "docker run -d -t --rm --name hadoopimg -h hadoopimg ubuntu:20.04\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "- Java 8 (OpenJDK) - https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\n",
    "- Other packages: ssh pdsh wget apt-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoopimg\n",
    "\n",
    "# Update package list\n",
    "apt -qq update > /install.log 2>&1\n",
    "\n",
    "# Install and configure tzdata\n",
    "DEBIAN_FRONTEND=noninteractive apt -qq install -y tzdata >> /install.log 2>&1\n",
    "ln -fs /usr/share/zoneinfo/America/Sao_Paulo /etc/localtime >> /install.log 2>&1\n",
    "dpkg-reconfigure --frontend noninteractive tzdata >> /install.log 2>&1\n",
    "\n",
    "# Install Hadoop dependencies\n",
    "apt -qq -f -y install openjdk-8-jdk ssh pdsh >> /install.log 2>&1\n",
    "\n",
    "# Install other dependencies\n",
    "apt -qq -f -y install vim wget apt-utils python3 python3-pip \\\n",
    "    ipython3 less unzip sudo net-tools >> /install.log 2>&1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Hadoop\n",
    "\n",
    "- http://hadoop.apache.org/\n",
    "- Version 3.3.6\n",
    "- Base directory: /opt\n",
    "- User/group: hadoop/hadoop\n",
    "- Package with binaries (version 3.3.6): https://hadoop.apache.org/releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoopimg\n",
    "\n",
    "# Enable rwx for all on /opt\n",
    "chmod 777 /opt\n",
    "\n",
    "# Create user/group hadoop\n",
    "useradd -m -U -s /bin/bash hadoop\n",
    "\n",
    "# Enable sudo for hadoop\n",
    "sed -i \"\\$ahadoop  ALL=(ALL) NOPASSWD:ALL\" /etc/sudoers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOPVERSION=hadoop-3.3.6\n",
    "\n",
    "# Download package\n",
    "cd ../pkgs\n",
    "wget -q -c https://dlcdn.apache.org/hadoop/common/$HADOOPVERSION/$HADOOPVERSION.tar.gz\n",
    "\n",
    "# Copy installation package to container\n",
    "docker cp $HADOOPVERSION.tar.gz hadoopimg:/opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "HADOOPVERSION=hadoop-3.3.6\n",
    "\n",
    "# Modify user/group permissions and unpack file\n",
    "sudo chown hadoop:hadoop /opt/$HADOOPVERSION.tar.gz\n",
    "tar -zxf /opt/$HADOOPVERSION.tar.gz -C /opt\n",
    "rm /opt/$HADOOPVERSION.tar.gz\n",
    "\n",
    "# Create link\n",
    "ln -s /opt/$HADOOPVERSION /opt/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment variables\n",
    "\n",
    "- Create file /opt/envvars.sh with environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
      "export PDSH_RCMD_TYPE=ssh\n",
      "\n",
      "export HADOOP_HOME=/opt/hadoop\n",
      "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
      "export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
      "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
      "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
      "export HADOOP_YARN_HOME=${HADOOP_HOME}\n",
      "\n",
      "export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/envvars.sh << EOF\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
    "export PDSH_RCMD_TYPE=ssh\n",
    "\n",
    "export HADOOP_HOME=/opt/hadoop\n",
    "export HADOOP_VERSION=3.3.6\n",
    "export HADOOP_COMMON_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_CONF_DIR=\\${HADOOP_HOME}/etc/hadoop\n",
    "export HADOOP_HDFS_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_MAPRED_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_YARN_HOME=\\${HADOOP_HOME}\n",
    "\n",
    "export PATH=\\${PATH}:\\${HADOOP_HOME}/bin:\\${HADOOP_HOME}/sbin     \n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure passwordless ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    StrictHostKeyChecking no\n",
      "    UserKnownHostsFile /dev/null\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "# Disable host key checking\n",
    "sudo tee -a /etc/ssh/ssh_config << EOF\n",
    "    StrictHostKeyChecking no\n",
    "    UserKnownHostsFile /dev/null\n",
    "EOF\n",
    "\n",
    "# Create ssh key\n",
    "ssh-keygen -q -t rsa -P \"\" -f ~/.ssh/id_rsa\n",
    "\n",
    "# Copy public key to authorized_keys file\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop configuration files\n",
    "\n",
    "- Hadoop configuration files location: \\$HADOOP\\_HOME\\/etc\\/hadoop\n",
    "- All cluster nodes contain the same files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hadoop-env.sh\n",
    "\n",
    "- Definition of environment variables used by Hadoop processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat >> /opt/hadoop/etc/hadoop/hadoop-env.sh << EOF\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### core-site.xml\n",
    "\n",
    "- Hadoop main configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/core-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/core-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://hadoop:9000</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hadoop.groups</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hadoop.hosts</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hdfs-site.xml\n",
    "\n",
    "- HDFS configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/hdfs-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>/opt/hadoop/data/nameNode</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>/opt/hadoop/data/dataNode</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>2</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.blocksize</name>\n",
    "    <value>33554432</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.hosts.exclude</name>\n",
    "    <value>/opt/hadoop/etc/hadoop/dfs.exclude</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.namenode.heartbeat.recheck-interval</name>\n",
    "    <value>10000</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### yarn-site.xml\n",
    "\n",
    "- YARN configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/yarn-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>hadoop</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "    <value>1536</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "    <value>1536</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "    <value>128</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.timeline-service.enabled</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.timeline-service.hostname</name>\n",
    "    <value>hadoop</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.system-metrics-publisher.enabled</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.log-aggregation-enable</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nm.liveness-monitor.expiry-interval-ms</name>\n",
    "    <value>10000</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mapred-site.xml\n",
    "\n",
    "- MapReduce configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/mapred-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.application.classpath</name>\n",
    "    <value>/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/mapreduce/lib/*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.app.mapreduce.am.resource.mb</name>\n",
    "    <value>512</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.map.memory.mb</name>\n",
    "    <value>256</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.reduce.memory.mb</name>\n",
    "    <value>256</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### workers\n",
    "\n",
    "- List of worker nodes (NodeManager and DataNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/workers << EOF\n",
    "hadoop1\n",
    "hadoop2\n",
    "hadoop3\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:19906e49e3fe6af61990eed834d7aabd462aac06a97cb33d442486a4c7914689\n",
      "hadoopimg\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create hadoopimg image based on hadoop container\n",
    "docker commit hadoopimg hadoopimg\n",
    "\n",
    "# Stop base container\n",
    "docker stop hadoopimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eb32245ae52436122205fa3247cff44ae7b4c6789ef5ae83823a8e155b2a74ca\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6e5189cc717145789a4b394c20541bc60fc410665127902ae66abbeccc70c7c1\n",
      "50a9def6bb153fe3668d800ea70f2ece39126f33c07505f439b47a1df7e39acd\n",
      "a59f1994b51c91e0eb87ca950e3a8a3ab7c38efee278be0f34c7801798b1d165\n",
      "CONTAINER ID   IMAGE       COMMAND       CREATED                  STATUS                  PORTS                                                                                                                                                                                                                                                                                                             NAMES\n",
      "a59f1994b51c   hadoopimg   \"/bin/bash\"   Less than a second ago   Up Less than a second   0.0.0.0:8044->8042/tcp, :::8044->8042/tcp, 0.0.0.0:9866->9864/tcp, :::9866->9864/tcp                                                                                                                                                                                                                              hadoop3\n",
      "50a9def6bb15   hadoopimg   \"/bin/bash\"   1 second ago             Up Less than a second   0.0.0.0:8043->8042/tcp, :::8043->8042/tcp, 0.0.0.0:9865->9864/tcp, :::9865->9864/tcp                                                                                                                                                                                                                              hadoop2\n",
      "6e5189cc7171   hadoopimg   \"/bin/bash\"   2 seconds ago            Up 1 second             0.0.0.0:8042->8042/tcp, :::8042->8042/tcp, 0.0.0.0:9864->9864/tcp, :::9864->9864/tcp                                                                                                                                                                                                                              hadoop1\n",
      "eb32245ae524   hadoopimg   \"/bin/bash\"   3 seconds ago            Up 2 seconds            0.0.0.0:4040->4040/tcp, :::4040->4040/tcp, 0.0.0.0:8080->8080/tcp, :::8080->8080/tcp, 0.0.0.0:8088->8088/tcp, :::8088->8088/tcp, 0.0.0.0:8188->8188/tcp, :::8188->8188/tcp, 0.0.0.0:9868->9868/tcp, :::9868->9868/tcp, 0.0.0.0:9870->9870/tcp, :::9870->9870/tcp, 0.0.0.0:19888->19888/tcp, :::19888->19888/tcp   hadoop\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# MASTER\n",
    "\n",
    "# Ports\n",
    "# 9870 - Namenode\n",
    "# 9868 - Secondary Namenode\n",
    "# 8088 - ResourceManager\n",
    "# 19888 - MapReduce Job History\n",
    "# 8188 - Timeline Service\n",
    "# 4040 - Spark Application UI\n",
    "# 8080 - Jupyter\n",
    "\n",
    "cd ..\n",
    "docker run -d -t --memory 4g --memory-swap 4g --rm --name hadoop -h hadoop -u hadoop \\\n",
    "    -v \"$(pwd)\"/pkgs:/opt/pkgs -v \"$(pwd)\"/notebooks:/opt/notebooks -v \"$(pwd)\"/datasets:/opt/datasets \\\n",
    "    -p 9870:9870 -p 9868:9868 -p 8088:8088 -p 19888:19888 -p 8188:8188 -p 4040:4040 -p 8080:8080 hadoopimg\n",
    "\n",
    "# WORKERS\n",
    "\n",
    "# Ports\n",
    "# 9864 - DataNode WebUI\n",
    "# 8042 - NodeManager WebUI\n",
    "\n",
    "# Hadoop1\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop1 -h hadoop1 -u hadoop \\\n",
    "    -p 9864:9864 -p 8042:8042 hadoopimg\n",
    "# Hadoop2\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop2 -h hadoop2 -u hadoop \\\n",
    "    -p 9865:9864 -p 8043:8042  hadoopimg\n",
    "# Hadoop3\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop3 -h hadoop3 -u hadoop \\\n",
    "    -p 9866:9864 -p 8044:8042  hadoopimg\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure hosts file on all nodes\n",
    "\n",
    "- /etc/hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.17.0.2 hadoop\n",
      "172.17.0.3 hadoop1\n",
      "172.17.0.4 hadoop2\n",
      "172.17.0.5 hadoop3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Get IPs\n",
    "M=$(docker inspect hadoop | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H1=$(docker inspect hadoop1 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H2=$(docker inspect hadoop2 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H3=$(docker inspect hadoop3 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "\n",
    "# Create hosts file\n",
    "cat > hosts << EOF  \n",
    "$M hadoop\n",
    "$H1 hadoop1\n",
    "$H2 hadoop2\n",
    "$H3 hadoop3\n",
    "EOF\n",
    "\n",
    "cat hosts\n",
    "\n",
    "# Copy to all nodes\n",
    "docker exec -i -u root hadoop sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop1 sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop2 sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop3 sh -c 'cat >> /etc/hosts' < hosts\n",
    "\n",
    "# Remove local file\n",
    "rm hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ssh server on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop1\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop2\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop3\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3\n",
    "do\n",
    "    echo $HOST\n",
    "    docker exec -u root $HOST service ssh restart\n",
    "    docker exec -u root $HOST service ssh status\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format HDFS on Namenode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: /opt/hadoop/logs does not exist. Creating.\n",
      "2023-11-23 08:38:39,686 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = hadoop/172.17.0.2\n",
      "STARTUP_MSG:   args = [-format, -force, -nonInteractive]\n",
      "STARTUP_MSG:   version = 3.3.6\n",
      "STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/opt/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/opt/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar\n",
      "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z\n",
      "STARTUP_MSG:   java = 1.8.0_382\n",
      "************************************************************/\n",
      "2023-11-23 08:38:39,702 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "2023-11-23 08:38:39,863 INFO namenode.NameNode: createNameNode [-format, -force, -nonInteractive]\n",
      "2023-11-23 08:38:40,474 INFO common.Util: Assuming 'file' scheme for path /opt/hadoop/data/nameNode in configuration.\n",
      "2023-11-23 08:38:40,475 INFO common.Util: Assuming 'file' scheme for path /opt/hadoop/data/nameNode in configuration.\n",
      "2023-11-23 08:38:40,487 INFO namenode.NameNode: Formatting using clusterid: CID-7f70f383-a97f-414c-a761-d4fed8dd0332\n",
      "2023-11-23 08:38:40,540 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "2023-11-23 08:38:40,572 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "2023-11-23 08:38:40,574 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "2023-11-23 08:38:40,575 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "2023-11-23 08:38:40,581 INFO namenode.FSNamesystem: fsOwner                = hadoop (auth:SIMPLE)\n",
      "2023-11-23 08:38:40,581 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
      "2023-11-23 08:38:40,581 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
      "2023-11-23 08:38:40,581 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
      "2023-11-23 08:38:40,582 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "2023-11-23 08:38:40,645 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "2023-11-23 08:38:40,786 ERROR blockmanagement.DatanodeManager: error reading hosts files: \n",
      "java.nio.file.NoSuchFileException: /opt/hadoop/etc/hadoop/dfs.exclude\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)\n",
      "\tat java.nio.file.Files.newByteChannel(Files.java:361)\n",
      "\tat java.nio.file.Files.newByteChannel(Files.java:407)\n",
      "\tat java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.newInputStream(Files.java:152)\n",
      "\tat org.apache.hadoop.util.HostsFileReader.readFileToSet(HostsFileReader.java:80)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.readFile(HostFileManager.java:78)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.refresh(HostFileManager.java:157)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.refresh(HostFileManager.java:70)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:284)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:479)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:904)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:825)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1269)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1739)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1847)\n",
      "2023-11-23 08:38:40,794 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
      "2023-11-23 08:38:40,794 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "2023-11-23 08:38:40,797 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "2023-11-23 08:38:40,798 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Nov 23 08:38:40\n",
      "2023-11-23 08:38:40,799 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "2023-11-23 08:38:40,799 INFO util.GSet: VM type       = 64-bit\n",
      "2023-11-23 08:38:40,800 INFO util.GSet: 2.0% max memory 910.5 MB = 18.2 MB\n",
      "2023-11-23 08:38:40,800 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "2023-11-23 08:38:40,811 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
      "2023-11-23 08:38:40,811 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
      "2023-11-23 08:38:40,819 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
      "2023-11-23 08:38:40,819 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "2023-11-23 08:38:40,820 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "2023-11-23 08:38:40,821 INFO blockmanagement.BlockManager: defaultReplication         = 2\n",
      "2023-11-23 08:38:40,821 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "2023-11-23 08:38:40,821 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "2023-11-23 08:38:40,821 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "2023-11-23 08:38:40,821 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
      "2023-11-23 08:38:40,821 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "2023-11-23 08:38:40,821 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "2023-11-23 08:38:40,872 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
      "2023-11-23 08:38:40,872 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
      "2023-11-23 08:38:40,872 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
      "2023-11-23 08:38:40,872 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
      "2023-11-23 08:38:40,888 INFO util.GSet: Computing capacity for map INodeMap\n",
      "2023-11-23 08:38:40,888 INFO util.GSet: VM type       = 64-bit\n",
      "2023-11-23 08:38:40,889 INFO util.GSet: 1.0% max memory 910.5 MB = 9.1 MB\n",
      "2023-11-23 08:38:40,889 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "2023-11-23 08:38:40,889 INFO namenode.FSDirectory: ACLs enabled? true\n",
      "2023-11-23 08:38:40,889 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
      "2023-11-23 08:38:40,889 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "2023-11-23 08:38:40,890 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "2023-11-23 08:38:40,902 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
      "2023-11-23 08:38:40,904 INFO snapshot.SnapshotManager: SkipList is disabled\n",
      "2023-11-23 08:38:40,909 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "2023-11-23 08:38:40,909 INFO util.GSet: VM type       = 64-bit\n",
      "2023-11-23 08:38:40,909 INFO util.GSet: 0.25% max memory 910.5 MB = 2.3 MB\n",
      "2023-11-23 08:38:40,910 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
      "2023-11-23 08:38:40,921 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "2023-11-23 08:38:40,921 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "2023-11-23 08:38:40,921 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "2023-11-23 08:38:40,932 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "2023-11-23 08:38:40,933 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "2023-11-23 08:38:40,941 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "2023-11-23 08:38:40,941 INFO util.GSet: VM type       = 64-bit\n",
      "2023-11-23 08:38:40,941 INFO util.GSet: 0.029999999329447746% max memory 910.5 MB = 279.7 KB\n",
      "2023-11-23 08:38:40,941 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
      "2023-11-23 08:38:40,991 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1859505927-172.17.0.2-1700739520971\n",
      "2023-11-23 08:38:41,041 INFO common.Storage: Storage directory /opt/hadoop-3.3.6/data/nameNode has been successfully formatted.\n",
      "2023-11-23 08:38:41,267 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/hadoop-3.3.6/data/nameNode/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "2023-11-23 08:38:41,455 INFO namenode.FSImageFormatProtobuf: Image file /opt/hadoop-3.3.6/data/nameNode/current/fsimage.ckpt_0000000000000000000 of size 398 bytes saved in 0 seconds .\n",
      "2023-11-23 08:38:41,499 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "2023-11-23 08:38:41,523 INFO namenode.FSNamesystem: Stopping services started for active state\n",
      "2023-11-23 08:38:41,524 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
      "2023-11-23 08:38:41,527 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
      "2023-11-23 08:38:41,527 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at hadoop/172.17.0.2\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs namenode -format -force -nonInteractive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Hadoop daemons\n",
    "\n",
    "- manual execution: ```hdfs --daemon start (namenode|datanode)``` and ```yarn --daemon start (resourcemanager|nodemanager)```\n",
    "- auxilliary scripts to run all processes on the cluster: start-dfs.sh (HDFS) and start-yarn.sh (YARN)\n",
    "- some services still need to be executed manually (timelineserver, historyserver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "Starting datanodes\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "hadoop2: WARNING: /opt/hadoop-3.3.6/logs does not exist. Creating.\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop3: WARNING: /opt/hadoop-3.3.6/logs does not exist. Creating.\n",
      "hadoop1: WARNING: /opt/hadoop-3.3.6/logs does not exist. Creating.\n",
      "Starting secondary namenodes [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "Starting resourcemanager\n",
      "Starting nodemanagers\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# HDFS\n",
    "start-dfs.sh\n",
    "\n",
    "# YARN\n",
    "start-yarn.sh\n",
    "\n",
    "# timelineserver\n",
    "yarn --daemon start timelineserver\n",
    "\n",
    "# historyserver\n",
    "mapred --daemon start historyserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      "721 ApplicationHistoryServer\n",
      "225 NameNode\n",
      "418 SecondaryNameNode\n",
      "595 ResourceManager\n",
      "1028 Jps\n",
      "781 JobHistoryServer\n",
      "hadoop1\n",
      "197 NodeManager\n",
      "294 Jps\n",
      "92 DataNode\n",
      "hadoop2\n",
      "293 Jps\n",
      "197 NodeManager\n",
      "92 DataNode\n",
      "hadoop3\n",
      "196 NodeManager\n",
      "293 Jps\n",
      "91 DataNode\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Listing all processes\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3; do\n",
    "    echo $HOST\n",
    "    docker exec $HOST jps\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HDFS directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp': File exists\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/hadoop\n",
    "hdfs dfs -chown hadoop:hadoop /user/hadoop\n",
    "hdfs dfs -mkdir /tmp\n",
    "hdfs dfs -chmod 777 /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and run Jupyter on master node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2023-11-23 08:50:35.211 ServerApp] ServerApp.token config is deprecated in 2.0. Use IdentityProvider.token.\n",
      "[I 2023-11-23 08:50:35.222 ServerApp] Package jupyterlab took 0.0000s to import\n",
      "[I 2023-11-23 08:50:35.231 ServerApp] Package jupyter_lsp took 0.0083s to import\n",
      "[W 2023-11-23 08:50:35.231 ServerApp] A `_jupyter_server_extension_points` function was not found in jupyter_lsp. Instead, a `_jupyter_server_extension_paths` function was found and will be used for now. This function name will be deprecated in future releases of Jupyter Server.\n",
      "[I 2023-11-23 08:50:35.235 ServerApp] Package jupyter_server_terminals took 0.0034s to import\n",
      "[I 2023-11-23 08:50:35.236 ServerApp] Package notebook_shim took 0.0000s to import\n",
      "[W 2023-11-23 08:50:35.236 ServerApp] A `_jupyter_server_extension_points` function was not found in notebook_shim. Instead, a `_jupyter_server_extension_paths` function was found and will be used for now. This function name will be deprecated in future releases of Jupyter Server.\n",
      "[I 2023-11-23 08:50:35.237 ServerApp] jupyter_lsp | extension was successfully linked.\n",
      "[I 2023-11-23 08:50:35.243 ServerApp] jupyter_server_terminals | extension was successfully linked.\n",
      "[I 2023-11-23 08:50:35.253 ServerApp] jupyterlab | extension was successfully linked.\n",
      "[I 2023-11-23 08:50:35.418 ServerApp] notebook_shim | extension was successfully linked.\n",
      "[W 2023-11-23 08:50:35.457 ServerApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.\n",
      "[I 2023-11-23 08:50:35.458 ServerApp] notebook_shim | extension was successfully loaded.\n",
      "[I 2023-11-23 08:50:35.460 ServerApp] jupyter_lsp | extension was successfully loaded.\n",
      "[I 2023-11-23 08:50:35.461 ServerApp] jupyter_server_terminals | extension was successfully loaded.\n",
      "[I 2023-11-23 08:50:35.463 LabApp] JupyterLab extension loaded from /home/hadoop/.local/lib/python3.8/site-packages/jupyterlab\n",
      "[I 2023-11-23 08:50:35.463 LabApp] JupyterLab application directory is /home/hadoop/.local/share/jupyter/lab\n",
      "[I 2023-11-23 08:50:35.464 LabApp] Extension Manager is 'pypi'.\n",
      "[I 2023-11-23 08:50:35.468 ServerApp] jupyterlab | extension was successfully loaded.\n",
      "[I 2023-11-23 08:50:35.469 ServerApp] Serving notebooks from local directory: /\n",
      "[I 2023-11-23 08:50:35.469 ServerApp] Jupyter Server 2.10.1 is running at:\n",
      "[I 2023-11-23 08:50:35.469 ServerApp] http://172.17.0.2:8080/lab\n",
      "[I 2023-11-23 08:50:35.469 ServerApp]     http://127.0.0.1:8080/lab\n",
      "[I 2023-11-23 08:50:35.469 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
      "[I 2023-11-23 08:50:35.496 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# pip3 -q install notebook\n",
    "pip3 -q install pip\n",
    "pip3 -q install traitlets==5.6.0\n",
    "pip3 -q install jupyterlab\n",
    "\n",
    "IP=$(ifconfig eth0 | grep inet | awk '{ print $2 }')\n",
    "\n",
    "cd /opt\n",
    "\n",
    "export SHELL=/bin/bash\n",
    "# nohup /home/hadoop/.local/bin/jupyter-notebook --ip=$IP --port=8080 --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.notebook_dir='/' --no-browser &\n",
    "nohup /home/hadoop/.local/bin/jupyter-lab --ip=$IP --port=8080 --notebook-dir='/' --ServerApp.token='' --ServerApp.password='' --no-browser &\n",
    "\n",
    "echo $! > /tmp/jupyter.pid\n",
    "\n",
    "# To kill\n",
    "# kill $(cat /tmp/jupyter.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access web interfaces\n",
    "\n",
    "- Jupyterlab\n",
    "    - http://localhost:8080\n",
    "- Master - hadoop\n",
    "    - Resource Manager: http://localhost:8088\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "    - MapReduce Job History: http://localhost:19888\n",
    "    - Timeline Service: http://localhost:8188\n",
    "- Workers\n",
    "    - hadoop1\n",
    "        - NodeManager: http://localhost:8042\n",
    "        - DataNode: http://localhost:9864\n",
    "    - hadoop2\n",
    "        - NodeManager: http://localhost:8043\n",
    "        - DataNode: http://localhost:9865\n",
    "    - hadoop3\n",
    "        - NodeManager: http://localhost:8044\n",
    "        - DataNode: http://localhost:9866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run mapreduce Pi example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Maps  = 6\n",
      "Samples per Map = 10000\n",
      "Wrote input for Map #0\n",
      "Wrote input for Map #1\n",
      "Wrote input for Map #2\n",
      "Wrote input for Map #3\n",
      "Wrote input for Map #4\n",
      "Wrote input for Map #5\n",
      "Starting Job\n",
      "2023-11-23 08:56:20,836 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2023-11-23 08:56:21,154 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2023-11-23 08:56:21,451 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1700739592673_0001\n",
      "2023-11-23 08:56:22,224 INFO input.FileInputFormat: Total input files to process : 6\n",
      "2023-11-23 08:56:22,331 INFO mapreduce.JobSubmitter: number of splits:6\n",
      "2023-11-23 08:56:22,518 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1700739592673_0001\n",
      "2023-11-23 08:56:22,518 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-11-23 08:56:22,712 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-11-23 08:56:22,713 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-11-23 08:56:23,292 INFO impl.YarnClientImpl: Submitted application application_1700739592673_0001\n",
      "2023-11-23 08:56:23,485 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1700739592673_0001/\n",
      "2023-11-23 08:56:23,485 INFO mapreduce.Job: Running job: job_1700739592673_0001\n",
      "2023-11-23 08:56:33,634 INFO mapreduce.Job: Job job_1700739592673_0001 running in uber mode : false\n",
      "2023-11-23 08:56:33,635 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-11-23 08:56:50,836 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2023-11-23 08:56:54,918 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2023-11-23 08:56:55,924 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "HADOOP_VERSION=3.3.6\n",
    "\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar pi 6 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHUTDOWN PROCEDURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "kill $(cat /tmp/jupyter.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Hadoop daemons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "Stopping datanodes\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "Stopping secondary namenodes [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "2023-03-27 13:53:53,388 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping nodemanagers\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "Stopping resourcemanager\n",
      "WARNING: resourcemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "stop-dfs.sh\n",
    "stop-yarn.sh\n",
    "yarn --daemon stop timelineserver\n",
    "mapred --daemon stop historyserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Docker containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      "hadoop1\n",
      "hadoop2\n",
      "hadoop3\n",
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3; do\n",
    "    docker stop $HOST\n",
    "done\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
