{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop - multi-node cluster setup \n",
    "![Hadoop](https://hadoop.apache.org/elephant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hadoop base image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker container\n",
    "\n",
    "- Ubuntu 20.04 (https://ubuntu.com/)\n",
    "- Docker (https://www.docker.com/)\n",
    "    - container based virtualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0bf7845958fa884352ffc55b1425428b4d526b22dedef05e0c2cf0d653c46e9d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE          COMMAND       CREATED        STATUS                  PORTS     NAMES\n",
      "0bf7845958fa   ubuntu:20.04   \"/bin/bash\"   1 second ago   Up Less than a second             hadoopimg\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "docker run -d -t --rm --name hadoopimg -h hadoopimg ubuntu:20.04\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "- Java 8 (OpenJDK) - https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\n",
    "- Other packages: ssh pdsh wget apt-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoopimg\n",
    "\n",
    "# Update package list\n",
    "apt -qq update > /install.log 2>&1\n",
    "\n",
    "# Install and configure tzdata\n",
    "DEBIAN_FRONTEND=noninteractive apt -qq install -y tzdata >> /install.log 2>&1\n",
    "ln -fs /usr/share/zoneinfo/America/Sao_Paulo /etc/localtime >> /install.log 2>&1\n",
    "dpkg-reconfigure --frontend noninteractive tzdata >> /install.log 2>&1\n",
    "\n",
    "# Install Hadoop dependencies\n",
    "apt -qq -f -y install openjdk-8-jdk ssh pdsh >> /install.log 2>&1\n",
    "\n",
    "# Install other dependencies\n",
    "apt -qq -f -y install vim wget apt-utils python3 python3-pip \\\n",
    "    ipython3 less unzip sudo net-tools >> /install.log 2>&1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Hadoop\n",
    "\n",
    "- http://hadoop.apache.org/\n",
    "- Version 3.3.6\n",
    "- Base directory: /opt\n",
    "- User/group: hadoop/hadoop\n",
    "- Package with binaries (version 3.3.6): https://hadoop.apache.org/releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoopimg\n",
    "\n",
    "# Enable rwx for all on /opt\n",
    "chmod 777 /opt\n",
    "\n",
    "# Create user/group hadoop\n",
    "useradd -m -U -s /bin/bash hadoop\n",
    "\n",
    "# Enable sudo for hadoop\n",
    "sed -i \"\\$ahadoop  ALL=(ALL) NOPASSWD:ALL\" /etc/sudoers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_VERSION=3.3.6\n",
    "\n",
    "# Download package\n",
    "cd ../pkgs\n",
    "wget -q -c https://dlcdn.apache.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\n",
    "\n",
    "# Copy installation package to container\n",
    "docker cp hadoop-$HADOOP_VERSION.tar.gz hadoopimg:/opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "HADOOP_VERSION=3.3.6\n",
    "\n",
    "# Modify user/group permissions and unpack file\n",
    "sudo chown hadoop:hadoop /opt/hadoop-$HADOOP_VERSION.tar.gz\n",
    "tar -zxf /opt/hadoop-$HADOOP_VERSION.tar.gz -C /opt\n",
    "rm /opt/hadoop-$HADOOP_VERSION.tar.gz\n",
    "\n",
    "# Create link\n",
    "ln -s /opt/hadoop-$HADOOP_VERSION /opt/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment variables\n",
    "\n",
    "- Create file /opt/envvars.sh with environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
      "export PDSH_RCMD_TYPE=ssh\n",
      "\n",
      "export HADOOP_HOME=/opt/hadoop\n",
      "export HADOOP_VERSION=3.3.6\n",
      "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
      "export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
      "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
      "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
      "export HADOOP_YARN_HOME=${HADOOP_HOME}\n",
      "\n",
      "export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/envvars.sh << EOF\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
    "export PDSH_RCMD_TYPE=ssh\n",
    "\n",
    "export HADOOP_HOME=/opt/hadoop\n",
    "export HADOOP_VERSION=3.3.6\n",
    "export HADOOP_COMMON_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_CONF_DIR=\\${HADOOP_HOME}/etc/hadoop\n",
    "export HADOOP_HDFS_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_MAPRED_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_YARN_HOME=\\${HADOOP_HOME}\n",
    "\n",
    "export PATH=\\${PATH}:\\${HADOOP_HOME}/bin:\\${HADOOP_HOME}/sbin     \n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure passwordless ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    StrictHostKeyChecking no\n",
      "    UserKnownHostsFile /dev/null\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "# Disable host key checking\n",
    "sudo tee -a /etc/ssh/ssh_config << EOF\n",
    "    StrictHostKeyChecking no\n",
    "    UserKnownHostsFile /dev/null\n",
    "EOF\n",
    "\n",
    "# Create ssh key\n",
    "ssh-keygen -q -t rsa -P \"\" -f ~/.ssh/id_rsa\n",
    "\n",
    "# Copy public key to authorized_keys file\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop configuration files\n",
    "\n",
    "- Hadoop configuration files location: \\$HADOOP\\_HOME\\/etc\\/hadoop\n",
    "- All cluster nodes contain the same files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hadoop-env.sh\n",
    "\n",
    "- Definition of environment variables used by Hadoop processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat >> /opt/hadoop/etc/hadoop/hadoop-env.sh << EOF\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### core-site.xml\n",
    "\n",
    "- Hadoop main configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/core-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/core-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://hadoop:9000</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hadoop.groups</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hadoop.hosts</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hdfs-site.xml\n",
    "\n",
    "- HDFS configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/hdfs-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>/opt/hadoop/data/nameNode</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>/opt/hadoop/data/dataNode</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>2</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.blocksize</name>\n",
    "    <value>33554432</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.hosts.exclude</name>\n",
    "    <value>/opt/hadoop/etc/hadoop/dfs.exclude</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.namenode.heartbeat.recheck-interval</name>\n",
    "    <value>10000</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### yarn-site.xml\n",
    "\n",
    "- YARN configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/yarn-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>hadoop</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "    <value>1536</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "    <value>1536</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "    <value>128</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.timeline-service.enabled</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.timeline-service.hostname</name>\n",
    "    <value>hadoop</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.system-metrics-publisher.enabled</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.log-aggregation-enable</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nm.liveness-monitor.expiry-interval-ms</name>\n",
    "    <value>10000</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mapred-site.xml\n",
    "\n",
    "- MapReduce configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/mapred-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.application.classpath</name>\n",
    "    <value>/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/mapreduce/lib/*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.app.mapreduce.am.resource.mb</name>\n",
    "    <value>512</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.map.memory.mb</name>\n",
    "    <value>256</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.reduce.memory.mb</name>\n",
    "    <value>256</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### workers\n",
    "\n",
    "- List of worker nodes (NodeManager and DataNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/workers << EOF\n",
    "hadoop1\n",
    "hadoop2\n",
    "hadoop3\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:c32c48b8f4a0c2faa0c89438496f25dfd900c67293f46c5bcea13c695f569b39\n",
      "hadoopimg\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create hadoopimg image based on hadoop container\n",
    "docker commit hadoopimg hadoopimg\n",
    "\n",
    "# Stop base container\n",
    "docker stop hadoopimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b14776f2d9980daef4737fcbec7832964f1297908720dff3c472bca5f5c99497\n",
      "b68e4737db15a4f1098e0b28c28d57e78d958d5547305f290d73c6752f57e929\n",
      "83fe481e7d948991918b96e11e750885f6e7337711cc32e4b90966d1623ce39a\n",
      "44d8e5b2cab1115b750d587574a48849db1e27838830799cfd8c0c88a0bfe9e7\n",
      "CONTAINER ID   IMAGE       COMMAND       CREATED         STATUS                  PORTS                                                                                                                                                                                                                                                                                                             NAMES\n",
      "44d8e5b2cab1   hadoopimg   \"/bin/bash\"   1 second ago    Up Less than a second   0.0.0.0:8044->8042/tcp, :::8044->8042/tcp, 0.0.0.0:9866->9864/tcp, :::9866->9864/tcp                                                                                                                                                                                                                              hadoop3\n",
      "83fe481e7d94   hadoopimg   \"/bin/bash\"   2 seconds ago   Up Less than a second   0.0.0.0:8043->8042/tcp, :::8043->8042/tcp, 0.0.0.0:9865->9864/tcp, :::9865->9864/tcp                                                                                                                                                                                                                              hadoop2\n",
      "b68e4737db15   hadoopimg   \"/bin/bash\"   2 seconds ago   Up 1 second             0.0.0.0:8042->8042/tcp, :::8042->8042/tcp, 0.0.0.0:9864->9864/tcp, :::9864->9864/tcp                                                                                                                                                                                                                              hadoop1\n",
      "b14776f2d998   hadoopimg   \"/bin/bash\"   3 seconds ago   Up 1 second             0.0.0.0:4040->4040/tcp, :::4040->4040/tcp, 0.0.0.0:8080->8080/tcp, :::8080->8080/tcp, 0.0.0.0:8088->8088/tcp, :::8088->8088/tcp, 0.0.0.0:8188->8188/tcp, :::8188->8188/tcp, 0.0.0.0:9868->9868/tcp, :::9868->9868/tcp, 0.0.0.0:9870->9870/tcp, :::9870->9870/tcp, 0.0.0.0:19888->19888/tcp, :::19888->19888/tcp   hadoop\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# MASTER\n",
    "\n",
    "# Ports\n",
    "# 9870 - Namenode\n",
    "# 9868 - Secondary Namenode\n",
    "# 8088 - ResourceManager\n",
    "# 19888 - MapReduce Job History\n",
    "# 8188 - Timeline Service\n",
    "# 4040 - Spark Application UI\n",
    "# 8080 - Jupyter\n",
    "\n",
    "cd ..\n",
    "docker run -d -t --memory 4g --memory-swap 4g --rm --name hadoop -h hadoop -u hadoop \\\n",
    "    -v \"$(pwd)\"/pkgs:/opt/pkgs -v \"$(pwd)\"/notebooks:/opt/notebooks -v \"$(pwd)\"/datasets:/opt/datasets \\\n",
    "    -p 9870:9870 -p 9868:9868 -p 8088:8088 -p 19888:19888 -p 8188:8188 -p 4040:4040 -p 8080:8080 hadoopimg\n",
    "\n",
    "# WORKERS\n",
    "\n",
    "# Ports\n",
    "# 9864 - DataNode WebUI\n",
    "# 8042 - NodeManager WebUI\n",
    "\n",
    "# Hadoop1\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop1 -h hadoop1 -u hadoop \\\n",
    "    -p 9864:9864 -p 8042:8042 hadoopimg\n",
    "# Hadoop2\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop2 -h hadoop2 -u hadoop \\\n",
    "    -p 9865:9864 -p 8043:8042  hadoopimg\n",
    "# Hadoop3\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop3 -h hadoop3 -u hadoop \\\n",
    "    -p 9866:9864 -p 8044:8042  hadoopimg\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure hosts file on all nodes\n",
    "\n",
    "- /etc/hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.17.0.2 hadoop\n",
      "172.17.0.3 hadoop1\n",
      "172.17.0.4 hadoop2\n",
      "172.17.0.5 hadoop3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Get IPs\n",
    "M=$(docker inspect hadoop | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H1=$(docker inspect hadoop1 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H2=$(docker inspect hadoop2 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H3=$(docker inspect hadoop3 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "\n",
    "# Create hosts file\n",
    "cat > hosts << EOF  \n",
    "$M hadoop\n",
    "$H1 hadoop1\n",
    "$H2 hadoop2\n",
    "$H3 hadoop3\n",
    "EOF\n",
    "\n",
    "cat hosts\n",
    "\n",
    "# Copy to all nodes\n",
    "docker exec -i -u root hadoop sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop1 sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop2 sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop3 sh -c 'cat >> /etc/hosts' < hosts\n",
    "\n",
    "# Remove local file\n",
    "rm hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ssh server on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop1\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop2\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop3\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3\n",
    "do\n",
    "    echo $HOST\n",
    "    docker exec -u root $HOST service ssh restart\n",
    "    docker exec -u root $HOST service ssh status\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format HDFS on Namenode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: /opt/hadoop/logs does not exist. Creating.\n",
      "2023-11-23 09:11:36,162 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = hadoop/172.17.0.2\n",
      "STARTUP_MSG:   args = [-format, -force, -nonInteractive]\n",
      "STARTUP_MSG:   version = 3.3.6\n",
      "STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/opt/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/opt/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar\n",
      "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z\n",
      "STARTUP_MSG:   java = 1.8.0_382\n",
      "************************************************************/\n",
      "2023-11-23 09:11:36,169 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "2023-11-23 09:11:36,249 INFO namenode.NameNode: createNameNode [-format, -force, -nonInteractive]\n",
      "2023-11-23 09:11:36,558 INFO common.Util: Assuming 'file' scheme for path /opt/hadoop/data/nameNode in configuration.\n",
      "2023-11-23 09:11:36,558 INFO common.Util: Assuming 'file' scheme for path /opt/hadoop/data/nameNode in configuration.\n",
      "2023-11-23 09:11:36,562 INFO namenode.NameNode: Formatting using clusterid: CID-3d2938f2-2683-43c4-b8fb-46e50a126e03\n",
      "2023-11-23 09:11:36,592 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "2023-11-23 09:11:36,614 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "2023-11-23 09:11:36,615 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "2023-11-23 09:11:36,616 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "2023-11-23 09:11:36,620 INFO namenode.FSNamesystem: fsOwner                = hadoop (auth:SIMPLE)\n",
      "2023-11-23 09:11:36,620 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
      "2023-11-23 09:11:36,620 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
      "2023-11-23 09:11:36,620 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
      "2023-11-23 09:11:36,620 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "2023-11-23 09:11:36,653 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "2023-11-23 09:11:36,769 ERROR blockmanagement.DatanodeManager: error reading hosts files: \n",
      "java.nio.file.NoSuchFileException: /opt/hadoop/etc/hadoop/dfs.exclude\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)\n",
      "\tat java.nio.file.Files.newByteChannel(Files.java:361)\n",
      "\tat java.nio.file.Files.newByteChannel(Files.java:407)\n",
      "\tat java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.newInputStream(Files.java:152)\n",
      "\tat org.apache.hadoop.util.HostsFileReader.readFileToSet(HostsFileReader.java:80)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.readFile(HostFileManager.java:78)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.refresh(HostFileManager.java:157)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.refresh(HostFileManager.java:70)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:284)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:479)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:904)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:825)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1269)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1739)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1847)\n",
      "2023-11-23 09:11:36,775 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
      "2023-11-23 09:11:36,775 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "2023-11-23 09:11:36,778 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "2023-11-23 09:11:36,778 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Nov 23 09:11:36\n",
      "2023-11-23 09:11:36,779 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "2023-11-23 09:11:36,779 INFO util.GSet: VM type       = 64-bit\n",
      "2023-11-23 09:11:36,780 INFO util.GSet: 2.0% max memory 910.5 MB = 18.2 MB\n",
      "2023-11-23 09:11:36,780 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "2023-11-23 09:11:36,790 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
      "2023-11-23 09:11:36,790 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
      "2023-11-23 09:11:36,795 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
      "2023-11-23 09:11:36,795 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "2023-11-23 09:11:36,795 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "2023-11-23 09:11:36,796 INFO blockmanagement.BlockManager: defaultReplication         = 2\n",
      "2023-11-23 09:11:36,796 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "2023-11-23 09:11:36,796 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "2023-11-23 09:11:36,796 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "2023-11-23 09:11:36,796 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
      "2023-11-23 09:11:36,796 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "2023-11-23 09:11:36,796 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "2023-11-23 09:11:36,824 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
      "2023-11-23 09:11:36,824 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
      "2023-11-23 09:11:36,824 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
      "2023-11-23 09:11:36,824 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
      "2023-11-23 09:11:36,834 INFO util.GSet: Computing capacity for map INodeMap\n",
      "2023-11-23 09:11:36,834 INFO util.GSet: VM type       = 64-bit\n",
      "2023-11-23 09:11:36,834 INFO util.GSet: 1.0% max memory 910.5 MB = 9.1 MB\n",
      "2023-11-23 09:11:36,834 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "2023-11-23 09:11:36,835 INFO namenode.FSDirectory: ACLs enabled? true\n",
      "2023-11-23 09:11:36,835 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
      "2023-11-23 09:11:36,835 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "2023-11-23 09:11:36,835 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "2023-11-23 09:11:36,839 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
      "2023-11-23 09:11:36,840 INFO snapshot.SnapshotManager: SkipList is disabled\n",
      "2023-11-23 09:11:36,843 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "2023-11-23 09:11:36,843 INFO util.GSet: VM type       = 64-bit\n",
      "2023-11-23 09:11:36,844 INFO util.GSet: 0.25% max memory 910.5 MB = 2.3 MB\n",
      "2023-11-23 09:11:36,844 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
      "2023-11-23 09:11:36,849 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "2023-11-23 09:11:36,849 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "2023-11-23 09:11:36,849 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "2023-11-23 09:11:36,853 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "2023-11-23 09:11:36,853 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "2023-11-23 09:11:36,854 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "2023-11-23 09:11:36,854 INFO util.GSet: VM type       = 64-bit\n",
      "2023-11-23 09:11:36,855 INFO util.GSet: 0.029999999329447746% max memory 910.5 MB = 279.7 KB\n",
      "2023-11-23 09:11:36,855 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
      "2023-11-23 09:11:36,874 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1977806058-172.17.0.2-1700741496867\n",
      "2023-11-23 09:11:36,909 INFO common.Storage: Storage directory /opt/hadoop-3.3.6/data/nameNode has been successfully formatted.\n",
      "2023-11-23 09:11:36,969 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/hadoop-3.3.6/data/nameNode/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "2023-11-23 09:11:37,048 INFO namenode.FSImageFormatProtobuf: Image file /opt/hadoop-3.3.6/data/nameNode/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .\n",
      "2023-11-23 09:11:37,071 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "2023-11-23 09:11:37,088 INFO namenode.FSNamesystem: Stopping services started for active state\n",
      "2023-11-23 09:11:37,088 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
      "2023-11-23 09:11:37,091 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
      "2023-11-23 09:11:37,091 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at hadoop/172.17.0.2\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs namenode -format -force -nonInteractive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Hadoop daemons\n",
    "\n",
    "- manual execution: ```hdfs --daemon start (namenode|datanode)``` and ```yarn --daemon start (resourcemanager|nodemanager)```\n",
    "- auxilliary scripts to run all processes on the cluster: start-dfs.sh (HDFS) and start-yarn.sh (YARN)\n",
    "- some services still need to be executed manually (timelineserver, historyserver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "Starting datanodes\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop2: WARNING: /opt/hadoop-3.3.6/logs does not exist. Creating.\n",
      "hadoop1: WARNING: /opt/hadoop-3.3.6/logs does not exist. Creating.\n",
      "hadoop3: WARNING: /opt/hadoop-3.3.6/logs does not exist. Creating.\n",
      "Starting secondary namenodes [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "Starting resourcemanager\n",
      "Starting nodemanagers\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# HDFS\n",
    "start-dfs.sh\n",
    "\n",
    "# YARN\n",
    "start-yarn.sh\n",
    "\n",
    "# timelineserver\n",
    "yarn --daemon start timelineserver\n",
    "\n",
    "# historyserver\n",
    "mapred --daemon start historyserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      "950 ApplicationHistoryServer\n",
      "998 JobHistoryServer\n",
      "231 NameNode\n",
      "1095 Jps\n",
      "619 ResourceManager\n",
      "430 SecondaryNameNode\n",
      "hadoop1\n",
      "199 NodeManager\n",
      "90 DataNode\n",
      "302 Jps\n",
      "hadoop2\n",
      "199 NodeManager\n",
      "90 DataNode\n",
      "302 Jps\n",
      "hadoop3\n",
      "199 NodeManager\n",
      "90 DataNode\n",
      "302 Jps\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Listing all processes\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3; do\n",
    "    echo $HOST\n",
    "    docker exec $HOST jps\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HDFS directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp': File exists\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/hadoop\n",
    "hdfs dfs -chown hadoop:hadoop /user/hadoop\n",
    "hdfs dfs -mkdir /tmp\n",
    "hdfs dfs -chmod 777 /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and run Jupyter on master node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2023-11-23 09:55:23.225 ServerApp] ServerApp.token config is deprecated in 2.0. Use IdentityProvider.token.\n",
      "[I 2023-11-23 09:55:23.235 ServerApp] Package jupyterlab took 0.0000s to import\n",
      "[I 2023-11-23 09:55:23.244 ServerApp] Package jupyter_lsp took 0.0079s to import\n",
      "[W 2023-11-23 09:55:23.244 ServerApp] A `_jupyter_server_extension_points` function was not found in jupyter_lsp. Instead, a `_jupyter_server_extension_paths` function was found and will be used for now. This function name will be deprecated in future releases of Jupyter Server.\n",
      "[I 2023-11-23 09:55:23.248 ServerApp] Package jupyter_server_terminals took 0.0035s to import\n",
      "[I 2023-11-23 09:55:23.249 ServerApp] Package notebook_shim took 0.0000s to import\n",
      "[W 2023-11-23 09:55:23.249 ServerApp] A `_jupyter_server_extension_points` function was not found in notebook_shim. Instead, a `_jupyter_server_extension_paths` function was found and will be used for now. This function name will be deprecated in future releases of Jupyter Server.\n",
      "[I 2023-11-23 09:55:23.249 ServerApp] jupyter_lsp | extension was successfully linked.\n",
      "[I 2023-11-23 09:55:23.255 ServerApp] jupyter_server_terminals | extension was successfully linked.\n",
      "[I 2023-11-23 09:55:23.263 ServerApp] jupyterlab | extension was successfully linked.\n",
      "[I 2023-11-23 09:55:23.415 ServerApp] notebook_shim | extension was successfully linked.\n",
      "[W 2023-11-23 09:55:23.437 ServerApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.\n",
      "[I 2023-11-23 09:55:23.437 ServerApp] notebook_shim | extension was successfully loaded.\n",
      "[I 2023-11-23 09:55:23.438 ServerApp] jupyter_lsp | extension was successfully loaded.\n",
      "[I 2023-11-23 09:55:23.439 ServerApp] jupyter_server_terminals | extension was successfully loaded.\n",
      "[I 2023-11-23 09:55:23.441 LabApp] JupyterLab extension loaded from /home/hadoop/.local/lib/python3.8/site-packages/jupyterlab\n",
      "[I 2023-11-23 09:55:23.441 LabApp] JupyterLab application directory is /home/hadoop/.local/share/jupyter/lab\n",
      "[I 2023-11-23 09:55:23.441 LabApp] Extension Manager is 'pypi'.\n",
      "[I 2023-11-23 09:55:23.444 ServerApp] jupyterlab | extension was successfully loaded.\n",
      "[I 2023-11-23 09:55:23.445 ServerApp] Serving notebooks from local directory: /\n",
      "[I 2023-11-23 09:55:23.445 ServerApp] Jupyter Server 2.10.1 is running at:\n",
      "[I 2023-11-23 09:55:23.445 ServerApp] http://172.17.0.2:8080/lab\n",
      "[I 2023-11-23 09:55:23.445 ServerApp]     http://127.0.0.1:8080/lab\n",
      "[I 2023-11-23 09:55:23.445 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
      "[I 2023-11-23 09:55:23.467 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# pip3 -q install notebook\n",
    "pip3 -q install pip\n",
    "pip3 -q install traitlets==5.6.0\n",
    "pip3 -q install jupyterlab\n",
    "\n",
    "IP=$(ifconfig eth0 | grep inet | awk '{ print $2 }')\n",
    "\n",
    "cd /opt\n",
    "\n",
    "export SHELL=/bin/bash\n",
    "# nohup /home/hadoop/.local/bin/jupyter-notebook --ip=$IP --port=8080 --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.notebook_dir='/' --no-browser &\n",
    "nohup /home/hadoop/.local/bin/jupyter-lab --ip=$IP --port=8080 --notebook-dir='/' --ServerApp.token='' --ServerApp.password='' --no-browser &\n",
    "\n",
    "echo $! > /tmp/jupyter.pid\n",
    "\n",
    "# To kill\n",
    "# kill $(cat /tmp/jupyter.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access web interfaces\n",
    "\n",
    "- Jupyterlab\n",
    "    - http://localhost:8080\n",
    "- Master - hadoop\n",
    "    - Resource Manager: http://localhost:8088\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "    - MapReduce Job History: http://localhost:19888\n",
    "    - Timeline Service: http://localhost:8188\n",
    "- Workers\n",
    "    - hadoop1\n",
    "        - NodeManager: http://localhost:8042\n",
    "        - DataNode: http://localhost:9864\n",
    "    - hadoop2\n",
    "        - NodeManager: http://localhost:8043\n",
    "        - DataNode: http://localhost:9865\n",
    "    - hadoop3\n",
    "        - NodeManager: http://localhost:8044\n",
    "        - DataNode: http://localhost:9866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run mapreduce Pi example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Maps  = 24\n",
      "Samples per Map = 100000\n",
      "Wrote input for Map #0\n",
      "Wrote input for Map #1\n",
      "Wrote input for Map #2\n",
      "Wrote input for Map #3\n",
      "Wrote input for Map #4\n",
      "Wrote input for Map #5\n",
      "Wrote input for Map #6\n",
      "Wrote input for Map #7\n",
      "Wrote input for Map #8\n",
      "Wrote input for Map #9\n",
      "Wrote input for Map #10\n",
      "Wrote input for Map #11\n",
      "Wrote input for Map #12\n",
      "Wrote input for Map #13\n",
      "Wrote input for Map #14\n",
      "Wrote input for Map #15\n",
      "Wrote input for Map #16\n",
      "Wrote input for Map #17\n",
      "Wrote input for Map #18\n",
      "Wrote input for Map #19\n",
      "Wrote input for Map #20\n",
      "Wrote input for Map #21\n",
      "Wrote input for Map #22\n",
      "Wrote input for Map #23\n",
      "Starting Job\n",
      "2023-11-23 10:06:30,991 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2023-11-23 10:06:31,109 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2023-11-23 10:06:31,207 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1700741518592_0007\n",
      "2023-11-23 10:06:31,361 INFO input.FileInputFormat: Total input files to process : 24\n",
      "2023-11-23 10:06:31,438 INFO mapreduce.JobSubmitter: number of splits:24\n",
      "2023-11-23 10:06:31,563 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1700741518592_0007\n",
      "2023-11-23 10:06:31,563 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-11-23 10:06:31,691 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-11-23 10:06:31,692 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-11-23 10:06:31,739 INFO impl.YarnClientImpl: Submitted application application_1700741518592_0007\n",
      "2023-11-23 10:06:31,800 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1700741518592_0007/\n",
      "2023-11-23 10:06:31,800 INFO mapreduce.Job: Running job: job_1700741518592_0007\n",
      "2023-11-23 10:06:36,909 INFO mapreduce.Job: Job job_1700741518592_0007 running in uber mode : false\n",
      "2023-11-23 10:06:36,910 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-11-23 10:06:56,203 INFO mapreduce.Job:  map 4% reduce 0%\n",
      "2023-11-23 10:06:57,208 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "2023-11-23 10:07:02,297 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "2023-11-23 10:07:03,309 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2023-11-23 10:07:04,330 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "2023-11-23 10:07:05,336 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2023-11-23 10:07:10,418 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "2023-11-23 10:07:11,436 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "2023-11-23 10:07:14,451 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-11-23 10:07:24,478 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "2023-11-23 10:10:03,833 INFO mapreduce.Job: Task Id : attempt_1700741518592_0007_r_000000_0, Status : FAILED\n",
      "Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#3\n",
      "\tat org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:136)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:377)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out.\n",
      "\tat org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:396)\n",
      "\tat org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:311)\n",
      "\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.openShuffleUrl(Fetcher.java:293)\n",
      "\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:332)\n",
      "\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:200)\n",
      "\n",
      "2023-11-23 10:10:04,855 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-11-23 10:10:07,862 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "2023-11-23 10:10:15,922 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "2023-11-23 10:10:16,924 INFO mapreduce.Job:  map 92% reduce 0%\n",
      "2023-11-23 10:10:17,927 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-11-23 10:10:17,930 INFO mapreduce.Job: Job job_1700741518592_0007 completed successfully\n",
      "2023-11-23 10:10:17,991 INFO mapreduce.Job: Counters: 57\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=534\n",
      "\t\tFILE: Number of bytes written=6920190\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6326\n",
      "\t\tHDFS: Number of bytes written=215\n",
      "\t\tHDFS: Number of read operations=101\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tFailed reduce tasks=1\n",
      "\t\tKilled map tasks=6\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tOther local map tasks=6\n",
      "\t\tData-local map tasks=24\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3479602\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=384366\n",
      "\t\tTotal time spent by all map tasks (ms)=1739801\n",
      "\t\tTotal time spent by all reduce tasks (ms)=192183\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1739801\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=192183\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=445389056\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=49198848\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24\n",
      "\t\tMap output records=48\n",
      "\t\tMap output bytes=432\n",
      "\t\tMap output materialized bytes=672\n",
      "\t\tInput split bytes=3494\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=672\n",
      "\t\tReduce input records=48\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=96\n",
      "\t\tShuffled Maps =24\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=24\n",
      "\t\tGC time elapsed (ms)=6936\n",
      "\t\tCPU time spent (ms)=9950\n",
      "\t\tPhysical memory (bytes) snapshot=7154917376\n",
      "\t\tVirtual memory (bytes) snapshot=46888771584\n",
      "\t\tTotal committed heap usage (bytes)=4792516608\n",
      "\t\tPeak Map Physical memory (bytes)=294182912\n",
      "\t\tPeak Map Virtual memory (bytes)=1877217280\n",
      "\t\tPeak Reduce Physical memory (bytes)=187805696\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1881313280\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2832\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=97\n",
      "Job Finished in 227.174 seconds\n",
      "Estimated value of Pi is 3.14157333333333333333\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar pi 6 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHUTDOWN PROCEDURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "kill $(cat /tmp/jupyter.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Hadoop daemons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "Stopping datanodes\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "Stopping secondary namenodes [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "Stopping nodemanagers\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "Stopping resourcemanager\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "stop-dfs.sh\n",
    "stop-yarn.sh\n",
    "yarn --daemon stop timelineserver\n",
    "mapred --daemon stop historyserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Docker containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      "hadoop1\n",
      "hadoop2\n",
      "hadoop3\n",
      "CONTAINER ID   IMAGE       COMMAND       CREATED             STATUS             PORTS                                                                                  NAMES\n",
      "44d8e5b2cab1   hadoopimg   \"/bin/bash\"   About an hour ago   Up About an hour   0.0.0.0:8044->8042/tcp, :::8044->8042/tcp, 0.0.0.0:9866->9864/tcp, :::9866->9864/tcp   hadoop3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3; do\n",
    "    docker stop $HOST\n",
    "done\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
