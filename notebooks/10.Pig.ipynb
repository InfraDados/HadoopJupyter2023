{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pig\n",
    "![Pig](https://pig.apache.org/images/pig-logo.gif)\n",
    "\n",
    "- https://pig.apache.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- version 0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# Download package\n",
    "mkdir -p /opt/pkgs\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://downloads.apache.org/pig/pig-0.17.0/pig-0.17.0.tar.gz\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf pig-0.17.0.tar.gz -C /opt\n",
    "ln -s /opt/pig-0.17.0 /opt/pig\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Pig\n",
    "export PIG_HOME=/opt/pig\n",
    "export PATH=\\${PATH}:\\${PIG_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir -p /opt/datasets_pig\n",
    "cd /opt/datasets_pig\n",
    "wget -q -c https://tinyurl.com/y5roz8kz -O stations.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/list_stations_local.pig\n",
    "\n",
    "stations = LOAD 'stations.csv' USING PigStorage(',') AS \n",
    "(station_id:int, name:chararray, lat:float, long:float, dockcount:int, landmark:chararray, installation:chararray);\n",
    "station_ids_names = FOREACH stations GENERATE station_id, name;\n",
    "ordered = ORDER station_ids_names BY name;\n",
    "DESCRIBE stations;\n",
    "ILLUSTRATE ordered;\n",
    "DUMP ordered;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets_pig\n",
    "\n",
    "# run local\n",
    "pig -x local -f /opt/src/list_stations_local.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/list_stations_cluster.pig\n",
    "\n",
    "stations = LOAD 'stations' USING PigStorage(',') AS \n",
    "(station_id:int, name:chararray, lat:float, long:float, dockcount:int, landmark:chararray, installation:chararray);\n",
    "station_ids_names = FOREACH stations GENERATE station_id, name;\n",
    "ordered = ORDER station_ids_names BY name;\n",
    "STORE ordered INTO 'ordered';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets_pig\n",
    "\n",
    "# Upload to HDFS\n",
    "hdfs dfs -mkdir stations\n",
    "hdfs dfs -put stations.csv stations\n",
    "\n",
    "# run in Hadoop cluster using mapreduce\n",
    "pig -x mapreduce -f /opt/src/list_stations_cluster.pig 2>/dev/null\n",
    "\n",
    "hdfs dfs -cat ordered/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount using Pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets_pig\n",
    "\n",
    "wget -q -c https://tinyurl.com/y68jxy7f -O stop-word-list.csv\n",
    "hdfs dfs -mkdir -p stopwords\n",
    "hdfs dfs -put stop-word-list.csv stopwords\n",
    "hdfs dfs -cat stopwords/stop-word-list.csv\n",
    "\n",
    "# download book \"The Complete Works of William Shakespeare, by William Shakespeare\" from Gutenberg Project\n",
    "wget -q -c http://www.gutenberg.org/files/100/100-0.txt -O shakespeare.txt\n",
    "\n",
    "# create directory in HDFS and put file\n",
    "hdfs dfs -mkdir -p shakespeare\n",
    "hdfs dfs -put shakespeare.txt shakespeare\n",
    "hdfs dfs -ls -h shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/wordcount.pig\n",
    "\n",
    "-- List HDFS content\n",
    "fs -ls\n",
    "fs -ls shakespeare\n",
    "\n",
    "-- Job name to appear in YARN\n",
    "SET job.name 'Word Count in Pig';\n",
    "\n",
    "-- Load shakespeare dataset\n",
    "shakespeare = LOAD 'shakespeare' AS (lineoftext:chararray);\n",
    "\n",
    "-- Load stopwords\n",
    "stopwords = LOAD 'stopwords' USING PigStorage() AS (stopword:chararray);\n",
    "\n",
    "-- Create bag of words\n",
    "words = FOREACH shakespeare GENERATE\n",
    "        FLATTEN(TOKENIZE(REPLACE(LOWER(TRIM(lineoftext)),\n",
    "        '[\\\\p{Punct},\\\\p{Cntrl}]',''))) AS word;\n",
    "\n",
    "-- Remove empty words\n",
    "realwords = FILTER words BY SIZE(word) > 0;\n",
    "\n",
    "-- Create bag of stop words\n",
    "flattened_stopwords = FOREACH stopwords GENERATE\n",
    "       FLATTEN(TOKENIZE(stopword)) AS stopword;\n",
    "\n",
    "-- Associate words with respective stop words\n",
    "right_joined = JOIN flattened_stopwords\n",
    "               BY stopword RIGHT OUTER,\n",
    "               realwords BY word;\n",
    "\n",
    "-- Remove stop words\n",
    "meaningful_words = FILTER right_joined BY\n",
    "          (flattened_stopwords::stopword IS NULL);\n",
    "\n",
    "-- Retrieve remaining words\n",
    "shakespeare_real_words = FOREACH meaningful_words\n",
    "          GENERATE realwords::word AS word;\n",
    "\n",
    "-- Group words\n",
    "grouped = GROUP shakespeare_real_words BY word;\n",
    "\n",
    "-- Count grouped words\n",
    "counted = FOREACH grouped GENERATE group AS word,\n",
    "          COUNT(shakespeare_real_words) AS wordcount;\n",
    "\n",
    "-- Sort bag in descending order\n",
    "ordered = ORDER counted BY wordcount DESC;\n",
    "\n",
    "-- Select 30 first words\n",
    "top30 = LIMIT ordered 30;\n",
    "\n",
    "-- Store output\n",
    "STORE top30 INTO 'shakespeare_top30';\n",
    "\n",
    "-- Show output from HDFS\n",
    "fs -cat shakespeare_top30/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# run in Hadoop cluster using mapreduce\n",
    "pig -x mapreduce -f /opt/src/wordcount.pig 2> wordcount.log\n",
    "\n",
    "# remove output in HDFS\n",
    "# hdfs dfs -rm -r shakespeare_top30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
