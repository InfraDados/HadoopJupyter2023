{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive\n",
    "![Hive](https://hive.apache.org/images/hive_logo_medium.jpg)\n",
    "\n",
    "- https://hive.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- version 3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
      "export PDSH_RCMD_TYPE=ssh\n",
      "export HADOOP_HOME=/opt/hadoop\n",
      "export HADOOP_VERSION=3.3.6\n",
      "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
      "export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
      "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
      "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
      "export HADOOP_YARN_HOME=${HADOOP_HOME}\n",
      "export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin\n",
      "# Hive\n",
      "export HIVE_HOME=/opt/hive\n",
      "export PATH=${PATH}:${HIVE_HOME}/bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# Download package\n",
    "mkdir -p /opt/pkgs\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf apache-hive-3.1.3-bin.tar.gz -C /opt\n",
    "ln -s /opt/apache-hive-3.1.3-bin /opt/hive\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Hive\n",
    "export HIVE_HOME=/opt/hive\n",
    "export PATH=\\${PATH}:\\${HIVE_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "# Fix slf4j\n",
    "rm /opt/hive/lib/log4j-slf4j-impl-2.17.1.jar\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop configuration (for beeline)\n",
    "\n",
    "- core-site.xml\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "...\n",
    "<property>\n",
    "  <name>hadoop.proxyuser.hadoop.groups</name>\n",
    "  <value>*</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>hadoop.proxyuser.hadoop.hosts</name>\n",
    "  <value>*</value>\n",
    "</property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Metastore\n",
    "\n",
    "- using local Derby database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directory in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/hive/warehouse\n",
    "hdfs dfs -chmod g+w /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metastore connection URL:\t jdbc:derby:;databaseName=metastore_db;create=true\n",
      "Metastore Connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\n",
      "Metastore connection User:\t APP\n",
      "Starting metastore schema initialization to 3.1.0\n",
      "Initialization script hive-schema-3.1.0.derby.sql\n",
      "Initialization script completed\n",
      "schemaTool completed\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir -p $HIVE_HOME/hiveserver2\n",
    "cd $HIVE_HOME/hiveserver2\n",
    "$HIVE_HOME/bin/schematool -dbType derby -initSchema 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start hiveserver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/hive/hiveserver2\n",
    "nohup /opt/hive/bin/hive --service hiveserver2 \\\n",
    "--hiveconf hive.security.authorization.createtable.owner.grants=ALL \\\n",
    "--hiveconf hive.root.logger=INFO,console > hiveserver2.out 2>&1 &\n",
    "echo $! > hiveserver2.pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "- SF Bay Area Bike Share (https://www.kaggle.com/benhamner/sf-bay-area-bike-share)\n",
    "- stations.csv and trips.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir -p /opt/datasets_hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# copy datasets used by hive examples to hadoop container\n",
    "docker cp hivedataset.tgz hadoop:/opt/datasets_hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stations.csv\n",
      "trips.csv\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets_hive\n",
    "tar -zxf hivedataset.tgz\n",
    "rm hivedataset.tgz\n",
    "ls\n",
    "\n",
    "hdfs dfs -mkdir -p bikeshare/stations\n",
    "hdfs dfs -put stations.csv bikeshare/stations\n",
    "hdfs dfs -mkdir -p bikeshare/trips\n",
    "hdfs dfs -put trips.csv bikeshare/trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using beeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[sPreparing to copy...\u001b[?25l\u001b[u\u001b[2KCopying to container - 0B\u001b[?25h\u001b[u\u001b[2KSuccessfully copied 2.05kB to hadoop:/opt/script.sql\n"
     ]
    }
   ],
   "source": [
    "%%dockerwrite hadoop /opt/script.sql\n",
    "\n",
    "-- configure jobs executor\n",
    "SET hive.execution.engine=mr;\n",
    "SET mapreduce.framework.name=yarn;\n",
    "\n",
    "-- create bikeshare database\n",
    "CREATE DATABASE bikeshare;\n",
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.6/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+----------------+\n",
      "| database_name  |\n",
      "+----------------+\n",
      "| bikeshare      |\n",
      "| default        |\n",
      "+----------------+\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/script.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[sPreparing to copy...\u001b[?25l\u001b[u\u001b[2KCopying to container - 0B\u001b[?25h\u001b[u\u001b[2KSuccessfully copied 2.56kB to hadoop:/opt/script.sql\n"
     ]
    }
   ],
   "source": [
    "%%dockerwrite hadoop /opt/script.sql\n",
    "\n",
    "USE bikeshare;\n",
    "\n",
    "-- create stations table\n",
    "CREATE EXTERNAL TABLE stations (\n",
    "    station_id INT,\n",
    "    name STRING,\n",
    "    lat DOUBLE,\n",
    "    long DOUBLE,\n",
    "    dockcount INT,\n",
    "    landmark STRING,\n",
    "    installation STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs:///user/hadoop/bikeshare/stations';\n",
    "\n",
    "-- create trips table\n",
    "CREATE EXTERNAL TABLE trips (\n",
    "    trip_id INT,\n",
    "    duration INT,\n",
    "    start_date STRING,\n",
    "    start_station STRING,\n",
    "    start_terminal INT,\n",
    "    end_date STRING,\n",
    "    end_station STRING,\n",
    "    end_terminal INT,\n",
    "    bike_num INT,\n",
    "    subscription_type STRING,\n",
    "    zip_code STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs:///user/hadoop/bikeshare/trips';\n",
    "\n",
    "-- show tables\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+-----------+\n",
      "| tab_name  |\n",
      "+-----------+\n",
      "| stations  |\n",
      "| trips     |\n",
      "+-----------+\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/script.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[sPreparing to copy...\u001b[?25l\u001b[u\u001b[2KCopying to container - 0B\u001b[?25h\u001b[u\u001b[2KSuccessfully copied 2.05kB to hadoop:/opt/script.sql\n"
     ]
    }
   ],
   "source": [
    "%%dockerwrite hadoop /opt/script.sql\n",
    "\n",
    "USE bikeshare;\n",
    "\n",
    "DESCRIBE stations;\n",
    "DESCRIBE trips;\n",
    "DESCRIBE FORMATTED stations;\n",
    "DESCRIBE FORMATTED trips;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "+---------------+------------+----------+\n",
      "|   col_name    | data_type  | comment  |\n",
      "+---------------+------------+----------+\n",
      "| station_id    | int        |          |\n",
      "| name          | string     |          |\n",
      "| lat           | double     |          |\n",
      "| long          | double     |          |\n",
      "| dockcount     | int        |          |\n",
      "| landmark      | string     |          |\n",
      "| installation  | string     |          |\n",
      "+---------------+------------+----------+\n",
      "\n",
      "+--------------------+------------+----------+\n",
      "|      col_name      | data_type  | comment  |\n",
      "+--------------------+------------+----------+\n",
      "| trip_id            | int        |          |\n",
      "| duration           | int        |          |\n",
      "| start_date         | string     |          |\n",
      "| start_station      | string     |          |\n",
      "| start_terminal     | int        |          |\n",
      "| end_date           | string     |          |\n",
      "| end_station        | string     |          |\n",
      "| end_terminal       | int        |          |\n",
      "| bike_num           | int        |          |\n",
      "| subscription_type  | string     |          |\n",
      "| zip_code           | string     |          |\n",
      "+--------------------+------------+----------+\n",
      "\n",
      "+-------------------------------+----------------------------------------------------+-----------------------+\n",
      "|           col_name            |                     data_type                      |        comment        |\n",
      "+-------------------------------+----------------------------------------------------+-----------------------+\n",
      "| # col_name                    | data_type                                          | comment               |\n",
      "| station_id                    | int                                                |                       |\n",
      "| name                          | string                                             |                       |\n",
      "| lat                           | double                                             |                       |\n",
      "| long                          | double                                             |                       |\n",
      "| dockcount                     | int                                                |                       |\n",
      "| landmark                      | string                                             |                       |\n",
      "| installation                  | string                                             |                       |\n",
      "|                               | NULL                                               | NULL                  |\n",
      "| # Detailed Table Information  | NULL                                               | NULL                  |\n",
      "| Database:                     | bikeshare                                          | NULL                  |\n",
      "| OwnerType:                    | USER                                               | NULL                  |\n",
      "| Owner:                        | hadoop                                             | NULL                  |\n",
      "| CreateTime:                   | Mon Dec 18 21:20:40 BRT 2023                       | NULL                  |\n",
      "| LastAccessTime:               | UNKNOWN                                            | NULL                  |\n",
      "| Retention:                    | 0                                                  | NULL                  |\n",
      "| Location:                     | hdfs://hadoop:9000/user/hadoop/bikeshare/stations  | NULL                  |\n",
      "| Table Type:                   | EXTERNAL_TABLE                                     | NULL                  |\n",
      "| Table Parameters:             | NULL                                               | NULL                  |\n",
      "|                               | EXTERNAL                                           | TRUE                  |\n",
      "|                               | bucketing_version                                  | 2                     |\n",
      "|                               | numFiles                                           | 1                     |\n",
      "|                               | totalSize                                          | 5143                  |\n",
      "|                               | transient_lastDdlTime                              | 1702945240            |\n",
      "|                               | NULL                                               | NULL                  |\n",
      "| # Storage Information         | NULL                                               | NULL                  |\n",
      "| SerDe Library:                | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe | NULL                  |\n",
      "| InputFormat:                  | org.apache.hadoop.mapred.TextInputFormat           | NULL                  |\n",
      "| OutputFormat:                 | org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat | NULL                  |\n",
      "| Compressed:                   | No                                                 | NULL                  |\n",
      "| Num Buckets:                  | -1                                                 | NULL                  |\n",
      "| Bucket Columns:               | []                                                 | NULL                  |\n",
      "| Sort Columns:                 | []                                                 | NULL                  |\n",
      "| Storage Desc Params:          | NULL                                               | NULL                  |\n",
      "|                               | field.delim                                        | ,                     |\n",
      "|                               | serialization.format                               | ,                     |\n",
      "+-------------------------------+----------------------------------------------------+-----------------------+\n",
      "\n",
      "+-------------------------------+----------------------------------------------------+-----------------------+\n",
      "|           col_name            |                     data_type                      |        comment        |\n",
      "+-------------------------------+----------------------------------------------------+-----------------------+\n",
      "| # col_name                    | data_type                                          | comment               |\n",
      "| trip_id                       | int                                                |                       |\n",
      "| duration                      | int                                                |                       |\n",
      "| start_date                    | string                                             |                       |\n",
      "| start_station                 | string                                             |                       |\n",
      "| start_terminal                | int                                                |                       |\n",
      "| end_date                      | string                                             |                       |\n",
      "| end_station                   | string                                             |                       |\n",
      "| end_terminal                  | int                                                |                       |\n",
      "| bike_num                      | int                                                |                       |\n",
      "| subscription_type             | string                                             |                       |\n",
      "| zip_code                      | string                                             |                       |\n",
      "|                               | NULL                                               | NULL                  |\n",
      "| # Detailed Table Information  | NULL                                               | NULL                  |\n",
      "| Database:                     | bikeshare                                          | NULL                  |\n",
      "| OwnerType:                    | USER                                               | NULL                  |\n",
      "| Owner:                        | hadoop                                             | NULL                  |\n",
      "| CreateTime:                   | Mon Dec 18 21:20:40 BRT 2023                       | NULL                  |\n",
      "| LastAccessTime:               | UNKNOWN                                            | NULL                  |\n",
      "| Retention:                    | 0                                                  | NULL                  |\n",
      "| Location:                     | hdfs://hadoop:9000/user/hadoop/bikeshare/trips     | NULL                  |\n",
      "| Table Type:                   | EXTERNAL_TABLE                                     | NULL                  |\n",
      "| Table Parameters:             | NULL                                               | NULL                  |\n",
      "|                               | EXTERNAL                                           | TRUE                  |\n",
      "|                               | bucketing_version                                  | 2                     |\n",
      "|                               | numFiles                                           | 1                     |\n",
      "|                               | totalSize                                          | 43012526              |\n",
      "|                               | transient_lastDdlTime                              | 1702945240            |\n",
      "|                               | NULL                                               | NULL                  |\n",
      "| # Storage Information         | NULL                                               | NULL                  |\n",
      "| SerDe Library:                | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe | NULL                  |\n",
      "| InputFormat:                  | org.apache.hadoop.mapred.TextInputFormat           | NULL                  |\n",
      "| OutputFormat:                 | org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat | NULL                  |\n",
      "| Compressed:                   | No                                                 | NULL                  |\n",
      "| Num Buckets:                  | -1                                                 | NULL                  |\n",
      "| Bucket Columns:               | []                                                 | NULL                  |\n",
      "| Sort Columns:                 | []                                                 | NULL                  |\n",
      "| Storage Desc Params:          | NULL                                               | NULL                  |\n",
      "|                               | field.delim                                        | ,                     |\n",
      "|                               | serialization.format                               | ,                     |\n",
      "+-------------------------------+----------------------------------------------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/script.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[sPreparing to copy...\u001b[?25l\u001b[u\u001b[2KCopying to container - 0B\u001b[?25h\u001b[u\u001b[2KSuccessfully copied 2.05kB to hadoop:/opt/script.sql\n"
     ]
    }
   ],
   "source": [
    "%%dockerwrite hadoop /opt/script.sql\n",
    "\n",
    "USE bikeshare;\n",
    "\n",
    "-- query - number of trips per terminal\n",
    "SELECT start_terminal, start_station, COUNT(1) AS count\n",
    "FROM trips\n",
    "GROUP BY start_terminal, start_station\n",
    "ORDER BY count\n",
    "DESC LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+-----------------+------------------------------------------------+--------+\n",
      "| start_terminal  |                 start_station                  | count  |\n",
      "+-----------------+------------------------------------------------+--------+\n",
      "| 70              | San Francisco Caltrain (Townsend at 4th)       | 26304  |\n",
      "| 69              | San Francisco Caltrain 2 (330 Townsend)        | 21758  |\n",
      "| 50              | Harry Bridges Plaza (Ferry Building)           | 17255  |\n",
      "| 55              | Temporary Transbay Terminal (Howard at Beale)  | 14436  |\n",
      "| 60              | Embarcadero at Sansome                         | 14158  |\n",
      "| 61              | 2nd at Townsend                                | 14026  |\n",
      "| 65              | Townsend at 7th                                | 13752  |\n",
      "| 74              | Steuart at Market                              | 13687  |\n",
      "| 67              | Market at 10th                                 | 11885  |\n",
      "| 77              | Market at Sansome                              | 11431  |\n",
      "+-----------------+------------------------------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/script.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[sPreparing to copy...\u001b[?25l\u001b[u\u001b[2KCopying to container - 0B\u001b[?25h\u001b[u\u001b[2KSuccessfully copied 2.05kB to hadoop:/opt/script.sql\n"
     ]
    }
   ],
   "source": [
    "%%dockerwrite hadoop /opt/script.sql\n",
    "\n",
    "USE bikeshare;\n",
    "\n",
    "-- query - join between stations and trips\n",
    "SELECT t.trip_id, t.duration, t.start_date, s.name, s.lat, s.long, s.landmark\n",
    "FROM stations s\n",
    "JOIN trips t ON s.station_id = t.start_terminal\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+------------+-------------+------------------+------------------------------------------------+------------+--------------+----------------+\n",
      "| t.trip_id  | t.duration  |   t.start_date   |                     s.name                     |   s.lat    |    s.long    |   s.landmark   |\n",
      "+------------+-------------+------------------+------------------------------------------------+------------+--------------+----------------+\n",
      "| 913460     | 765         | 8/31/2015 23:26  | Harry Bridges Plaza (Ferry Building)           | 37.795392  | -122.394203  | San Francisco  |\n",
      "| 913459     | 1036        | 8/31/2015 23:11  | San Antonio Shopping Center                    | 37.400443  | -122.108338  | Mountain View  |\n",
      "| 913455     | 307         | 8/31/2015 23:13  | Post at Kearney                                | 37.788975  | -122.403452  | San Francisco  |\n",
      "| 913454     | 409         | 8/31/2015 23:10  | San Jose City Hall                             | 37.337391  | -121.886995  | San Jose       |\n",
      "| 913453     | 789         | 8/31/2015 23:09  | Embarcadero at Folsom                          | 37.791464  | -122.391034  | San Francisco  |\n",
      "| 913452     | 293         | 8/31/2015 23:07  | Yerba Buena Center of the Arts (3rd @ Howard)  | 37.784878  | -122.401014  | San Francisco  |\n",
      "| 913451     | 896         | 8/31/2015 23:07  | Embarcadero at Folsom                          | 37.791464  | -122.391034  | San Francisco  |\n",
      "| 913450     | 255         | 8/31/2015 22:16  | Embarcadero at Sansome                         | 37.80477   | -122.403234  | San Francisco  |\n",
      "| 913449     | 126         | 8/31/2015 22:12  | Beale at Market                                | 37.792251  | -122.397086  | San Francisco  |\n",
      "| 913448     | 932         | 8/31/2015 21:57  | Post at Kearney                                | 37.788975  | -122.403452  | San Francisco  |\n",
      "+------------+-------------+------------------+------------------------------------------------+------------+--------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/script.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount using Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: line 3: cd: /opt/datasets_hive: No such file or directory\n",
      "a, able, about, across, after, all, almost, also, am, among, an, and, any, are, as, at, be, because, been, but, by, can, cannot, could, dear, did, do, does, either, else, ever, every, for, from, get, got, had, has, have, he, her, hers, him, his, how, however, i, if, in, into, is, it, its, just, least, let, like, likely, may, me, might, most, must, my, neither, no, nor, not, of, off, often, on, only, or, other, our, own, rather, said, say, says, she, should, since, so, some, than, that, the, their, them, then, there, these, they, this, tis, to, too, twas, us, wants, was, we, were, what, when, where, which, while, who, whom, why, will, with, would, yet, you, yourFound 1 items\n",
      "-rw-r--r--   2 hadoop hadoop      5.4 M 2023-12-18 22:05 shakespeare/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir -p /opt/datasets_hive\n",
    "cd /opt/datasets_hive\n",
    "\n",
    "wget -q -c https://tinyurl.com/y68jxy7f -O stop-word-list.csv\n",
    "hdfs dfs -mkdir -p stopwords\n",
    "hdfs dfs -put stop-word-list.csv stopwords\n",
    "hdfs dfs -cat stopwords/stop-word-list.csv\n",
    "\n",
    "# download book \"The Complete Works of William Shakespeare, by William Shakespeare\" from Gutenberg Project\n",
    "wget -q -c http://www.gutenberg.org/files/100/100-0.txt -O shakespeare.txt\n",
    "\n",
    "# create directory in HDFS and put file\n",
    "hdfs dfs -mkdir -p shakespeare\n",
    "hdfs dfs -put shakespeare.txt shakespeare\n",
    "hdfs dfs -ls -h shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[sPreparing to copy...\u001b[?25l\u001b[u\u001b[2KCopying to container - 0B\u001b[?25h\u001b[u\u001b[2KSuccessfully copied 2.05kB to hadoop:/opt/wordcount.sql\n"
     ]
    }
   ],
   "source": [
    "%%dockerwrite hadoop /opt/wordcount.sql\n",
    "\n",
    "CREATE TABLE shakespeare_text (line STRING);\n",
    "LOAD DATA INPATH '/user/hadoop/shakespeare/shakespeare.txt' INTO TABLE shakespeare_text;\n",
    "\n",
    "CREATE TABLE stopwords (word STRING);\n",
    "CREATE TABLE tempwords (line STRING);\n",
    "LOAD DATA INPATH '/user/hadoop/stopwords/stop-word-list.csv' INTO TABLE tempwords;\n",
    "\n",
    "-- split comma-separated stopwords to rows\n",
    "INSERT INTO stopwords\n",
    "SELECT word\n",
    "FROM tempwords\n",
    "LATERAL VIEW explode(split(line, ',')) t AS word;\n",
    "DROP TABLE tempwords;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+----------------------------------------------------+\n",
      "|               shakespeare_text.line                |\n",
      "+----------------------------------------------------+\n",
      "| The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare |\n",
      "|                                                    |\n",
      "| This eBook is for the use of anyone anywhere in the United States and |\n",
      "| most other parts of the world at no cost and with almost no restrictions |\n",
      "| whatsoever. You may copy it, give it away or re-use it under the terms |\n",
      "| of the Project Gutenberg License included with this eBook or online at |\n",
      "| www.gutenberg.org. If you are not located in the United States, you |\n",
      "| will have to check the laws of the country where you are located before |\n",
      "| using this eBook.                                  |\n",
      "|                                                    |\n",
      "+----------------------------------------------------+\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+-----------------+\n",
      "| stopwords.word  |\n",
      "+-----------------+\n",
      "| a               |\n",
      "|  able           |\n",
      "|  about          |\n",
      "|  across         |\n",
      "|  after          |\n",
      "|  all            |\n",
      "|  almost         |\n",
      "|  also           |\n",
      "|  am             |\n",
      "|  among          |\n",
      "+-----------------+\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/wordcount.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[sPreparing to copy...\u001b[?25l\u001b[u\u001b[2KCopying to container - 0B\u001b[?25h\u001b[u\u001b[2KSuccessfully copied 2.05kB to hadoop:/opt/wordcount.sql\n"
     ]
    }
   ],
   "source": [
    "%%dockerwrite hadoop /opt/wordcount.sql\n",
    "\n",
    "SELECT w.word, count(1) AS count\n",
    "FROM (\n",
    "    SELECT explode(split(regexp_replace(lower(line), '[^a-z\\\\s]', ''), '\\\\s+')) AS word\n",
    "    FROM shakespeare_text\n",
    ") w\n",
    "LEFT OUTER JOIN (\n",
    "    SELECT lower(trim(word)) AS word\n",
    "    FROM stopwords\n",
    ") s ON w.word = s.word\n",
    "WHERE s.word IS NULL AND w.word != ''\n",
    "GROUP BY w.word\n",
    "ORDER BY count DESC\n",
    "LIMIT 30;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+---------+--------+\n",
      "| w.word  | count  |\n",
      "+---------+--------+\n",
      "| thou    | 5856   |\n",
      "| thy     | 4353   |\n",
      "| shall   | 3851   |\n",
      "| thee    | 3416   |\n",
      "| lord    | 3124   |\n",
      "| king    | 3019   |\n",
      "| now     | 3014   |\n",
      "| sir     | 2976   |\n",
      "| good    | 2962   |\n",
      "| o       | 2769   |\n",
      "| well    | 2627   |\n",
      "| come    | 2625   |\n",
      "| more    | 2515   |\n",
      "| enter   | 2408   |\n",
      "| love    | 2291   |\n",
      "| here    | 2263   |\n",
      "| ill     | 2146   |\n",
      "| hath    | 2062   |\n",
      "| one     | 1959   |\n",
      "| man     | 1911   |\n",
      "| upon    | 1866   |\n",
      "| go      | 1796   |\n",
      "| make    | 1789   |\n",
      "| know    | 1759   |\n",
      "| scene   | 1631   |\n",
      "| see     | 1552   |\n",
      "| such    | 1532   |\n",
      "| out     | 1472   |\n",
      "| give    | 1423   |\n",
      "| first   | 1383   |\n",
      "+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/wordcount.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/hive/hiveserver2\n",
    "\n",
    "# kill hiveserver2\n",
    "kill $(cat hiveserver2.pid)\n",
    "rm hiveserver2.pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
