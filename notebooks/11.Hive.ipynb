{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive\n",
    "![Hive](https://hive.apache.org/images/hive_logo_medium.jpg)\n",
    "\n",
    "- https://hive.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- version 3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# Download package\n",
    "mkdir -p /opt/pkgs\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf apache-hive-3.1.3-bin.tar.gz -C /opt\n",
    "ln -s /opt/apache-hive-3.1.3-bin /opt/hive\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Hive\n",
    "export HIVE_HOME=/opt/hive\n",
    "export PATH=\\${PATH}:\\${HIVE_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "# Fix slf4j\n",
    "rm /opt/hive/lib/log4j-slf4j-impl-2.17.1.jar\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop configuration (for beeline)\n",
    "\n",
    "- core-site.xml\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "...\n",
    "<property>\n",
    "  <name>hadoop.proxyuser.hadoop.groups</name>\n",
    "  <value>*</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>hadoop.proxyuser.hadoop.hosts</name>\n",
    "  <value>*</value>\n",
    "</property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Metastore\n",
    "\n",
    "- using local Derby database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directory in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/hive/warehouse\n",
    "hdfs dfs -chmod g+w /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir -p $HIVE_HOME/hiveserver2\n",
    "cd $HIVE_HOME/hiveserver2\n",
    "$HIVE_HOME/bin/schematool -dbType derby -initSchema 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start hiveserver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/hive/hiveserver2\n",
    "nohup /opt/hive/bin/hive --service hiveserver2 \\\n",
    "--hiveconf hive.security.authorization.createtable.owner.grants=ALL \\\n",
    "--hiveconf hive.root.logger=INFO,console > hiveserver2.out 2>&1 &\n",
    "echo $! > hiveserver2.pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "- SF Bay Area Bike Share (https://www.kaggle.com/benhamner/sf-bay-area-bike-share)\n",
    "- stations.csv and trips.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir -p /opt/datasets_hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# copy datasets used by hive examples to hadoop container\n",
    "docker cp hivedataset.tgz hadoop:/opt/datasets_hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets_hive\n",
    "tar -zxf hivedataset.tgz\n",
    "rm hivedataset.tgz\n",
    "ls\n",
    "\n",
    "hdfs dfs -mkdir -p bikeshare/stations\n",
    "hdfs dfs -put stations.csv bikeshare/stations\n",
    "hdfs dfs -mkdir -p bikeshare/trips\n",
    "hdfs dfs -put trips.csv bikeshare/trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using beeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/script.sql\n",
    "\n",
    "-- configure jobs executor\n",
    "SET hive.execution.engine=mr;\n",
    "SET mapreduce.framework.name=yarn;\n",
    "\n",
    "-- create bikeshare database\n",
    "CREATE DATABASE bikeshare;\n",
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/script.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/script.sql\n",
    "\n",
    "USE bikeshare;\n",
    "\n",
    "-- create stations table\n",
    "CREATE EXTERNAL TABLE stations (\n",
    "    station_id INT,\n",
    "    name STRING,\n",
    "    lat DOUBLE,\n",
    "    long DOUBLE,\n",
    "    dockcount INT,\n",
    "    landmark STRING,\n",
    "    installation STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs:///user/hadoop/bikeshare/stations';\n",
    "\n",
    "-- create trips table\n",
    "CREATE EXTERNAL TABLE trips (\n",
    "    trip_id INT,\n",
    "    duration INT,\n",
    "    start_date STRING,\n",
    "    start_station STRING,\n",
    "    start_terminal INT,\n",
    "    end_date STRING,\n",
    "    end_station STRING,\n",
    "    end_terminal INT,\n",
    "    bike_num INT,\n",
    "    subscription_type STRING,\n",
    "    zip_code STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs:///user/hadoop/bikeshare/trips';\n",
    "\n",
    "-- show tables\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/script.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/script.sql\n",
    "\n",
    "USE bikeshare;\n",
    "\n",
    "DESCRIBE stations;\n",
    "DESCRIBE trips;\n",
    "DESCRIBE FORMATTED stations;\n",
    "DESCRIBE FORMATTED trips;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/script.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/script.sql\n",
    "\n",
    "USE bikeshare;\n",
    "\n",
    "-- query - number of trips per terminal\n",
    "SELECT start_terminal, start_station, COUNT(1) AS count\n",
    "FROM trips\n",
    "GROUP BY start_terminal, start_station\n",
    "ORDER BY count\n",
    "DESC LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/script.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/script.sql\n",
    "\n",
    "USE bikeshare;\n",
    "\n",
    "-- query - join between stations and trips\n",
    "SELECT t.trip_id, t.duration, t.start_date, s.name, s.lat, s.long, s.landmark\n",
    "FROM stations s\n",
    "JOIN trips t ON s.station_id = t.start_terminal\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/script.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount using Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir -p /opt/datasets_hive\n",
    "cd /opt/datasets_hive\n",
    "\n",
    "wget -q -c https://tinyurl.com/y68jxy7f -O stop-word-list.csv\n",
    "hdfs dfs -mkdir -p stopwords\n",
    "hdfs dfs -put stop-word-list.csv stopwords\n",
    "hdfs dfs -cat stopwords/stop-word-list.csv\n",
    "\n",
    "# download book \"The Complete Works of William Shakespeare, by William Shakespeare\" from Gutenberg Project\n",
    "wget -q -c http://www.gutenberg.org/files/100/100-0.txt -O shakespeare.txt\n",
    "\n",
    "# create directory in HDFS and put file\n",
    "hdfs dfs -mkdir -p shakespeare\n",
    "hdfs dfs -put shakespeare.txt shakespeare\n",
    "hdfs dfs -ls -h shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/wordcount.sql\n",
    "\n",
    "CREATE TABLE shakespeare_text (line STRING);\n",
    "LOAD DATA INPATH '/user/hadoop/shakespeare/shakespeare.txt' INTO TABLE shakespeare_text;\n",
    "\n",
    "CREATE TABLE stopwords (word STRING);\n",
    "CREATE TABLE tempwords (line STRING);\n",
    "LOAD DATA INPATH '/user/hadoop/stopwords/stop-word-list.csv' INTO TABLE tempwords;\n",
    "\n",
    "-- split comma-separated stopwords to rows\n",
    "INSERT INTO stopwords\n",
    "SELECT word\n",
    "FROM tempwords\n",
    "LATERAL VIEW explode(split(line, ',')) t AS word;\n",
    "DROP TABLE tempwords;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/wordcount.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/wordcount.sql\n",
    "\n",
    "SELECT w.word, count(1) AS count\n",
    "FROM (\n",
    "    SELECT explode(split(regexp_replace(lower(line), '[^a-z\\\\s]', ''), '\\\\s+')) AS word\n",
    "    FROM shakespeare_text\n",
    ") w\n",
    "LEFT OUTER JOIN (\n",
    "    SELECT lower(trim(word)) AS word\n",
    "    FROM stopwords\n",
    ") s ON w.word = s.word\n",
    "WHERE s.word IS NULL AND w.word != ''\n",
    "GROUP BY w.word\n",
    "ORDER BY count DESC\n",
    "LIMIT 30;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000 --silent=true -f /opt/wordcount.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/hive/hiveserver2\n",
    "\n",
    "# kill hiveserver2\n",
    "kill $(cat hiveserver2.pid)\n",
    "rm hiveserver2.pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
