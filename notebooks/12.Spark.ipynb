{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "![Spark](https://spark.apache.org/images/spark-logo-trademark.png)\n",
    "\n",
    "- https://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- version 3.5.0 (Pre-built for Apache Hadoop 3.3 and later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# Download package\n",
    "mkdir -p /opt/pkgs\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf spark-3.5.0-bin-hadoop3.tgz -C /opt\n",
    "ln -s /opt/spark-3.5.0-bin-hadoop3 /opt/spark\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Spark\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PYSPARK_PYTHON=python3\n",
    "export PYSPARK_DRIVER_PYTHON=python3\n",
    "export PYTHONIOENCODING=utf8\n",
    "export PATH=\\${PATH}:\\${SPARK_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Local execution\n",
    "$SPARK_HOME/bin/run-example --master local SparkPi 10 2> /dev/null\n",
    "\n",
    "# Local execution with 4 processes\n",
    "# $SPARK_HOME/bin/run-example --master local[4] SparkPi 10 2> /dev/null\n",
    "\n",
    "# Execution using YARN\n",
    "# $SPARK_HOME/bin/run-example --master yarn SparkPi 10 2> /dev/null\n",
    "\n",
    "# Execution using spark-submit\n",
    "# $SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn \\\n",
    "#  $SPARK_HOME/examples/jars/spark-examples_2.12-3.5.0.jar 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark (interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir -p /opt/src/spark\n",
    "\n",
    "# download book \"The Complete Works of William Shakespeare, by William Shakespeare\" from Gutenberg Project\n",
    "wget -q -c http://www.gutenberg.org/files/100/100-0.txt -O shakespeare.txt\n",
    "\n",
    "# create directory in HDFS and put file\n",
    "hdfs dfs -mkdir -p shakespeare\n",
    "hdfs dfs -put shakespeare.txt shakespeare\n",
    "hdfs dfs -ls -h shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Enter master node using the terminal\n",
    "\n",
    "```bash\n",
    "docker exec -it hadoop /bin/bash\n",
    "```\n",
    "\n",
    "2. Execute pyspark\n",
    "\n",
    "```bash\n",
    "source /opt/envvars.sh\n",
    "pyspark --master local\n",
    "```\n",
    "3. Access Spark application UI\n",
    "\n",
    "- http://localhost:4040\n",
    "\n",
    "4. Write Python code in pyspark\n",
    "\n",
    "```python\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .filter(lambda word: len(word) > 0) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b) \\\n",
    "             .sortBy(lambda word_count: word_count[1], ascending=False)\n",
    "counts.take(30)\n",
    "# counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")\n",
    "```\n",
    "\n",
    "5. Exit from pyspark\n",
    "\n",
    "```python\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spark-submit (batch job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/spark/wordcount.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "text_file = spark.sparkContext.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .filter(lambda word: len(word) > 0) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(add) \\\n",
    "             .sortBy(lambda word_count: word_count[1], ascending=False)\n",
    "\n",
    "# Collect and print the results\n",
    "for word, count in counts.take(30):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "spark.stop()\n",
    "# counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# http://localhost:8088\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/src/spark\n",
    "spark-submit --master yarn wordcount.py # 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir -p /opt/src/spark\n",
    "\n",
    "wget -q -c https://tinyurl.com/y68jxy7f -O stop-word-list.csv\n",
    "hdfs dfs -mkdir -p stopwords\n",
    "hdfs dfs -put stop-word-list.csv stopwords\n",
    "hdfs dfs -cat stopwords/stop-word-list.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/spark/wordcount.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Function to clean text: remove punctuation and control characters\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f'[{string.punctuation}]', '', text)\n",
    "    # Remove control characters\n",
    "    text = re.sub(r'[\\r\\n\\t]', ' ', text)\n",
    "    return text.lower()\n",
    "\n",
    "# Read stopwords from HDFS\n",
    "stopwords_path = \"hdfs:///user/hadoop/stopwords/stop-word-list.csv\"\n",
    "stopwords = sc.textFile(stopwords_path) \\\n",
    "              .flatMap(lambda line: line.split(\",\")) \\\n",
    "              .map(lambda word: word.strip()) \\\n",
    "              .collect()\n",
    "stopwords_broadcast = sc.broadcast(set(stopwords))\n",
    "\n",
    "# Read the Shakespeare text file\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "\n",
    "# Word count excluding stopwords\n",
    "counts = text_file.flatMap(lambda line: clean_text(line).split(\" \")) \\\n",
    "                  .filter(lambda word: word and word not in stopwords_broadcast.value) \\\n",
    "                  .map(lambda word: (word, 1)) \\\n",
    "                  .reduceByKey(add) \\\n",
    "                  .sortBy(lambda word_count: word_count[1], ascending=False)\n",
    "\n",
    "# Save the results to HDFS\n",
    "# counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")\n",
    "\n",
    "# Collect and print the results\n",
    "for word, count in counts.take(30):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/src/spark\n",
    "\n",
    "spark-submit --master local wordcount.py # 2> /dev/null\n",
    "# spark-submit --master yarn wordcount.py # 2> /dev/null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/spark/wordcount_df.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "def main():\n",
    "    # Initialize Spark Session\n",
    "    spark = SparkSession.builder.appName(\"WordCountDataFrame\").getOrCreate()\n",
    "\n",
    "    # Read data from HDFS\n",
    "    text_df = spark.read.text(\"hdfs:///user/hadoop/shakespeare\")\n",
    "\n",
    "    # Split each line into words and create a new DataFrame\n",
    "    words_df = text_df.select(explode(split(col(\"value\"), \"\\\\s+\")).alias(\"word\"))\n",
    "\n",
    "    # Filter out empty strings\n",
    "    filtered_words_df = words_df.filter(words_df.word != \"\")\n",
    "\n",
    "    # Count each word\n",
    "    word_counts = filtered_words_df.groupBy(\"word\").count()\n",
    "\n",
    "    # Sort by count in descending order and take the top 30\n",
    "    top_words = word_counts.sort(col(\"count\").desc()).limit(30)\n",
    "\n",
    "    # Show the results\n",
    "    top_words.show(30)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/src/spark\n",
    "\n",
    "spark-submit --master local wordcount_df.py # 2> /dev/null\n",
    "# spark-submit --master yarn wordcount_df.py # 2> /dev/null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount using SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/spark/wordcount_sql.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "def main():\n",
    "    # Initialize Spark Session\n",
    "    spark = SparkSession.builder.appName(\"WordCountSparkSQL\").getOrCreate()\n",
    "\n",
    "    # Read data from HDFS\n",
    "    text_df = spark.read.text(\"hdfs:///user/hadoop/shakespeare\")\n",
    "\n",
    "    # Split each line into words and create a new DataFrame\n",
    "    words_df = text_df.select(explode(split(col(\"value\"), \"\\\\s+\")).alias(\"word\"))\n",
    "\n",
    "    # Filter out empty strings\n",
    "    filtered_words_df = words_df.filter(words_df.word != \"\")\n",
    "\n",
    "    # Register the DataFrame as a SQL temporary view\n",
    "    filtered_words_df.createOrReplaceTempView(\"words\")\n",
    "\n",
    "    # Perform SQL query to count, sort, and limit the words\n",
    "    top_words = spark.sql(\"\"\"\n",
    "        SELECT word, COUNT(*) as count\n",
    "        FROM words\n",
    "        GROUP BY word\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 30\n",
    "    \"\"\")\n",
    "\n",
    "    # Show the results\n",
    "    top_words.show(30)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/src/spark\n",
    "\n",
    "spark-submit --master local wordcount_sql.py # 2> /dev/null\n",
    "# spark-submit --master yarn wordcount_sql.py # 2> /dev/null"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
