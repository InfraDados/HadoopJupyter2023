{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "![Spark](https://spark.apache.org/images/spark-logo-trademark.png)\n",
    "\n",
    "- https://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- version 3.5.0 (Pre-built for Apache Hadoop 3.3 and later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# Download package\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf spark-3.5.0-bin-hadoop3.tgz -C /opt\n",
    "ln -s /opt/spark-3.5.0-bin-hadoop3 /opt/spark\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Spark\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PYSPARK_PYTHON=python3\n",
    "export PYSPARK_DRIVER_PYTHON=python3\n",
    "export PYTHONIOENCODING=utf8\n",
    "export PATH=\\${PATH}:\\${SPARK_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Local execution\n",
    "# $SPARK_HOME/bin/run-example --master local SparkPi 10 2> /dev/null\n",
    "\n",
    "# Local execution with 4 processes\n",
    "# $SPARK_HOME/bin/run-example --master local[4] SparkPi 10 2> /dev/null\n",
    "\n",
    "# Execution using YARN\n",
    "# $SPARK_HOME/bin/run-example --master yarn SparkPi 10\n",
    "\n",
    "# Execution using spark-submit\n",
    "$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn \\\n",
    " $SPARK_HOME/examples/jars/spark-examples_2.12-3.5.0.jar 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark\n",
    "\n",
    "```bash\n",
    "source /opt/envvars.sh\n",
    "pyspark --master yarn\n",
    "```\n",
    "\n",
    "- Spark application UI - http://localhost:4040\n",
    "\n",
    "```python\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")\n",
    "counts.collect()\n",
    "```\n",
    "\n",
    "```python\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir -p /opt/src/spark\n",
    "\n",
    "wget -q -c https://tinyurl.com/y68jxy7f -O stop-word-list.csv\n",
    "hdfs dfs -mkdir -p stopwords\n",
    "hdfs dfs -put stop-word-list.csv stopwords\n",
    "hdfs dfs -cat stopwords/stop-word-list.csv\n",
    "\n",
    "# download book \"The Complete Works of William Shakespeare, by William Shakespeare\" from Gutenberg Project\n",
    "wget -q -c http://www.gutenberg.org/files/100/100-0.txt -O shakespeare.txt\n",
    "\n",
    "# create directory in HDFS and put file\n",
    "hdfs dfs -mkdir -p shakespeare\n",
    "hdfs dfs -put shakespeare.txt shakespeare\n",
    "hdfs dfs -ls -h shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/spark/wordcount.py\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"WordCount\")\n",
    "\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/spark/wordcount.py\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(\"local\", \"WordCount\")\n",
    "\n",
    "# Function to clean text: remove punctuation and control characters\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f'[{string.punctuation}]', '', text)\n",
    "    # Remove control characters\n",
    "    text = re.sub(r'[\\r\\n\\t]', ' ', text)\n",
    "    return text.lower()\n",
    "\n",
    "# Read stopwords from HDFS\n",
    "stopwords_path = \"hdfs:///user/hadoop/stopwords/stop-word-list.csv\"\n",
    "stopwords = sc.textFile(stopwords_path) \\\n",
    "              .flatMap(lambda line: line.split(\",\")) \\\n",
    "              .map(lambda word: word.strip()) \\\n",
    "              .collect()\n",
    "stopwords_broadcast = sc.broadcast(set(stopwords))\n",
    "\n",
    "# Read the Shakespeare text file\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "\n",
    "# Word count excluding stopwords\n",
    "counts = text_file.flatMap(lambda line: clean_text(line).split(\" \")) \\\n",
    "                  .filter(lambda word: word and word not in stopwords_broadcast.value) \\\n",
    "                  .map(lambda word: (word, 1)) \\\n",
    "                  .reduceByKey(lambda a, b: a + b) \\\n",
    "                  .sortBy(lambda word_count: word_count[1], ascending=False) \\\n",
    "                  .take(30) \n",
    "\n",
    "# Save the results to HDFS\n",
    "# counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")\n",
    "\n",
    "# Print the top 30 words and their counts\n",
    "for word, count in counts:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# pyspark --master yarn\n",
    "\n",
    "hdfs dfs -rm -r /user/hadoop/shakespeare_result\n",
    "\n",
    "cd /opt/src/spark\n",
    "\n",
    "spark-submit --master local wordcount.py 2> /dev/null\n",
    "\n",
    "# pyspark --name WordCount --master local\n",
    "# pyspark --name WordCount --master yarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -ls /user/hadoop/shakespeare_result\n",
    "hdfs dfs -head /user/hadoop/shakespeare_result/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark-pictures\n",
    "\n",
    "- https://github.com/jkthompson/pyspark-pictures/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
