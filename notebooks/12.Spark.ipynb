{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "![Spark](https://spark.apache.org/images/spark-logo-trademark.png)\n",
    "\n",
    "- https://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- version 3.5.0 (Pre-built for Apache Hadoop 3.3 and later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
      "export PDSH_RCMD_TYPE=ssh\n",
      "export HADOOP_HOME=/opt/hadoop\n",
      "export HADOOP_VERSION=3.3.6\n",
      "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
      "export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
      "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
      "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
      "export HADOOP_YARN_HOME=${HADOOP_HOME}\n",
      "export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin\n",
      "# Hive\n",
      "export HIVE_HOME=/opt/hive\n",
      "export PATH=${PATH}:${HIVE_HOME}/bin\n",
      "\n",
      "# Spark\n",
      "export SPARK_HOME=/opt/spark\n",
      "export PYSPARK_PYTHON=python3\n",
      "export PYSPARK_DRIVER_PYTHON=python3\n",
      "export PYTHONIOENCODING=utf8\n",
      "export PATH=${PATH}:${SPARK_HOME}/bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# Download package\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf spark-3.5.0-bin-hadoop3.tgz -C /opt\n",
    "ln -s /opt/spark-3.5.0-bin-hadoop3 /opt/spark\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Spark\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PYSPARK_PYTHON=python3\n",
    "export PYSPARK_DRIVER_PYTHON=python3\n",
    "export PYTHONIOENCODING=utf8\n",
    "export PATH=\\${PATH}:\\${SPARK_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/18 22:32:42 INFO SparkContext: Running Spark version 3.5.0\n",
      "23/12/18 22:32:42 INFO SparkContext: OS info Linux, 6.2.0-1018-azure, amd64\n",
      "23/12/18 22:32:42 INFO SparkContext: Java version 1.8.0_392\n",
      "23/12/18 22:32:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/18 22:32:42 INFO ResourceUtils: ==============================================================\n",
      "23/12/18 22:32:42 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/12/18 22:32:42 INFO ResourceUtils: ==============================================================\n",
      "23/12/18 22:32:42 INFO SparkContext: Submitted application: Spark Pi\n",
      "23/12/18 22:32:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/12/18 22:32:42 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "23/12/18 22:32:42 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/12/18 22:32:42 INFO SecurityManager: Changing view acls to: hadoop\n",
      "23/12/18 22:32:42 INFO SecurityManager: Changing modify acls to: hadoop\n",
      "23/12/18 22:32:42 INFO SecurityManager: Changing view acls groups to: \n",
      "23/12/18 22:32:42 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/12/18 22:32:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY\n",
      "23/12/18 22:32:42 INFO Utils: Successfully started service 'sparkDriver' on port 46281.\n",
      "23/12/18 22:32:42 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/12/18 22:32:43 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/12/18 22:32:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/12/18 22:32:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/12/18 22:32:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/12/18 22:32:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-745b1504-ab0d-482d-9089-580eb2459784\n",
      "23/12/18 22:32:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "23/12/18 22:32:43 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/12/18 22:32:43 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "23/12/18 22:32:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/12/18 22:32:43 INFO SparkContext: Added JAR file:/opt/spark-3.5.0-bin-hadoop3/examples/jars/spark-examples_2.12-3.5.0.jar at spark://hadoop:46281/jars/spark-examples_2.12-3.5.0.jar with timestamp 1702949562512\n",
      "23/12/18 22:32:43 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "23/12/18 22:32:43 INFO AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "23/12/18 22:32:44 INFO Configuration: resource-types.xml not found\n",
      "23/12/18 22:32:44 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "23/12/18 22:32:44 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (1536 MB per container)\n",
      "23/12/18 22:32:44 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "23/12/18 22:32:44 INFO Client: Setting up container launch context for our AM\n",
      "23/12/18 22:32:44 INFO Client: Setting up the launch environment for our AM container\n",
      "23/12/18 22:32:44 INFO Client: Preparing resources for our AM container\n",
      "23/12/18 22:32:44 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/12/18 22:32:49 INFO Client: Uploading resource file:/tmp/spark-7e807492-4d86-458e-87c6-d3a851f8a6ad/__spark_libs__8652811337759006583.zip -> hdfs://hadoop:9000/user/hadoop/.sparkStaging/application_1702949458997_0002/__spark_libs__8652811337759006583.zip\n",
      "23/12/18 22:32:52 INFO Client: Uploading resource file:/tmp/spark-7e807492-4d86-458e-87c6-d3a851f8a6ad/__spark_conf__3624692511838772120.zip -> hdfs://hadoop:9000/user/hadoop/.sparkStaging/application_1702949458997_0002/__spark_conf__.zip\n",
      "23/12/18 22:32:52 INFO SecurityManager: Changing view acls to: hadoop\n",
      "23/12/18 22:32:52 INFO SecurityManager: Changing modify acls to: hadoop\n",
      "23/12/18 22:32:52 INFO SecurityManager: Changing view acls groups to: \n",
      "23/12/18 22:32:52 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/12/18 22:32:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY\n",
      "23/12/18 22:32:52 INFO Client: Submitting application application_1702949458997_0002 to ResourceManager\n",
      "23/12/18 22:32:52 INFO YarnClientImpl: Submitted application application_1702949458997_0002\n",
      "23/12/18 22:32:53 INFO Client: Application report for application_1702949458997_0002 (state: ACCEPTED)\n",
      "23/12/18 22:32:53 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1702949572433\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://hadoop:8088/proxy/application_1702949458997_0002/\n",
      "\t user: hadoop\n",
      "23/12/18 22:33:00 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hadoop, PROXY_URI_BASES -> http://hadoop:8088/proxy/application_1702949458997_0002), /proxy/application_1702949458997_0002\n",
      "23/12/18 22:33:01 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "23/12/18 22:33:01 INFO Client: Application report for application_1702949458997_0002 (state: RUNNING)\n",
      "23/12/18 22:33:01 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.18.0.2\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1702949572433\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://hadoop:8088/proxy/application_1702949458997_0002/\n",
      "\t user: hadoop\n",
      "23/12/18 22:33:01 INFO YarnClientSchedulerBackend: Application application_1702949458997_0002 has started running.\n",
      "23/12/18 22:33:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40599.\n",
      "23/12/18 22:33:01 INFO NettyBlockTransferService: Server created on hadoop:40599\n",
      "23/12/18 22:33:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/12/18 22:33:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop, 40599, None)\n",
      "23/12/18 22:33:01 INFO BlockManagerMasterEndpoint: Registering block manager hadoop:40599 with 366.3 MiB RAM, BlockManagerId(driver, hadoop, 40599, None)\n",
      "23/12/18 22:33:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop, 40599, None)\n",
      "23/12/18 22:33:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop, 40599, None)\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:01 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/12/18 22:33:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:58266) with ID 1,  ResourceProfileId 0\n",
      "23/12/18 22:33:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:59612) with ID 2,  ResourceProfileId 0\n",
      "23/12/18 22:33:10 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "23/12/18 22:33:10 INFO BlockManagerMasterEndpoint: Registering block manager hadoop3:42407 with 366.3 MiB RAM, BlockManagerId(1, hadoop3, 42407, None)\n",
      "23/12/18 22:33:10 INFO BlockManagerMasterEndpoint: Registering block manager hadoop2:45707 with 366.3 MiB RAM, BlockManagerId(2, hadoop2, 45707, None)\n",
      "23/12/18 22:33:11 INFO SparkContext: Starting job: reduce at SparkPi.scala:38\n",
      "23/12/18 22:33:11 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions\n",
      "23/12/18 22:33:11 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n",
      "23/12/18 22:33:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/18 22:33:11 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/18 22:33:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n",
      "23/12/18 22:33:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 366.3 MiB)\n",
      "23/12/18 22:33:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 366.3 MiB)\n",
      "23/12/18 22:33:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop:40599 (size: 2.3 KiB, free: 366.3 MiB)\n",
      "23/12/18 22:33:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/18 22:33:11 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "23/12/18 22:33:11 INFO YarnScheduler: Adding task set 0.0 with 10 tasks resource profile 0\n",
      "23/12/18 22:33:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (hadoop2, executor 2, partition 0, PROCESS_LOCAL, 7933 bytes) \n",
      "23/12/18 22:33:11 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (hadoop3, executor 1, partition 1, PROCESS_LOCAL, 7933 bytes) \n",
      "23/12/18 22:33:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop2:45707 (size: 2.3 KiB, free: 366.3 MiB)\n",
      "23/12/18 22:33:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop3:42407 (size: 2.3 KiB, free: 366.3 MiB)\n",
      "23/12/18 22:33:12 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (hadoop2, executor 2, partition 2, PROCESS_LOCAL, 7933 bytes) \n",
      "23/12/18 22:33:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1048 ms on hadoop2 (executor 2) (1/10)\n",
      "23/12/18 22:33:13 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (hadoop2, executor 2, partition 3, PROCESS_LOCAL, 7933 bytes) \n",
      "23/12/18 22:33:13 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 147 ms on hadoop2 (executor 2) (2/10)\n",
      "23/12/18 22:33:13 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (hadoop2, executor 2, partition 4, PROCESS_LOCAL, 7933 bytes) \n",
      "23/12/18 22:33:13 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 54 ms on hadoop2 (executor 2) (3/10)\n",
      "23/12/18 22:33:13 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (hadoop3, executor 1, partition 5, PROCESS_LOCAL, 7933 bytes) \n",
      "23/12/18 22:33:13 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (hadoop2, executor 2, partition 6, PROCESS_LOCAL, 7933 bytes) \n",
      "23/12/18 22:33:13 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1250 ms on hadoop3 (executor 1) (4/10)\n",
      "23/12/18 22:33:13 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 48 ms on hadoop2 (executor 2) (5/10)\n",
      "23/12/18 22:33:13 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 34 ms on hadoop2 (executor 2) (6/10)\n",
      "23/12/18 22:33:13 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (hadoop2, executor 2, partition 7, PROCESS_LOCAL, 7933 bytes) \n",
      "23/12/18 22:33:13 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (hadoop3, executor 1, partition 8, PROCESS_LOCAL, 7933 bytes) \n",
      "23/12/18 22:33:13 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 44 ms on hadoop3 (executor 1) (7/10)\n",
      "23/12/18 22:33:13 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (hadoop3, executor 1, partition 9, PROCESS_LOCAL, 7933 bytes) \n",
      "23/12/18 22:33:13 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 39 ms on hadoop3 (executor 1) (8/10)\n",
      "23/12/18 22:33:13 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 51 ms on hadoop2 (executor 2) (9/10)\n",
      "23/12/18 22:33:13 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 18 ms on hadoop3 (executor 1) (10/10)\n",
      "23/12/18 22:33:13 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/12/18 22:33:13 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 2.064 s\n",
      "23/12/18 22:33:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/12/18 22:33:13 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "23/12/18 22:33:13 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 2.119698 s\n",
      "Pi is roughly 3.13982713982714\n",
      "23/12/18 22:33:13 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "23/12/18 22:33:13 INFO SparkUI: Stopped Spark web UI at http://hadoop:4040\n",
      "23/12/18 22:33:13 INFO YarnClientSchedulerBackend: Interrupting monitor thread\n",
      "23/12/18 22:33:13 INFO YarnClientSchedulerBackend: Shutting down all executors\n",
      "23/12/18 22:33:13 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n",
      "23/12/18 22:33:13 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n",
      "23/12/18 22:33:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "23/12/18 22:33:13 INFO MemoryStore: MemoryStore cleared\n",
      "23/12/18 22:33:13 INFO BlockManager: BlockManager stopped\n",
      "23/12/18 22:33:13 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "23/12/18 22:33:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "23/12/18 22:33:13 INFO SparkContext: Successfully stopped SparkContext\n",
      "23/12/18 22:33:13 INFO ShutdownHookManager: Shutdown hook called\n",
      "23/12/18 22:33:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e807492-4d86-458e-87c6-d3a851f8a6ad\n",
      "23/12/18 22:33:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-18f8cb9e-027b-449d-8a90-0dce172a7514\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Local execution\n",
    "# $SPARK_HOME/bin/run-example --master local SparkPi 10 2> /dev/null\n",
    "\n",
    "# Local execution with 4 processes\n",
    "# $SPARK_HOME/bin/run-example --master local[4] SparkPi 10 2> /dev/null\n",
    "\n",
    "# Execution using YARN\n",
    "# $SPARK_HOME/bin/run-example --master yarn SparkPi 10\n",
    "\n",
    "# Execution using spark-submit\n",
    "$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn \\\n",
    " $SPARK_HOME/examples/jars/spark-examples_2.12-3.5.0.jar 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark\n",
    "\n",
    "```bash\n",
    "source /opt/envvars.sh\n",
    "pyspark --master yarn\n",
    "```\n",
    "\n",
    "- Spark application UI - http://localhost:4040\n",
    "\n",
    "```python\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")\n",
    "counts.collect()\n",
    "```\n",
    "\n",
    "```python\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a, able, about, across, after, all, almost, also, am, among, an, and, any, are, as, at, be, because, been, but, by, can, cannot, could, dear, did, do, does, either, else, ever, every, for, from, get, got, had, has, have, he, her, hers, him, his, how, however, i, if, in, into, is, it, its, just, least, let, like, likely, may, me, might, most, must, my, neither, no, nor, not, of, off, often, on, only, or, other, our, own, rather, said, say, says, she, should, since, so, some, than, that, the, their, them, then, there, these, they, this, tis, to, too, twas, us, wants, was, we, were, what, when, where, which, while, who, whom, why, will, with, would, yet, you, yourFound 1 items\n",
      "-rw-r--r--   2 hadoop hadoop      5.4 M 2023-12-18 23:08 shakespeare/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir -p /opt/src/spark\n",
    "\n",
    "wget -q -c https://tinyurl.com/y68jxy7f -O stop-word-list.csv\n",
    "hdfs dfs -mkdir -p stopwords\n",
    "hdfs dfs -put stop-word-list.csv stopwords\n",
    "hdfs dfs -cat stopwords/stop-word-list.csv\n",
    "\n",
    "# download book \"The Complete Works of William Shakespeare, by William Shakespeare\" from Gutenberg Project\n",
    "wget -q -c http://www.gutenberg.org/files/100/100-0.txt -O shakespeare.txt\n",
    "\n",
    "# create directory in HDFS and put file\n",
    "hdfs dfs -mkdir -p shakespeare\n",
    "hdfs dfs -put shakespeare.txt shakespeare\n",
    "hdfs dfs -ls -h shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[sPreparing to copy...\u001b[?25l\u001b[u\u001b[2KCopying to container - 0B\u001b[?25h\u001b[u\u001b[2KSuccessfully copied 2.05kB to hadoop:/opt/src/spark/wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%dockerwrite hadoop /opt/src/spark/wordcount.py\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"WordCount\")\n",
    "\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[sPreparing to copy...\u001b[?25l\u001b[u\u001b[2KCopying to container - 0B\u001b[?25h\u001b[u\u001b[2KSuccessfully copied 3.07kB to hadoop:/opt/src/spark/wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%dockerwrite hadoop /opt/src/spark/wordcount.py\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(\"local\", \"WordCount\")\n",
    "\n",
    "# Function to clean text: remove punctuation and control characters\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f'[{string.punctuation}]', '', text)\n",
    "    # Remove control characters\n",
    "    text = re.sub(r'[\\r\\n\\t]', ' ', text)\n",
    "    return text.lower()\n",
    "\n",
    "# Read stopwords from HDFS\n",
    "stopwords_path = \"hdfs:///user/hadoop/stopwords/stop-word-list.csv\"\n",
    "stopwords = sc.textFile(stopwords_path) \\\n",
    "              .flatMap(lambda line: line.split(\",\")) \\\n",
    "              .map(lambda word: word.strip()) \\\n",
    "              .collect()\n",
    "stopwords_broadcast = sc.broadcast(set(stopwords))\n",
    "\n",
    "# Read the Shakespeare text file\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "\n",
    "# Word count excluding stopwords\n",
    "counts = text_file.flatMap(lambda line: clean_text(line).split(\" \")) \\\n",
    "                  .filter(lambda word: word and word not in stopwords_broadcast.value) \\\n",
    "                  .map(lambda word: (word, 1)) \\\n",
    "                  .reduceByKey(lambda a, b: a + b) \\\n",
    "                  .sortBy(lambda word_count: word_count[1], ascending=False) \\\n",
    "                  .take(30) \n",
    "\n",
    "# Save the results to HDFS\n",
    "# counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")\n",
    "\n",
    "# Print the top 30 words and their counts\n",
    "for word, count in counts:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/hadoop/shakespeare_result': No such file or directory\n",
      "thou: 5836\n",
      "thy: 4349\n",
      "shall: 3837\n",
      "thee: 3403\n",
      "lord: 3089\n",
      "king: 3004\n",
      "now: 2996\n",
      "sir: 2952\n",
      "good: 2941\n",
      "come: 2613\n",
      "more: 2505\n",
      "enter: 2408\n",
      "o: 2319\n",
      "well: 2271\n",
      "love: 2271\n",
      "here: 2253\n",
      "hath: 2056\n",
      "one: 1948\n",
      "man: 1901\n",
      "i’ll: 1885\n",
      "upon: 1864\n",
      "make: 1787\n",
      "go: 1785\n",
      "know: 1752\n",
      "scene: 1631\n",
      "see: 1548\n",
      "such: 1531\n",
      "’tis: 1498\n",
      "out: 1459\n",
      "give: 1417\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# pyspark --master yarn\n",
    "\n",
    "hdfs dfs -rm -r /user/hadoop/shakespeare_result\n",
    "\n",
    "cd /opt/src/spark\n",
    "\n",
    "spark-submit --master local wordcount.py 2> /dev/null\n",
    "\n",
    "# pyspark --name WordCount --master local\n",
    "# pyspark --name WordCount --master yarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2023-12-18 23:19 /user/hadoop/shakespeare_result/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop     543162 2023-12-18 23:19 /user/hadoop/shakespeare_result/part-00000\n",
      "('project', 95)\n",
      "('gutenberg', 26)\n",
      "('ebook', 13)\n",
      "('complete', 24)\n",
      "('works', 65)\n",
      "('william', 90)\n",
      "('shakespeare', 10)\n",
      "('use', 360)\n",
      "('anyone', 15)\n",
      "('anywhere', 8)\n",
      "('united', 20)\n",
      "('states', 34)\n",
      "('parts', 121)\n",
      "('world', 671)\n",
      "('cost', 54)\n",
      "('restrictions', 2)\n",
      "('whatsoever', 17)\n",
      "('copy', 24)\n",
      "('give', 1417)\n",
      "('away', 900)\n",
      "('reuse', 2)\n",
      "('under', 316)\n",
      "('terms', 102)\n",
      "('license', 24)\n",
      "('included', 4)\n",
      "('online', 4)\n",
      "('wwwgutenbergorg', 5)\n",
      "('located', 7)\n",
      "('check', 31)\n",
      "('laws', 51)\n",
      "('country', 166)\n",
      "('before', 981)\n",
      "('using', 14)\n",
      "('title', 107)\n",
      "('author', 15)\n",
      "('release', 8)\n",
      "('date', 26)\n",
      "('january', 3)\n",
      "('1994', 1)\n",
      "('100', 3)\n",
      "('recently', 1)\n",
      "('updated', 2)\n",
      "('december', 6)\n",
      "('16', 3)\n",
      "('2023', 1)\n",
      "('language', 44)\n",
      "('english', 166)\n",
      "('start', 39)\n",
      "('contents', 66)\n",
      "('sonnets', 6)\n",
      "('all’s', 44)\n",
      "('well', 2271)\n",
      "('ends', 54)\n",
      "('tragedy', 28)\n",
      "('antony', 508)\n",
      "('cleopatra', 266)\n",
      "('comedy', 12)\n",
      "('errors', 16)\n",
      "('coriolanus', 207)\n",
      "('cymbeline', 110)\n",
      "('hamlet', 459)\n",
      "('prince', 741)\n",
      "('denmark', 26)\n",
      "('first', 1381)\n",
      "('part', 631)\n",
      "('king', 3004)\n",
      "('henry'"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -ls /user/hadoop/shakespeare_result\n",
    "hdfs dfs -head /user/hadoop/shakespeare_result/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark-pictures\n",
    "\n",
    "- https://github.com/jkthompson/pyspark-pictures/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
