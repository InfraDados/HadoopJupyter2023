{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN - Web interface\n",
    "\n",
    "- Master node\n",
    "    - Resource Manager: http://localhost:8088\n",
    "    - Timeline Service: http://localhost:8188\n",
    "- Worker node\n",
    "    - hadoop1\n",
    "        - NodeManager: http://localhost:8042\n",
    "    - hadoop2\n",
    "        - NodeManager: http://localhost:8043\n",
    "    - hadoop3\n",
    "        - NodeManager: http://localhost:8044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop MapReduce Examples\n",
    "\n",
    "```\n",
    "$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar\n",
    "```\n",
    "\n",
    "- aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
    "- aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
    "- bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
    "- dbcount: An example job that count the pageview counts from a database.\n",
    "- distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
    "- grep: A map/reduce program that counts the matches of a regex in the input.\n",
    "- join: A job that effects a join over sorted, equally partitioned datasets\n",
    "- multifilewc: A job that counts words from several files.\n",
    "- pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
    "- pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
    "- randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
    "- randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
    "- secondarysort: An example defining a secondary sort to the reduce.\n",
    "- sort: A map/reduce program that sorts the data written by the random writer.\n",
    "- sudoku: A sudoku solver.\n",
    "- teragen: Generate data for the terasort\n",
    "- terasort: Run the terasort\n",
    "- teravalidate: Checking results of terasort\n",
    "- wordcount: A map/reduce program that counts the words in the input files.\n",
    "- wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
    "- wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
    "- wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 10:03:58,975 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-12-04 10:03:59,133 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "Running 3 maps.\n",
      "Job started: Mon Dec 04 10:03:59 BRT 2023\n",
      "2023-12-04 10:03:59,666 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-12-04 10:03:59,667 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-12-04 10:03:59,817 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701695018387_0001\n",
      "2023-12-04 10:04:00,898 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2023-12-04 10:04:01,048 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701695018387_0001\n",
      "2023-12-04 10:04:01,048 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-12-04 10:04:01,219 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-12-04 10:04:01,219 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-12-04 10:04:01,608 INFO impl.YarnClientImpl: Submitted application application_1701695018387_0001\n",
      "2023-12-04 10:04:01,652 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701695018387_0001/\n",
      "2023-12-04 10:04:01,653 INFO mapreduce.Job: Running job: job_1701695018387_0001\n",
      "2023-12-04 10:04:07,735 INFO mapreduce.Job: Job job_1701695018387_0001 running in uber mode : false\n",
      "2023-12-04 10:04:07,736 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-12-04 10:04:16,879 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2023-12-04 10:04:17,883 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-12-04 10:04:17,888 INFO mapreduce.Job: Job job_1701695018387_0001 completed successfully\n",
      "2023-12-04 10:04:17,949 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=828729\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=360\n",
      "\t\tHDFS: Number of bytes written=157767886\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tOther local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=38804\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=19402\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=19402\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4966912\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=240085\n",
      "\t\tInput split bytes=360\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=592\n",
      "\t\tCPU time spent (ms)=4640\n",
      "\t\tPhysical memory (bytes) snapshot=569589760\n",
      "\t\tVirtual memory (bytes) snapshot=5637918720\n",
      "\t\tTotal committed heap usage (bytes)=268959744\n",
      "\t\tPeak Map Physical memory (bytes)=195395584\n",
      "\t\tPeak Map Virtual memory (bytes)=1879830528\n",
      "\torg.apache.hadoop.examples.RandomTextWriter$Counters\n",
      "\t\tBYTES_WRITTEN=157287716\n",
      "\t\tRECORDS_WRITTEN=240085\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=157767886\n",
      "Job ended: Mon Dec 04 10:04:17 BRT 2023\n",
      "The job took 18 seconds.\n",
      "2023-12-04 10:04:19,818 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-12-04 10:04:19,921 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-12-04 10:04:20,343 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701695018387_0002\n",
      "2023-12-04 10:04:20,622 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2023-12-04 10:04:20,724 INFO mapreduce.JobSubmitter: number of splits:6\n",
      "2023-12-04 10:04:20,860 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701695018387_0002\n",
      "2023-12-04 10:04:20,860 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-12-04 10:04:21,000 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-12-04 10:04:21,000 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-12-04 10:04:21,066 INFO impl.YarnClientImpl: Submitted application application_1701695018387_0002\n",
      "2023-12-04 10:04:21,101 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701695018387_0002/\n",
      "2023-12-04 10:04:21,102 INFO mapreduce.Job: Running job: job_1701695018387_0002\n",
      "2023-12-04 10:04:28,268 INFO mapreduce.Job: Job job_1701695018387_0002 running in uber mode : false\n",
      "2023-12-04 10:04:28,270 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-12-04 10:04:44,471 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2023-12-04 10:04:45,480 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2023-12-04 10:04:46,494 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2023-12-04 10:04:47,502 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-12-04 10:04:49,511 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-12-04 10:04:49,517 INFO mapreduce.Job: Job job_1701695018387_0002 completed successfully\n",
      "2023-12-04 10:04:49,583 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=249867\n",
      "\t\tFILE: Number of bytes written=2334809\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=157780888\n",
      "\t\tHDFS: Number of bytes written=16655\n",
      "\t\tHDFS: Number of read operations=23\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=7\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=7\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=183164\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5334\n",
      "\t\tTotal time spent by all map tasks (ms)=91582\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2667\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=91582\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2667\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=23444992\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=682752\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=240085\n",
      "\t\tMap output records=14759787\n",
      "\t\tMap output bytes=216326864\n",
      "\t\tMap output materialized bytes=149931\n",
      "\t\tInput split bytes=714\n",
      "\t\tCombine input records=14759787\n",
      "\t\tCombine output records=9000\n",
      "\t\tReduce input groups=1000\n",
      "\t\tReduce shuffle bytes=149931\n",
      "\t\tReduce input records=9000\n",
      "\t\tReduce output records=1000\n",
      "\t\tSpilled Records=24000\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=3021\n",
      "\t\tCPU time spent (ms)=31610\n",
      "\t\tPhysical memory (bytes) snapshot=1962987520\n",
      "\t\tVirtual memory (bytes) snapshot=13141266432\n",
      "\t\tTotal committed heap usage (bytes)=1338507264\n",
      "\t\tPeak Map Physical memory (bytes)=299843584\n",
      "\t\tPeak Map Virtual memory (bytes)=1879101440\n",
      "\t\tPeak Reduce Physical memory (bytes)=188280832\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1884160000\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=157780174\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=16655\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar\n",
    "\n",
    "# randomwriter: A map/reduce program that writes 10GB of random data per node\n",
    "# configured for 150MB of random text / 50 MB per map (3 maps)\n",
    "\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar randomtextwriter \\\n",
    "  -D mapreduce.randomtextwriter.totalbytes=157286400 \\\n",
    "  -D mapreduce.randomtextwriter.bytespermap=52428800 \\\n",
    "  -D mapreduce.output.fileoutputformat.compress=false \\\n",
    "  -outFormat org.apache.hadoop.mapreduce.lib.output.TextOutputFormat \\\n",
    "  randomtext\n",
    "\n",
    "# wordcount: A map/reduce program that counts the words in the input files\n",
    "\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar wordcount randomtext randomtextcount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 10:04:55,297 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-12-04 10:04:55,439 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-12-04 10:04:55,632 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701695018387_0003\n",
      "2023-12-04 10:04:55,886 INFO terasort.TeraGen: Generating 2000000 using 2\n",
      "2023-12-04 10:04:55,989 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2023-12-04 10:04:56,133 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701695018387_0003\n",
      "2023-12-04 10:04:56,133 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-12-04 10:04:56,285 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-12-04 10:04:56,286 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-12-04 10:04:56,336 INFO impl.YarnClientImpl: Submitted application application_1701695018387_0003\n",
      "2023-12-04 10:04:56,367 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701695018387_0003/\n",
      "2023-12-04 10:04:56,367 INFO mapreduce.Job: Running job: job_1701695018387_0003\n",
      "2023-12-04 10:05:02,431 INFO mapreduce.Job: Job job_1701695018387_0003 running in uber mode : false\n",
      "2023-12-04 10:05:02,432 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-12-04 10:05:09,506 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-12-04 10:05:10,517 INFO mapreduce.Job: Job job_1701695018387_0003 completed successfully\n",
      "2023-12-04 10:05:10,614 INFO mapreduce.Job: Counters: 34\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=552084\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=167\n",
      "\t\tHDFS: Number of bytes written=200000000\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=20764\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=10382\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10382\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2657792\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2000000\n",
      "\t\tMap output records=2000000\n",
      "\t\tInput split bytes=167\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=170\n",
      "\t\tCPU time spent (ms)=4010\n",
      "\t\tPhysical memory (bytes) snapshot=377880576\n",
      "\t\tVirtual memory (bytes) snapshot=3757436928\n",
      "\t\tTotal committed heap usage (bytes)=188219392\n",
      "\t\tPeak Map Physical memory (bytes)=191066112\n",
      "\t\tPeak Map Virtual memory (bytes)=1878827008\n",
      "\torg.apache.hadoop.examples.terasort.TeraGen$Counters\n",
      "\t\tCHECKSUM=4296625497551440\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=200000000\n",
      "2023-12-04 10:05:11,581 INFO terasort.TeraSort: starting\n",
      "2023-12-04 10:05:12,279 INFO input.FileInputFormat: Total input files to process : 2\n",
      "Spent 115ms computing base-splits.\n",
      "Spent 2ms computing TeraScheduler splits.\n",
      "Computing input splits took 117ms\n",
      "Sampling 6 splits of 6\n",
      "Making 1 from 99996 sampled records\n",
      "Computing parititions took 287ms\n",
      "Spent 407ms computing partitions.\n",
      "2023-12-04 10:05:12,744 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-12-04 10:05:12,861 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-12-04 10:05:12,973 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701695018387_0004\n",
      "2023-12-04 10:05:13,276 INFO mapreduce.JobSubmitter: number of splits:6\n",
      "2023-12-04 10:05:13,428 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701695018387_0004\n",
      "2023-12-04 10:05:13,428 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-12-04 10:05:13,569 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-12-04 10:05:13,569 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-12-04 10:05:13,659 INFO impl.YarnClientImpl: Submitted application application_1701695018387_0004\n",
      "2023-12-04 10:05:13,686 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701695018387_0004/\n",
      "2023-12-04 10:05:13,686 INFO mapreduce.Job: Running job: job_1701695018387_0004\n",
      "2023-12-04 10:05:21,773 INFO mapreduce.Job: Job job_1701695018387_0004 running in uber mode : false\n",
      "2023-12-04 10:05:21,774 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-12-04 10:05:33,968 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "2023-12-04 10:05:34,973 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-12-04 10:05:42,033 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-12-04 10:05:43,044 INFO mapreduce.Job: Job job_1701695018387_0004 completed successfully\n",
      "2023-12-04 10:05:43,114 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=208000036\n",
      "\t\tFILE: Number of bytes written=417942216\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=200000732\n",
      "\t\tHDFS: Number of bytes written=200000000\n",
      "\t\tHDFS: Number of read operations=23\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=6\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=130444\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10414\n",
      "\t\tTotal time spent by all map tasks (ms)=65222\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5207\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=65222\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5207\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16696832\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1332992\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2000000\n",
      "\t\tMap output records=2000000\n",
      "\t\tMap output bytes=204000000\n",
      "\t\tMap output materialized bytes=208000036\n",
      "\t\tInput split bytes=732\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2000000\n",
      "\t\tReduce shuffle bytes=208000036\n",
      "\t\tReduce input records=2000000\n",
      "\t\tReduce output records=2000000\n",
      "\t\tSpilled Records=4000000\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=1309\n",
      "\t\tCPU time spent (ms)=16440\n",
      "\t\tPhysical memory (bytes) snapshot=1955491840\n",
      "\t\tVirtual memory (bytes) snapshot=13146980352\n",
      "\t\tTotal committed heap usage (bytes)=1269825536\n",
      "\t\tPeak Map Physical memory (bytes)=293797888\n",
      "\t\tPeak Map Virtual memory (bytes)=1876447232\n",
      "\t\tPeak Reduce Physical memory (bytes)=204906496\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1892687872\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=200000000\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=200000000\n",
      "2023-12-04 10:05:43,117 INFO terasort.TeraSort: done\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# Teragen - 200MB\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar teragen \\\n",
    "  2000000 \\\n",
    "  teragenoutput\n",
    "\n",
    "# Terasort\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar terasort \\\n",
    "  teragenoutput terasortoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN - CLI\n",
    "\n",
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-yarn/hadoop-yarn-site/YarnCommands.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: yarn [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      " or    yarn [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
      "  where CLASSNAME is a user-provided Java class\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "daemonlog            get/set the log level for each daemon\n",
      "node                 prints node report(s)\n",
      "rmadmin              admin tools\n",
      "scmadmin             SharedCacheManager admin tools\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "applicationattempt   prints applicationattempt(s) report\n",
      "app|application      prints application(s) report/kill application/manage\n",
      "                     long running application\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "cluster              prints cluster information\n",
      "container            prints container(s) report\n",
      "envvars              display computed Hadoop environment variables\n",
      "fs2cs                converts Fair Scheduler configuration to Capacity\n",
      "                     Scheduler (EXPERIMENTAL)\n",
      "jar <jar>            run a jar file\n",
      "logs                 dump container logs\n",
      "nodeattributes       node attributes cli client\n",
      "queue                prints queue information\n",
      "schedulerconf        Updates scheduler configuration\n",
      "timelinereader       run the timeline reader server\n",
      "top                  view cluster information\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "nodemanager          run a nodemanager on each worker\n",
      "proxyserver          run the web app proxy server\n",
      "registrydns          run the registry DNS server\n",
      "resourcemanager      run the ResourceManager\n",
      "router               run the Router daemon\n",
      "sharedcachemanager   run the SharedCacheManager daemon\n",
      "timelineserver       run the timeline server\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "yarn help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List cluster node status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 10:00:35,696 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.21.0.5:8032\n",
      "2023-12-04 10:00:35,860 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.21.0.5:10200\n",
      "2023-12-04 10:00:35,997 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-12-04 10:00:35,998 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "Total Nodes:3\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "   hadoop2:34111\t        RUNNING\t     hadoop2:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1024, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:6432 MB, VMem:6432 MB, VCores:0.24333334\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n",
      "   hadoop1:44221\t        RUNNING\t     hadoop1:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1024, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:6432 MB, VMem:6432 MB, VCores:0.23325558\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n",
      "   hadoop3:34817\t        RUNNING\t     hadoop3:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1024, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:6432 MB, VMem:6432 MB, VCores:0.22659113\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "yarn node -all -list -showDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 07:54:46,245 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.21.0.5:8032\n",
      "2023-12-04 07:54:46,417 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.21.0.5:10200\n",
      "Total number of applications (application-types: [], states: [NEW, NEW_SAVING, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED] and tags: []):4\n",
      "                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\n",
      "application_1701686739457_0002\t          word count\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop1:19888/jobhistory/job/job_1701686739457_0002\n",
      "application_1701686739457_0003\t             TeraGen\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop1:19888/jobhistory/job/job_1701686739457_0003\n",
      "application_1701686739457_0004\t            TeraSort\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop3:19888/jobhistory/job/job_1701686739457_0004\n",
      "application_1701686739457_0001\t  random-text-writer\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop3:19888/jobhistory/job/job_1701686739457_0001\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Current applications (running or accepted)\n",
    "# yarn app -list\n",
    "# Applications already executed\n",
    "yarn app -list -appStates ALL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List queue status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 07:55:03,745 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.21.0.5:8032\n",
      "2023-12-04 07:55:03,904 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.21.0.5:10200\n",
      "Queue Information : \n",
      "Queue Name : default\n",
      "\tState : RUNNING\n",
      "\tCapacity : 100.00%\n",
      "\tCurrent Capacity : .00%\n",
      "\tMaximum Capacity : 100.00%\n",
      "\tDefault Node Label expression : <DEFAULT_PARTITION>\n",
      "\tAccessible Node Labels : *\n",
      "\tPreemption : disabled\n",
      "\tIntra-queue Preemption : disabled\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "yarn queue -status default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get application log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting logs for application_1701686739457_0001\n",
      "Container: container_1701686739457_0001_01_000003 on hadoop1_37989\n",
      "LogAggregationType: AGGREGATED\n",
      "==================================================================\n",
      "LogType:directory.info\n",
      "LogLastModifiedTime:Mon Dec 04 07:48:34 -0300 2023\n",
      "LogLength:1989\n",
      "LogContents:\n",
      "ls -l:\n",
      "total 32\n",
      "-rw-r--r-- 1 hadoop hadoop  129 Dec  4 07:48 container_tokens\n",
      "-rwx------ 1 hadoop hadoop  733 Dec  4 07:48 default_container_executor.sh\n",
      "-rwx------ 1 hadoop hadoop  678 Dec  4 07:48 default_container_executor_session.sh\n",
      "lrwxrwxrwx 1 hadoop hadoop  109 Dec  4 07:48 job.jar -> /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1701686739457_0001/filecache/10/job.jar\n",
      "lrwxrwxrwx 1 hadoop hadoop  109 Dec  4 07:48 job.xml -> /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1701686739457_0001/filecache/11/job.xml\n",
      "-rwx------ 1 hadoop hadoop 5108 Dec  4 07:48 launch_container.sh\n",
      "drwx--x--- 2 hadoop hadoop 4096 Dec  4 07:48 tmp\n",
      "find -L . -maxdepth 5 -ls:\n",
      "  1499357      4 drwx--x---   3 hadoop   hadoop       4096 Dec  4 07:48 .\n",
      "  1499364      4 -rw-r--r--   1 hadoop   hadoop         48 Dec  4 07:48 ./.launch_container.sh.crc\n",
      "  1499363      8 -rwx------   1 hadoop   hadoop       5108 Dec  4 07:48 ./launch_container.sh\n",
      "  1499361      4 -rw-r--r--   1 hadoop   hadoop         12 Dec  4 07:48 ./.container_tokens.crc\n",
      "  1499366      4 -rw-r--r--   1 hadoop   hadoop         16 Dec  4 07:48 ./.default_container_executor_session.sh.crc\n",
      "  1499370      4 -rw-r--r--   1 hadoop   hadoop         16 Dec  4 07:48 ./.default_container_executor.sh.crc\n",
      "  1499360      4 -rw-r--r--   1 hadoop   hadoop        129 Dec  4 07:48 ./container_tokens\n",
      "  1499326      4 drwx------   2 hadoop   hadoop       4096 Dec  4 07:48 ./job.jar\n",
      "  1499327    276 -r-x------   1 hadoop   hadoop     281348 Dec  4 07:48 ./job.jar/job.jar\n",
      "  1499369      4 -rwx------   1 hadoop   hadoop        733 Dec  4 07:48 ./default_container_executor.sh\n",
      "  1499332    232 -r-x------   1 hadoop   hadoop     236349 Dec  4 07:48 ./job.xml\n",
      "  1499365      4 -rwx------   1 hadoop   hadoop        678 Dec  4 07:48 ./default_container_executor_session.sh\n",
      "  1499359      4 drwx--x---   2 hadoop   hadoop       4096 Dec  4 07:48 ./tmp\n",
      "broken symlinks(find -L . -maxdepth 5 -type l -ls):\n",
      "\n",
      "End of LogType:directory.info\n",
      "*******************************************************************************\n",
      "\n",
      "Container: container_1701686739457_0001_01_000003 on hadoop1_37989\n",
      "LogAggregationType: AGGREGATED\n",
      "==================================================================\n",
      "LogType:launch_container.sh\n",
      "LogLastModifiedTime:Mon Dec 04 07:48:34 -0300 2023\n",
      "LogLength:5108\n",
      "LogContents:\n",
      "#!/bin/bash\n",
      "\n",
      "set -o pipefail -e\n",
      "export PRELAUNCH_OUT=\"/opt/hadoop-3.3.6/logs/userlogs/application_1701686739457_0001/container_1701686739457_0001_01_000003/prelaunch.out\"\n",
      "exec >\"${PRELAUNCH_OUT}\"\n",
      "export PRELAUNCH_ERR=\"/opt/hadoop-3.3.6/logs/userlogs/application_1701686739457_0001/container_1701686739457_0001_01_000003/prelaunch.err\"\n",
      "exec 2>\"${PRELAUNCH_ERR}\"\n",
      "echo \"Setting up env variables\"\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "APPID=$(yarn application -list -appStates FINISHED 2>/dev/null | tail -1 | awk '{ print $1 }')\n",
    "echo \"Getting logs for $APPID\"\n",
    "yarn logs -applicationId $APPID 2>/dev/null | head -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kill application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run application in background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Maps  = 24\n",
      "Samples per Map = 1000000\n",
      "Wrote input for Map #0\n",
      "Wrote input for Map #1\n",
      "Wrote input for Map #2\n",
      "Wrote input for Map #3\n",
      "Wrote input for Map #4\n",
      "Wrote input for Map #5\n",
      "Wrote input for Map #6\n",
      "Wrote input for Map #7\n",
      "Wrote input for Map #8\n",
      "Wrote input for Map #9\n",
      "Wrote input for Map #10\n",
      "Wrote input for Map #11\n",
      "Wrote input for Map #12\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Run application in background\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar pi 24 1000000 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killing application_1701686739457_0005\n",
      "2023-12-04 07:56:53,654 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.21.0.5:8032\n",
      "2023-12-04 07:56:54,295 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.21.0.5:10200\n",
      "Killing application application_1701686739457_0005\n",
      "2023-12-04 07:56:54,913 INFO impl.YarnClientImpl: Killed application application_1701686739457_0005\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "APPID=$(yarn application -list -appStates RUNNING 2>/dev/null | tail -1 | awk '{ print $1 }')\n",
    "echo \"Killing $APPID\"\n",
    "yarn application -kill $APPID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling nodemanager failures\n",
    "\n",
    "- yarn.nm.liveness-monitor.expiry-interval-ms property in yarn-site.xml\n",
    " - set to 10000 (10 seconds) / default is 600000 (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.21.0.3' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# simulate node fault\n",
    "ssh hadoop1 'kill -9 $(cat /tmp/hadoop-hadoop-nodemanager.pid)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8088/cluster/nodes\n",
    "- Wait 10 seconds to discover node failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 08:03:01,681 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.21.0.5:8032\n",
      "2023-12-04 08:03:01,840 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.21.0.5:10200\n",
      "2023-12-04 08:03:01,988 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-12-04 08:03:01,988 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "Total Nodes:3\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "   hadoop1:35481\t        RUNNING\t     hadoop1:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1024, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:9359 MB, VMem:9359 MB, VCores:0.19993335\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n",
      "   hadoop2:42659\t        RUNNING\t     hadoop2:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1024, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:9421 MB, VMem:9421 MB, VCores:0.84\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n",
      "   hadoop3:35627\t        RUNNING\t     hadoop3:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1024, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:9370 MB, VMem:9370 MB, VCores:0.23\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "yarn node -list -showDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.21.0.3' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Restart nodemanager\n",
    "ssh hadoop1 /opt/hadoop/bin/yarn --daemon start nodemanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
