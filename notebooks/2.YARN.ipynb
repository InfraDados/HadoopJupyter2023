{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN - Web interface\n",
    "\n",
    "- Master node\n",
    "    - Resource Manager: http://localhost:8088\n",
    "    - Timeline Service: http://localhost:8188\n",
    "- Worker node\n",
    "    - hadoop1\n",
    "        - NodeManager: http://localhost:8042\n",
    "    - hadoop2\n",
    "        - NodeManager: http://localhost:8043\n",
    "    - hadoop3\n",
    "        - NodeManager: http://localhost:8044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop MapReduce Examples\n",
    "\n",
    "```\n",
    "$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar\n",
    "```\n",
    "\n",
    "- aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
    "- aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
    "- bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
    "- dbcount: An example job that count the pageview counts from a database.\n",
    "- distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
    "- grep: A map/reduce program that counts the matches of a regex in the input.\n",
    "- join: A job that effects a join over sorted, equally partitioned datasets\n",
    "- multifilewc: A job that counts words from several files.\n",
    "- pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
    "- pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
    "- randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
    "- randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
    "- secondarysort: An example defining a secondary sort to the reduce.\n",
    "- sort: A map/reduce program that sorts the data written by the random writer.\n",
    "- sudoku: A sudoku solver.\n",
    "- teragen: Generate data for the terasort\n",
    "- terasort: Run the terasort\n",
    "- teravalidate: Checking results of terasort\n",
    "- wordcount: A map/reduce program that counts the words in the input files.\n",
    "- wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
    "- wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
    "- wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-30 16:24:55,475 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:24:55,618 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "Running 3 maps.\n",
      "Job started: Thu Nov 30 16:24:56 BRT 2023\n",
      "2023-11-30 16:24:56,168 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:24:56,169 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-11-30 16:24:56,304 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701371922987_0002\n",
      "2023-11-30 16:24:56,708 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2023-11-30 16:24:56,853 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701371922987_0002\n",
      "2023-11-30 16:24:56,853 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-11-30 16:24:56,989 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-11-30 16:24:56,989 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-11-30 16:24:57,034 INFO impl.YarnClientImpl: Submitted application application_1701371922987_0002\n",
      "2023-11-30 16:24:57,066 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701371922987_0002/\n",
      "2023-11-30 16:24:57,067 INFO mapreduce.Job: Running job: job_1701371922987_0002\n",
      "2023-11-30 16:25:02,126 INFO mapreduce.Job: Job job_1701371922987_0002 running in uber mode : false\n",
      "2023-11-30 16:25:02,127 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-11-30 16:25:10,245 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2023-11-30 16:25:11,248 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-11-30 16:25:11,254 INFO mapreduce.Job: Job job_1701371922987_0002 completed successfully\n",
      "2023-11-30 16:25:11,359 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=828729\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=360\n",
      "\t\tHDFS: Number of bytes written=157768116\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tOther local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36822\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=18411\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18411\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4713216\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=240083\n",
      "\t\tInput split bytes=360\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=672\n",
      "\t\tCPU time spent (ms)=4570\n",
      "\t\tPhysical memory (bytes) snapshot=561250304\n",
      "\t\tVirtual memory (bytes) snapshot=5636550656\n",
      "\t\tTotal committed heap usage (bytes)=266338304\n",
      "\t\tPeak Map Physical memory (bytes)=189210624\n",
      "\t\tPeak Map Virtual memory (bytes)=1879678976\n",
      "\torg.apache.hadoop.examples.RandomTextWriter$Counters\n",
      "\t\tBYTES_WRITTEN=157287950\n",
      "\t\tRECORDS_WRITTEN=240083\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=157768116\n",
      "Job ended: Thu Nov 30 16:25:11 BRT 2023\n",
      "The job took 15 seconds.\n",
      "2023-11-30 16:25:13,261 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:25:13,359 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-11-30 16:25:13,845 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701371922987_0003\n",
      "2023-11-30 16:25:14,118 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2023-11-30 16:25:14,632 INFO mapreduce.JobSubmitter: number of splits:6\n",
      "2023-11-30 16:25:14,753 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701371922987_0003\n",
      "2023-11-30 16:25:14,753 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-11-30 16:25:14,884 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-11-30 16:25:14,884 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-11-30 16:25:14,923 INFO impl.YarnClientImpl: Submitted application application_1701371922987_0003\n",
      "2023-11-30 16:25:14,949 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701371922987_0003/\n",
      "2023-11-30 16:25:14,950 INFO mapreduce.Job: Running job: job_1701371922987_0003\n",
      "2023-11-30 16:25:22,073 INFO mapreduce.Job: Job job_1701371922987_0003 running in uber mode : false\n",
      "2023-11-30 16:25:22,074 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-11-30 16:25:38,264 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "2023-11-30 16:25:39,274 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2023-11-30 16:25:41,293 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "2023-11-30 16:25:42,300 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-11-30 16:25:43,303 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-11-30 16:25:43,309 INFO mapreduce.Job: Job job_1701371922987_0003 completed successfully\n",
      "2023-11-30 16:25:43,382 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=249867\n",
      "\t\tFILE: Number of bytes written=2334809\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=157781118\n",
      "\t\tHDFS: Number of bytes written=16655\n",
      "\t\tHDFS: Number of read operations=23\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=7\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=6\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=190248\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5552\n",
      "\t\tTotal time spent by all map tasks (ms)=95124\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2776\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=95124\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2776\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=24351744\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=710656\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=240083\n",
      "\t\tMap output records=14760268\n",
      "\t\tMap output bytes=216329022\n",
      "\t\tMap output materialized bytes=149931\n",
      "\t\tInput split bytes=714\n",
      "\t\tCombine input records=14760268\n",
      "\t\tCombine output records=9000\n",
      "\t\tReduce input groups=1000\n",
      "\t\tReduce shuffle bytes=149931\n",
      "\t\tReduce input records=9000\n",
      "\t\tReduce output records=1000\n",
      "\t\tSpilled Records=24000\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=2955\n",
      "\t\tCPU time spent (ms)=32150\n",
      "\t\tPhysical memory (bytes) snapshot=1989832704\n",
      "\t\tVirtual memory (bytes) snapshot=13142077440\n",
      "\t\tTotal committed heap usage (bytes)=1349517312\n",
      "\t\tPeak Map Physical memory (bytes)=300920832\n",
      "\t\tPeak Map Virtual memory (bytes)=1879670784\n",
      "\t\tPeak Reduce Physical memory (bytes)=193753088\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1884893184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=157780404\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=16655\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar\n",
    "\n",
    "# randomwriter: A map/reduce program that writes 10GB of random data per node\n",
    "# configured for 150MB of random text / 50 MB per map (3 maps)\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar randomtextwriter \\\n",
    "  -D mapreduce.randomtextwriter.totalbytes=157286400 \\\n",
    "  -D mapreduce.randomtextwriter.bytespermap=52428800 \\\n",
    "  -D mapreduce.output.fileoutputformat.compress=false \\\n",
    "  -outFormat org.apache.hadoop.mapreduce.lib.output.TextOutputFormat \\\n",
    "  randomtext\n",
    "\n",
    "# wordcount: A map/reduce program that counts the words in the input files\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar wordcount randomtext randomtextcount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-30 16:26:08,564 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:26:08,662 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-11-30 16:26:08,849 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701371922987_0004\n",
      "2023-11-30 16:26:09,097 INFO terasort.TeraGen: Generating 2000000 using 2\n",
      "2023-11-30 16:26:09,194 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2023-11-30 16:26:09,342 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701371922987_0004\n",
      "2023-11-30 16:26:09,343 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-11-30 16:26:09,475 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-11-30 16:26:09,475 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-11-30 16:26:09,518 INFO impl.YarnClientImpl: Submitted application application_1701371922987_0004\n",
      "2023-11-30 16:26:09,542 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701371922987_0004/\n",
      "2023-11-30 16:26:09,543 INFO mapreduce.Job: Running job: job_1701371922987_0004\n",
      "2023-11-30 16:26:14,599 INFO mapreduce.Job: Job job_1701371922987_0004 running in uber mode : false\n",
      "2023-11-30 16:26:14,600 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-11-30 16:26:21,705 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-11-30 16:26:22,717 INFO mapreduce.Job: Job job_1701371922987_0004 completed successfully\n",
      "2023-11-30 16:26:22,859 INFO mapreduce.Job: Counters: 34\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=552084\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=167\n",
      "\t\tHDFS: Number of bytes written=200000000\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=21350\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=10675\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10675\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2732800\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2000000\n",
      "\t\tMap output records=2000000\n",
      "\t\tInput split bytes=167\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=189\n",
      "\t\tCPU time spent (ms)=4010\n",
      "\t\tPhysical memory (bytes) snapshot=383606784\n",
      "\t\tVirtual memory (bytes) snapshot=3759435776\n",
      "\t\tTotal committed heap usage (bytes)=181927936\n",
      "\t\tPeak Map Physical memory (bytes)=193552384\n",
      "\t\tPeak Map Virtual memory (bytes)=1880584192\n",
      "\torg.apache.hadoop.examples.terasort.TeraGen$Counters\n",
      "\t\tCHECKSUM=4296625497551440\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=200000000\n",
      "2023-11-30 16:26:23,777 INFO terasort.TeraSort: starting\n",
      "2023-11-30 16:26:24,588 INFO input.FileInputFormat: Total input files to process : 2\n",
      "Spent 139ms computing base-splits.\n",
      "Spent 1ms computing TeraScheduler splits.\n",
      "Computing input splits took 141ms\n",
      "Sampling 6 splits of 6\n",
      "Making 1 from 99996 sampled records\n",
      "Computing parititions took 325ms\n",
      "Spent 467ms computing partitions.\n",
      "2023-11-30 16:26:25,129 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:26:25,258 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-11-30 16:26:25,365 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701371922987_0005\n",
      "2023-11-30 16:26:25,622 INFO mapreduce.JobSubmitter: number of splits:6\n",
      "2023-11-30 16:26:25,747 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701371922987_0005\n",
      "2023-11-30 16:26:25,747 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-11-30 16:26:25,895 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-11-30 16:26:25,896 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-11-30 16:26:25,978 INFO impl.YarnClientImpl: Submitted application application_1701371922987_0005\n",
      "2023-11-30 16:26:26,005 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701371922987_0005/\n",
      "2023-11-30 16:26:26,006 INFO mapreduce.Job: Running job: job_1701371922987_0005\n",
      "2023-11-30 16:26:33,076 INFO mapreduce.Job: Job job_1701371922987_0005 running in uber mode : false\n",
      "2023-11-30 16:26:33,077 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-11-30 16:26:45,293 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "2023-11-30 16:26:46,296 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-11-30 16:26:52,340 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-11-30 16:26:54,351 INFO mapreduce.Job: Job job_1701371922987_0005 completed successfully\n",
      "2023-11-30 16:26:54,414 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=208000036\n",
      "\t\tFILE: Number of bytes written=417942216\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=200000732\n",
      "\t\tHDFS: Number of bytes written=200000000\n",
      "\t\tHDFS: Number of read operations=23\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=6\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=123020\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10262\n",
      "\t\tTotal time spent by all map tasks (ms)=61510\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5131\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=61510\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5131\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15746560\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1313536\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2000000\n",
      "\t\tMap output records=2000000\n",
      "\t\tMap output bytes=204000000\n",
      "\t\tMap output materialized bytes=208000036\n",
      "\t\tInput split bytes=732\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2000000\n",
      "\t\tReduce shuffle bytes=208000036\n",
      "\t\tReduce input records=2000000\n",
      "\t\tReduce output records=2000000\n",
      "\t\tSpilled Records=4000000\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=1144\n",
      "\t\tCPU time spent (ms)=15380\n",
      "\t\tPhysical memory (bytes) snapshot=1936891904\n",
      "\t\tVirtual memory (bytes) snapshot=13136588800\n",
      "\t\tTotal committed heap usage (bytes)=1271398400\n",
      "\t\tPeak Map Physical memory (bytes)=294211584\n",
      "\t\tPeak Map Virtual memory (bytes)=1876762624\n",
      "\t\tPeak Reduce Physical memory (bytes)=190324736\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1884094464\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=200000000\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=200000000\n",
      "2023-11-30 16:26:54,415 INFO terasort.TeraSort: done\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# Teragen - 200MB\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar teragen \\\n",
    "  2000000 \\\n",
    "  teragenoutput\n",
    "\n",
    "# Terasort\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar terasort \\\n",
    "  teragenoutput terasortoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN - CLI\n",
    "\n",
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-yarn/hadoop-yarn-site/YarnCommands.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: yarn [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      " or    yarn [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
      "  where CLASSNAME is a user-provided Java class\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "daemonlog            get/set the log level for each daemon\n",
      "node                 prints node report(s)\n",
      "rmadmin              admin tools\n",
      "scmadmin             SharedCacheManager admin tools\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "applicationattempt   prints applicationattempt(s) report\n",
      "app|application      prints application(s) report/kill application/manage\n",
      "                     long running application\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "cluster              prints cluster information\n",
      "container            prints container(s) report\n",
      "envvars              display computed Hadoop environment variables\n",
      "fs2cs                converts Fair Scheduler configuration to Capacity\n",
      "                     Scheduler (EXPERIMENTAL)\n",
      "jar <jar>            run a jar file\n",
      "logs                 dump container logs\n",
      "nodeattributes       node attributes cli client\n",
      "queue                prints queue information\n",
      "schedulerconf        Updates scheduler configuration\n",
      "timelinereader       run the timeline reader server\n",
      "top                  view cluster information\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "nodemanager          run a nodemanager on each worker\n",
      "proxyserver          run the web app proxy server\n",
      "registrydns          run the registry DNS server\n",
      "resourcemanager      run the ResourceManager\n",
      "router               run the Router daemon\n",
      "sharedcachemanager   run the SharedCacheManager daemon\n",
      "timelineserver       run the timeline server\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "yarn help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List cluster node status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-30 16:28:48,473 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:28:48,623 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-11-30 16:28:48,762 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-11-30 16:28:48,763 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "Total Nodes:3\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "   hadoop2:38671\t        RUNNING\t     hadoop2:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1024, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:8917 MB, VMem:8917 MB, VCores:0.26333335\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n",
      "   hadoop3:38769\t        RUNNING\t     hadoop3:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1024, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:8914 MB, VMem:8914 MB, VCores:0.42\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n",
      "   hadoop1:35235\t        RUNNING\t     hadoop1:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1024, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:8917 MB, VMem:8917 MB, VCores:0.32333332\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "yarn node -all -list -showDetails\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-30 16:29:00,193 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:29:00,353 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "Total number of applications (application-types: [], states: [NEW, NEW_SAVING, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED] and tags: []):5\n",
      "                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\n",
      "application_1701371922987_0004\t             TeraGen\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop2:19888/jobhistory/job/job_1701371922987_0004\n",
      "application_1701371922987_0005\t            TeraSort\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop3:19888/jobhistory/job/job_1701371922987_0005\n",
      "application_1701371922987_0002\t  random-text-writer\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop1:19888/jobhistory/job/job_1701371922987_0002\n",
      "application_1701371922987_0003\t          word count\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop3:19888/jobhistory/job/job_1701371922987_0003\n",
      "application_1701371922987_0001\t     QuasiMonteCarlo\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop1:19888/jobhistory/job/job_1701371922987_0001\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Current applications (running or accepted)\n",
    "# yarn app -list\n",
    "# Applications already executed\n",
    "yarn app -list -appStates ALL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List queue status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-30 16:30:50,546 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:30:50,716 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "Queue Information : \n",
      "Queue Name : default\n",
      "\tState : RUNNING\n",
      "\tCapacity : 100.00%\n",
      "\tCurrent Capacity : .00%\n",
      "\tMaximum Capacity : 100.00%\n",
      "\tDefault Node Label expression : <DEFAULT_PARTITION>\n",
      "\tAccessible Node Labels : *\n",
      "\tPreemption : disabled\n",
      "\tIntra-queue Preemption : disabled\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "yarn queue -status default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get application log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting logs for application_1701371922987_0001\n",
      "Container: container_1701371922987_0001_01_000001 on hadoop1_35235\n",
      "LogAggregationType: AGGREGATED\n",
      "==================================================================\n",
      "LogType:directory.info\n",
      "LogLastModifiedTime:Thu Nov 30 16:21:08 -0300 2023\n",
      "LogLength:2336\n",
      "LogContents:\n",
      "ls -l:\n",
      "total 36\n",
      "-rw-r--r-- 1 hadoop hadoop  105 Nov 30 16:20 container_tokens\n",
      "-rwx------ 1 hadoop hadoop  733 Nov 30 16:20 default_container_executor.sh\n",
      "-rwx------ 1 hadoop hadoop  678 Nov 30 16:20 default_container_executor_session.sh\n",
      "lrwxrwxrwx 1 hadoop hadoop  109 Nov 30 16:20 job.jar -> /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1701371922987_0001/filecache/11/job.jar\n",
      "lrwxrwxrwx 1 hadoop hadoop  109 Nov 30 16:20 job.xml -> /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1701371922987_0001/filecache/13/job.xml\n",
      "drwxrwxr-x 2 hadoop hadoop 4096 Nov 30 16:20 jobSubmitDir\n",
      "-rwx------ 1 hadoop hadoop 5130 Nov 30 16:20 launch_container.sh\n",
      "drwx--x--- 2 hadoop hadoop 4096 Nov 30 16:20 tmp\n",
      "find -L . -maxdepth 5 -ls:\n",
      "  1856116      4 drwx--x---   4 hadoop   hadoop       4096 Nov 30 16:20 .\n",
      "  1856102      4 drwx------   2 hadoop   hadoop       4096 Nov 30 16:20 ./job.jar\n",
      "  1856103    276 -r-x------   1 hadoop   hadoop     281348 Nov 30 16:20 ./job.jar/job.jar\n",
      "  1856126      4 -rw-r--r--   1 hadoop   hadoop         16 Nov 30 16:20 ./.default_container_executor.sh.crc\n",
      "  1856118      4 drwx--x---   2 hadoop   hadoop       4096 Nov 30 16:20 ./tmp\n",
      "  1856124      4 -rw-r--r--   1 hadoop   hadoop         16 Nov 30 16:20 ./.default_container_executor_session.sh.crc\n",
      "  1856125      4 -rwx------   1 hadoop   hadoop        733 Nov 30 16:20 ./default_container_executor.sh\n",
      "  1856119      4 -rw-r--r--   1 hadoop   hadoop        105 Nov 30 16:20 ./container_tokens\n",
      "  1856123      4 -rwx------   1 hadoop   hadoop        678 Nov 30 16:20 ./default_container_executor_session.sh\n",
      "  1856130      4 drwxrwxr-x   2 hadoop   hadoop       4096 Nov 30 16:20 ./jobSubmitDir\n",
      "  1856105      4 -r-x------   1 hadoop   hadoop        877 Nov 30 16:20 ./jobSubmitDir/job.split\n",
      "  1856098      4 -r-x------   1 hadoop   hadoop        133 Nov 30 16:20 ./jobSubmitDir/job.splitmetainfo\n",
      "  1856122      4 -rw-r--r--   1 hadoop   hadoop         52 Nov 30 16:20 ./.launch_container.sh.crc\n",
      "  1856121      8 -rwx------   1 hadoop   hadoop       5130 Nov 30 16:20 ./launch_container.sh\n",
      "  1856120      4 -rw-r--r--   1 hadoop   hadoop         12 Nov 30 16:20 ./.container_tokens.crc\n",
      "  1856108    232 -r-x------   1 hadoop   hadoop     236779 Nov 30 16:20 ./job.xml\n",
      "broken symlinks(find -L . -maxdepth 5 -type l -ls):\n",
      "\n",
      "End of LogType:directory.info\n",
      "*******************************************************************************\n",
      "\n",
      "Container: container_1701371922987_0001_01_000001 on hadoop1_35235\n",
      "LogAggregationType: AGGREGATED\n",
      "==================================================================\n",
      "LogType:launch_container.sh\n",
      "LogLastModifiedTime:Thu Nov 30 16:21:08 -0300 2023\n",
      "LogLength:5130\n",
      "LogContents:\n",
      "#!/bin/bash\n",
      "\n",
      "set -o pipefail -e\n",
      "export PRELAUNCH_OUT=\"/opt/hadoop-3.3.6/logs/userlogs/application_1701371922987_0001/container_1701371922987_0001_01_000001/prelaunch.out\"\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "APPID=$(yarn application -list -appStates FINISHED 2>/dev/null | tail -1 | awk '{ print $1 }')\n",
    "echo \"Getting logs for $APPID\"\n",
    "yarn logs -applicationId $APPID 2>/dev/null | head -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kill application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killing Application-Id\n",
      "2023-11-30 16:32:52,112 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:32:52,249 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "Exception in thread \"main\" java.lang.IllegalArgumentException: Invalid ApplicationId prefix: Application-Id. The valid ApplicationId should start with prefix application\n",
      "\tat org.apache.hadoop.yarn.api.records.ApplicationId.fromString(ApplicationId.java:112)\n",
      "\tat org.apache.hadoop.yarn.client.cli.ApplicationCLI.killApplication(ApplicationCLI.java:539)\n",
      "\tat org.apache.hadoop.yarn.client.cli.ApplicationCLI.killApplication(ApplicationCLI.java:519)\n",
      "\tat org.apache.hadoop.yarn.client.cli.ApplicationCLI.executeKillCommand(ApplicationCLI.java:1172)\n",
      "\tat org.apache.hadoop.yarn.client.cli.ApplicationCLI.run(ApplicationCLI.java:186)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)\n",
      "\tat org.apache.hadoop.yarn.client.cli.ApplicationCLI.main(ApplicationCLI.java:128)\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "APPID=$(yarn application -list -appStates RUNNING 2>/dev/null | tail -1 | awk '{ print $1 }')\n",
    "echo \"Killing $APPID\"\n",
    "yarn application -kill $APPID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling nodemanager failures\n",
    "\n",
    "- yarn.nm.liveness-monitor.expiry-interval-ms property in yarn-site.xml\n",
    " - set to 10000 (10 seconds) / default is 600000 (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.18.0.2' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# simulate node fault\n",
    "ssh hadoop1 'kill -9 $(cat /tmp/hadoop-hadoop-nodemanager.pid)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8088/cluster/nodes\n",
    "- Wait 10 seconds to discover node failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-30 16:33:43,660 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:33:43,798 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-11-30 16:33:43,914 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-11-30 16:33:43,915 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "Total Nodes:2\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "   hadoop2:38671\t        RUNNING\t     hadoop2:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1024, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:8732 MB, VMem:8732 MB, VCores:0.29666665\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n",
      "   hadoop3:38769\t        RUNNING\t     hadoop3:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1024, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:8732 MB, VMem:8732 MB, VCores:0.35988003\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "yarn node -list -showDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.18.0.2' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Restart nodemanager\n",
    "ssh hadoop1 /opt/hadoop/bin/yarn --daemon start nodemanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
