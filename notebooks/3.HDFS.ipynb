{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - Web Interface\n",
    "\n",
    "- Master node\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "- Worker node\n",
    "    - hadoop1\n",
    "        - DataNode: http://localhost:9864\n",
    "    - hadoop2\n",
    "        - DataNode: http://localhost:9865\n",
    "    - hadoop3\n",
    "        - DataNode: http://localhost:9866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "cacheadmin           configure the HDFS cache\n",
      "crypto               configure HDFS encryption zones\n",
      "debug                run a Debug Admin to execute HDFS debug commands\n",
      "dfsadmin             run a DFS admin client\n",
      "dfsrouteradmin       manage Router-based federation\n",
      "ec                   run a HDFS ErasureCoding CLI\n",
      "fsck                 run a DFS filesystem checking utility\n",
      "haadmin              run a DFS HA admin client\n",
      "jmxget               get JMX exported values from NameNode or DataNode.\n",
      "oev                  apply the offline edits viewer to an edits file\n",
      "oiv                  apply the offline fsimage viewer to an fsimage\n",
      "oiv_legacy           apply the offline fsimage viewer to a legacy fsimage\n",
      "storagepolicies      list/get/set/satisfyStoragePolicy block storage policies\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "dfs                  run a filesystem command on the file system\n",
      "envvars              display computed Hadoop environment variables\n",
      "fetchdt              fetch a delegation token from the NameNode\n",
      "getconf              get config values from configuration\n",
      "groups               get the groups which users belong to\n",
      "lsSnapshottableDir   list all snapshottable dirs owned by the current user\n",
      "snapshotDiff         diff two snapshots of a directory or diff the current\n",
      "                     directory contents with a snapshot\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "balancer             run a cluster balancing utility\n",
      "datanode             run a DFS datanode\n",
      "dfsrouter            run the DFS router\n",
      "diskbalancer         Distributes data evenly among disks on a given node\n",
      "journalnode          run the DFS journalnode\n",
      "mover                run a utility to move block replicas across storage types\n",
      "namenode             run the DFS namenode\n",
      "nfs3                 run an NFS version 3 gateway\n",
      "portmap              run a portmap service\n",
      "secondarynamenode    run the DFS secondary namenode\n",
      "sps                  run external storagepolicysatisfier\n",
      "zkfc                 run the ZK Failover Controller daemon\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "hdfs help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem Basic Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "Download books from Gutenberg project (http://www.gutenberg.org/)\n",
    "\n",
    "- Moby Dick; Or, The Whale by Herman Melville\n",
    "- Pride and Prejudice by Jane Austen\n",
    "- Dracula by Bram Stoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dracula.txt\n",
      "mobydick.txt\n",
      "prideandprejudice.txt\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "mkdir /opt/datasets\n",
    "cd /opt/datasets\n",
    "\n",
    "wget -qc http://www.gutenberg.org/files/2701/2701-0.txt -O mobydick.txt\n",
    "wget -qc http://www.gutenberg.org/files/1342/1342-0.txt -O prideandprejudice.txt\n",
    "wget -qc http://www.gutenberg.org/cache/epub/345/pg345.txt -O dracula.txt\n",
    "\n",
    "ls /opt/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dracula.txt regular file 890355 2\n",
      "mobydick.txt regular file 1276235 2\n",
      "prideandprejudice.txt regular file 772186 2\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "# create gutenberg folder in HDFS\n",
    "# hdfs dfs -mkdir /user/hadoop/gutenberg\n",
    "\n",
    "# copy books to HDFS\n",
    "# hdfs dfs -put * /user/hadoop/gutenberg\n",
    "# hdfs dfs -copyFromLocal * /user/hadoop/gutenberg\n",
    "\n",
    "# list files in HDFS\n",
    "# hdfs dfs -ls /user/hadoop/gutenberg\n",
    "\n",
    "# show first KB of file\n",
    "# hdfs dfs -head /user/hadoop/gutenberg/mobydick.txt\n",
    "\n",
    "# show last KB of file\n",
    "# hdfs dfs -tail /user/hadoop/gutenberg/prideandprejudice.txt\n",
    "\n",
    "# show whole file - CAREFUL\n",
    "# hdfs dfs -cat /user/hadoop/gutenberg/dracula.txt\n",
    "\n",
    "# append file contents to a file in HDFS\n",
    "# hdfs dfs -appendToFile mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/allbooks.txt\n",
    "\n",
    "# copy allbooks.txt (in HDFS) to gutenberg directory (in HDFS)\n",
    "# hdfs dfs -cp allbooks.txt /user/hadoop/gutenberg\n",
    "# hdfs dfs -ls -h -R\n",
    "\n",
    "# retrieve allbooks.txt from HDFS\n",
    "# hdfs dfs -get allbooks.txt .\n",
    "# hdfs dfs -copyToLocal /user/hadoop/allbooks.txt .\n",
    "\n",
    "# remove file\n",
    "# hdfs dfs -rm allbooks.txt\n",
    "# hdfs dfs -rm /user/hadoop/allbooks.txt\n",
    "\n",
    "# mv file (also used for renaming)\n",
    "# hdfs dfs -mv gutenberg/allbooks.txt gutenberg/books.txt\n",
    "\n",
    "# print statistics on folder\n",
    "# printf \"name\\ttype\\tsize\\treps\\n\"\n",
    "hdfs dfs -stat \"%n %F %b %r\" /user/hadoop/gutenberg/*\n",
    "\n",
    "# getmerge\n",
    "# hdfs dfs -getmerge /user/hadoop/gutenberg mergebooks.txt\n",
    "\n",
    "# remove directory and files (-R recursive)\n",
    "# hdfs dfs -rm -R /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilization in a MapReduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/hadoop/gutenberg': File exists\n",
      "put: `/user/hadoop/gutenberg/mobydick.txt': File exists\n",
      "put: `/user/hadoop/gutenberg/prideandprejudice.txt': File exists\n",
      "put: `/user/hadoop/gutenberg/dracula.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/gutenberg\n",
    "hdfs dfs -put mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-30 16:41:01,703 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:41:01,800 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-11-30 16:41:01,975 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701371922987_0006\n",
      "2023-11-30 16:41:02,231 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2023-11-30 16:41:02,342 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2023-11-30 16:41:02,477 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701371922987_0006\n",
      "2023-11-30 16:41:02,477 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-11-30 16:41:02,639 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-11-30 16:41:02,640 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-11-30 16:41:02,686 INFO impl.YarnClientImpl: Submitted application application_1701371922987_0006\n",
      "2023-11-30 16:41:02,717 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701371922987_0006/\n",
      "2023-11-30 16:41:02,718 INFO mapreduce.Job: Running job: job_1701371922987_0006\n",
      "2023-11-30 16:41:08,784 INFO mapreduce.Job: Job job_1701371922987_0006 running in uber mode : false\n",
      "2023-11-30 16:41:08,812 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-11-30 16:41:14,921 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2023-11-30 16:41:15,936 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-11-30 16:41:20,954 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-11-30 16:41:20,959 INFO mapreduce.Job: Job job_1701371922987_0006 completed successfully\n",
      "2023-11-30 16:41:21,027 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1001370\n",
      "\t\tFILE: Number of bytes written=3108449\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2939138\n",
      "\t\tHDFS: Number of bytes written=572618\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=28880\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6388\n",
      "\t\tTotal time spent by all map tasks (ms)=14440\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3194\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14440\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3194\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3696640\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=817664\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=53071\n",
      "\t\tMap output records=510618\n",
      "\t\tMap output bytes=4905696\n",
      "\t\tMap output materialized bytes=1001382\n",
      "\t\tInput split bytes=362\n",
      "\t\tCombine input records=510618\n",
      "\t\tCombine output records=67251\n",
      "\t\tReduce input groups=50416\n",
      "\t\tReduce shuffle bytes=1001382\n",
      "\t\tReduce input records=67251\n",
      "\t\tReduce output records=50416\n",
      "\t\tSpilled Records=134502\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=392\n",
      "\t\tCPU time spent (ms)=6180\n",
      "\t\tPhysical memory (bytes) snapshot=1079025664\n",
      "\t\tVirtual memory (bytes) snapshot=7512870912\n",
      "\t\tTotal committed heap usage (bytes)=682622976\n",
      "\t\tPeak Map Physical memory (bytes)=298995712\n",
      "\t\tPeak Map Virtual memory (bytes)=1878503424\n",
      "\t\tPeak Reduce Physical memory (bytes)=190287872\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1882144768\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2938776\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=572618\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar wordcount \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2023-11-30 16:41 /user/hadoop/gutenberg-output/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop     572618 2023-11-30 16:41 /user/hadoop/gutenberg-output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# list output folder contents\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Defects,\"\t2\n",
      "\"Information\t2\n",
      "\"Plain\t4\n",
      "\"Project\t10\n",
      "\"Right\t2\n",
      "#1342]\t1\n",
      "#2701]\t1\n",
      "#345]\t1\n",
      "$20,000,000!\t1\n",
      "$5,000)\t3\n",
      "$7,000,000.\t1\n",
      "&\t24\n",
      "'AS-IS',\t2\n",
      "(\"the\t2\n",
      "($1\t3\n",
      "(1775)\t1\n",
      "(801)\t3\n",
      "(Ahab’s)\t1\n",
      "(Albatross)\t1\n",
      "(American)\t1\n",
      "(Bunger,\t1\n",
      "(By\t1\n",
      "(Entered\t1\n",
      "(Fife)\t1\n",
      "(Greenland\t1\n",
      "(I\t2\n",
      "(It\t1\n",
      "(Kept\t1\n",
      "(Lady\t1\n",
      "(Not\t1\n",
      "(Pig-fish\t1\n",
      "(Pull,\t1\n",
      "(Sent\t1\n",
      "(Sperm\t3\n",
      "(Spiders\t1\n",
      "(Spring,\t1\n",
      "(Steelkilt)\t1\n",
      "(Steelkilt’s)\t1\n",
      "(Strong,\t1\n",
      "(Supplied\t3\n",
      "(Terra\t1\n",
      "(This\t1\n",
      "(Unopened\t2\n",
      "(Why\t1\n",
      "(_A\t1\n",
      "(_Advancing_.)\t1\n",
      "(_Ahab\t3\n",
      "(_Algerine\t1\n",
      "(_As\t1\n",
      "(_Ascending,\t1\n",
      "(_Aside_)\t1\n",
      "(_Aside_.)\t1\n",
      "(_Black\t1\n",
      "(_Carpenter\t1\n",
      "(_Confidence_)\t1\n",
      "(_Dancing_)\t1\n",
      "(_Duodecimo_),\t3\n",
      "(_Duodecimo_).\t1\n",
      "(_During\t1\n",
      "(_Enter\t1\n",
      "(_Fin-Back_).—Under\t1\n",
      "(_Folio_)\t1\n",
      "(_Folio_),\t6\n",
      "(_Foresail\t1\n",
      "(_Grampus_).—Though\t1\n",
      "(_Hump\t1\n",
      "(_Huzza\t1\n",
      "(_Kept\t1\n",
      "(_Killer_).—Of\t1\n",
      "(_Leaps\t1\n",
      "(_Mealy-mouthed\t1\n",
      "(_Mem._,\t6\n",
      "(_Narwhale_),\t1\n",
      "(_Nudging_.)\t1\n",
      "(_Octavo_),\t6\n",
      "(_Octavo_).\t1\n",
      "(_Pasted\t1\n",
      "(_Quietly\t1\n",
      "(_Razor\t1\n",
      "(_Reclining\t2\n",
      "(_Reclining_.)\t1\n",
      "(_Right\t1\n",
      "(_Sings,\t1\n",
      "(_Sperm\t1\n",
      "(_Stubb\t1\n",
      "(_Sulky\t1\n",
      "(_Sulphur\t1\n",
      "(_The\t1\n",
      "(_They\t2\n",
      "(_"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "# show head\n",
    "hdfs dfs -head /user/hadoop/gutenberg-output/part-r-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Defects,\"\t2\n",
      "\"Information\t2\n",
      "\"Plain\t4\n",
      "\"Project\t10\n",
      "\"Right\t2\n",
      "#1342]\t1\n",
      "#2701]\t1\n",
      "#345]\t1\n",
      "$20,000,000!\t1\n",
      "$5,000)\t3\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /tmp\n",
    "\n",
    "# copy HDFS file to local filesystem\n",
    "hdfs dfs -get /user/hadoop/gutenberg-output/part-r-00000 gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/hadoop/gutenberg-output\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# remove folder on HDFS\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-30 16:42:44,757 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-11-30 16:42:44,860 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-11-30 16:42:45,025 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701371922987_0007\n",
      "2023-11-30 16:42:45,326 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2023-11-30 16:42:45,436 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2023-11-30 16:42:45,574 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701371922987_0007\n",
      "2023-11-30 16:42:45,575 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-11-30 16:42:45,709 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-11-30 16:42:45,709 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-11-30 16:42:45,751 INFO impl.YarnClientImpl: Submitted application application_1701371922987_0007\n",
      "2023-11-30 16:42:45,774 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701371922987_0007/\n",
      "2023-11-30 16:42:45,774 INFO mapreduce.Job: Running job: job_1701371922987_0007\n",
      "2023-11-30 16:42:50,902 INFO mapreduce.Job: Job job_1701371922987_0007 running in uber mode : false\n",
      "2023-11-30 16:42:50,903 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-11-30 16:42:58,008 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2023-11-30 16:42:59,012 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-11-30 16:43:03,036 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-11-30 16:43:04,045 INFO mapreduce.Job: Job job_1701371922987_0007 completed successfully\n",
      "2023-11-30 16:43:04,107 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1001376\n",
      "\t\tFILE: Number of bytes written=3384923\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2939138\n",
      "\t\tHDFS: Number of bytes written=572618\n",
      "\t\tHDFS: Number of read operations=19\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=30282\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12182\n",
      "\t\tTotal time spent by all map tasks (ms)=15141\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6091\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15141\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6091\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3876096\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1559296\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=53071\n",
      "\t\tMap output records=510618\n",
      "\t\tMap output bytes=4905696\n",
      "\t\tMap output materialized bytes=1001400\n",
      "\t\tInput split bytes=362\n",
      "\t\tCombine input records=510618\n",
      "\t\tCombine output records=67251\n",
      "\t\tReduce input groups=50416\n",
      "\t\tReduce shuffle bytes=1001400\n",
      "\t\tReduce input records=67251\n",
      "\t\tReduce output records=50416\n",
      "\t\tSpilled Records=134502\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=433\n",
      "\t\tCPU time spent (ms)=7530\n",
      "\t\tPhysical memory (bytes) snapshot=1268195328\n",
      "\t\tVirtual memory (bytes) snapshot=9391898624\n",
      "\t\tTotal committed heap usage (bytes)=780664832\n",
      "\t\tPeak Map Physical memory (bytes)=298156032\n",
      "\t\tPeak Map Virtual memory (bytes)=1877164032\n",
      "\t\tPeak Reduce Physical memory (bytes)=189825024\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1881329664\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2938776\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=572618\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application with 2 reducers\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar wordcount \\\n",
    "-Dmapreduce.job.reduces=2 \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2023-11-30 16:43 /user/hadoop/gutenberg-output/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop     285660 2023-11-30 16:43 /user/hadoop/gutenberg-output/part-r-00000\n",
      "-rw-r--r--   2 hadoop hadoop     286958 2023-11-30 16:43 /user/hadoop/gutenberg-output/part-r-00001\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# list output folder contents\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Project\t10\n",
      "$20,000,000!\t1\n",
      "$7,000,000.\t1\n",
      "'AS-IS',\t2\n",
      "(\"the\t2\n",
      "($1\t3\n",
      "(1775)\t1\n",
      "(Ahab’s)\t1\n",
      "(American)\t1\n",
      "(Bunger,\t1\n",
      "Deleted /user/hadoop/gutenberg-output\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /tmp\n",
    "\n",
    "# copy HDFS file to local filesystem\n",
    "hdfs dfs -getmerge /user/hadoop/gutenberg-output gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt\n",
    "\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Commands\n",
    "\n",
    "- https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify HDFS cluster status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rack: /default-rack\n",
      "   172.18.0.4:9866 (hadoop2.docker_hadoopnet) In Service\n",
      "   172.18.0.2:9866 (hadoop1.docker_hadoopnet) In Service\n",
      "   172.18.0.3:9866 (hadoop3.docker_hadoopnet) In Service\n",
      "\n",
      "\n",
      "========================================\n",
      "\n",
      "Configured Capacity: 100908072960 (93.98 GB)\n",
      "Present Capacity: 24622888724 (22.93 GB)\n",
      "DFS Remaining: 23685783552 (22.06 GB)\n",
      "DFS Used: 937105172 (893.69 MB)\n",
      "DFS Used%: 3.81%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.18.0.2:9866 (hadoop1.docker_hadoopnet)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 158695065 (151.34 MB)\n",
      "Non DFS Used: 23847309671 (22.21 GB)\n",
      "DFS Remaining: 7895261184 (7.35 GB)\n",
      "DFS Used%: 0.47%\n",
      "DFS Remaining%: 23.47%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Thu Nov 30 16:43:51 BRT 2023\n",
      "Last Block Report: Thu Nov 30 16:18:33 BRT 2023\n",
      "Num of Blocks: 28\n",
      "\n",
      "\n",
      "Name: 172.18.0.3:9866 (hadoop3.docker_hadoopnet)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 464646107 (443.12 MB)\n",
      "Non DFS Used: 23541358629 (21.92 GB)\n",
      "DFS Remaining: 7895261184 (7.35 GB)\n",
      "DFS Used%: 1.38%\n",
      "DFS Remaining%: 23.47%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Thu Nov 30 16:43:51 BRT 2023\n",
      "Last Block Report: Thu Nov 30 16:18:33 BRT 2023\n",
      "Num of Blocks: 33\n",
      "\n",
      "\n",
      "Name: 172.18.0.4:9866 (hadoop2.docker_hadoopnet)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 313764000 (299.23 MB)\n",
      "Non DFS Used: 23692240736 (22.07 GB)\n",
      "DFS Remaining: 7895261184 (7.35 GB)\n",
      "DFS Used%: 0.93%\n",
      "DFS Remaining%: 23.47%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Thu Nov 30 16:43:51 BRT 2023\n",
      "Last Block Report: Thu Nov 30 16:18:33 BRT 2023\n",
      "Num of Blocks: 37\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# print topology\n",
    "hdfs dfsadmin -printTopology\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" |tr \" \" \"=\"\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://hadoop:9870/fsck?ugi=hadoop&files=1&blocks=1&locations=1&path=%2Fuser%2Fhadoop%2Fgutenberg\n",
      "FSCK started by hadoop (auth:SIMPLE) from /172.18.0.5 for path /user/hadoop/gutenberg at Thu Nov 30 16:44:25 BRT 2023\n",
      "\n",
      "/user/hadoop/gutenberg <dir>\n",
      "/user/hadoop/gutenberg/dracula.txt 890355 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-1253975299-172.18.0.5-1701371903365:blk_1073741909_1085 len=890355 Live_repl=2  [DatanodeInfoWithStorage[172.18.0.4:9866,DS-53b865e7-f9b3-4d65-8bf8-c6df9e92dd27,DISK], DatanodeInfoWithStorage[172.18.0.3:9866,DS-3afdbdfe-2ce1-4c1c-8bf9-b25389e6ebc9,DISK]]\n",
      "\n",
      "/user/hadoop/gutenberg/mobydick.txt 1276235 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-1253975299-172.18.0.5-1701371903365:blk_1073741910_1086 len=1276235 Live_repl=2  [DatanodeInfoWithStorage[172.18.0.2:9866,DS-5077a3a4-0e1b-40f0-875f-ff1a597a1b1b,DISK], DatanodeInfoWithStorage[172.18.0.4:9866,DS-53b865e7-f9b3-4d65-8bf8-c6df9e92dd27,DISK]]\n",
      "\n",
      "/user/hadoop/gutenberg/prideandprejudice.txt 772186 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-1253975299-172.18.0.5-1701371903365:blk_1073741911_1087 len=772186 Live_repl=2  [DatanodeInfoWithStorage[172.18.0.4:9866,DS-53b865e7-f9b3-4d65-8bf8-c6df9e92dd27,DISK], DatanodeInfoWithStorage[172.18.0.2:9866,DS-5077a3a4-0e1b-40f0-875f-ff1a597a1b1b,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t3\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t1\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t2938776 B\n",
      " Total files:\t3\n",
      " Total blocks (validated):\t3 (avg. block size 979592 B)\n",
      " Minimally replicated blocks:\t3 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t2.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Thu Nov 30 16:44:25 BRT 2023 in 6 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/hadoop/gutenberg' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# list folder block location\n",
    "hdfs fsck /user/hadoop/gutenberg -files -blocks -locations\n",
    "\n",
    "# change replication factor of all files in directory to 3\n",
    "# hdfs dfs -setrep 3 /user/hadoop/gutenberg\n",
    "\n",
    "# list folder block location\n",
    "# hdfs fsck /user/hadoop/gutenberg -files -blocks -locations\n",
    "\n",
    "# change replication factor back to 2\n",
    "# hdfs dfs -setrep 2 /user/hadoop/gutenberg\n",
    "\n",
    "# list folder block location\n",
    "# hdfs fsck /user/hadoop/gutenberg -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomission nodes\n",
    "\n",
    "- dfs.hosts.exclude in hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Decomissioning hadoop1\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "hadoop1\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# report HDFS status\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Recomission all nodes\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# report HDFS status\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling datanode failures\n",
    "\n",
    "- timeouts defined in hdfs-site.xml \n",
    "    - dfs.namenode.heartbeat.recheck-interval = 10000 (10 seconds)\n",
    "    - dfs.heartbeat.interval = 3 seconds\n",
    "- timeout = 2 x recheck-interval + 10 x heartbeat.interval\n",
    "    - timeout = 50 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# get dfs.namenode.heartbeat.recheck-interval\n",
    "hdfs getconf -confKey dfs.namenode.heartbeat.recheck-interval\n",
    "\n",
    "# get dfs.heartbeat.interval\n",
    "hdfs getconf -confKey dfs.heartbeat.interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# simulate node fault\n",
    "ssh hadoop1 'kill -9 $(cat /tmp/hadoop-hadoop-datanode.pid)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Restart nodemanager\n",
    "ssh hadoop1 /opt/hadoop/bin/hdfs --daemon start datanode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
