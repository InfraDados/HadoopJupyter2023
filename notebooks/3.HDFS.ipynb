{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - Web Interface\n",
    "\n",
    "- Master node\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "- Worker node\n",
    "    - hadoop1\n",
    "        - DataNode: http://localhost:9864\n",
    "    - hadoop2\n",
    "        - DataNode: http://localhost:9865\n",
    "    - hadoop3\n",
    "        - DataNode: http://localhost:9866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "cacheadmin           configure the HDFS cache\n",
      "crypto               configure HDFS encryption zones\n",
      "debug                run a Debug Admin to execute HDFS debug commands\n",
      "dfsadmin             run a DFS admin client\n",
      "dfsrouteradmin       manage Router-based federation\n",
      "ec                   run a HDFS ErasureCoding CLI\n",
      "fsck                 run a DFS filesystem checking utility\n",
      "haadmin              run a DFS HA admin client\n",
      "jmxget               get JMX exported values from NameNode or DataNode.\n",
      "oev                  apply the offline edits viewer to an edits file\n",
      "oiv                  apply the offline fsimage viewer to an fsimage\n",
      "oiv_legacy           apply the offline fsimage viewer to a legacy fsimage\n",
      "storagepolicies      list/get/set/satisfyStoragePolicy block storage policies\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "dfs                  run a filesystem command on the file system\n",
      "envvars              display computed Hadoop environment variables\n",
      "fetchdt              fetch a delegation token from the NameNode\n",
      "getconf              get config values from configuration\n",
      "groups               get the groups which users belong to\n",
      "lsSnapshottableDir   list all snapshottable dirs owned by the current user\n",
      "snapshotDiff         diff two snapshots of a directory or diff the current\n",
      "                     directory contents with a snapshot\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "tput: No value for $TERM and no -T specified\n",
      "balancer             run a cluster balancing utility\n",
      "datanode             run a DFS datanode\n",
      "dfsrouter            run the DFS router\n",
      "diskbalancer         Distributes data evenly among disks on a given node\n",
      "journalnode          run the DFS journalnode\n",
      "mover                run a utility to move block replicas across storage types\n",
      "namenode             run the DFS namenode\n",
      "nfs3                 run an NFS version 3 gateway\n",
      "portmap              run a portmap service\n",
      "secondarynamenode    run the DFS secondary namenode\n",
      "sps                  run external storagepolicysatisfier\n",
      "zkfc                 run the ZK Failover Controller daemon\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "hdfs help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Administration Commands\n",
    "\n",
    "- https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify HDFS cluster status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rack: /default-rack\n",
      "   172.19.0.2:9866 (hadoop2.docker_hadoopnet) In Service\n",
      "   172.19.0.4:9866 (hadoop1.docker_hadoopnet) In Service\n",
      "   172.19.0.3:9866 (hadoop3.docker_hadoopnet) In Service\n",
      "\n",
      "\n",
      "========================================\n",
      "\n",
      "Configured Capacity: 100908072960 (93.98 GB)\n",
      "Present Capacity: 43334434816 (40.36 GB)\n",
      "DFS Remaining: 43334361088 (40.36 GB)\n",
      "DFS Used: 73728 (72 KB)\n",
      "DFS Used%: 0.00%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.19.0.2:9866 (hadoop2.docker_hadoopnet)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "Non DFS Used: 17456451584 (16.26 GB)\n",
      "DFS Remaining: 14444789760 (13.45 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 42.94%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:23:50 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:21:23 BRT 2023\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n",
      "Name: 172.19.0.3:9866 (hadoop3.docker_hadoopnet)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "Non DFS Used: 17456455680 (16.26 GB)\n",
      "DFS Remaining: 14444785664 (13.45 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 42.94%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:23:50 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:20:35 BRT 2023\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n",
      "Name: 172.19.0.4:9866 (hadoop1.docker_hadoopnet)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "Non DFS Used: 17456455680 (16.26 GB)\n",
      "DFS Remaining: 14444785664 (13.45 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 42.94%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:23:50 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:20:35 BRT 2023\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# print topology\n",
    "hdfs dfsadmin -printTopology\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" |tr \" \" \"=\"\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run randomtext application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 10:24:13,338 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.19.0.5:8032\n",
      "2023-12-04 10:24:13,521 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.19.0.5:10200\n",
      "Running 3 maps.\n",
      "Job started: Mon Dec 04 10:24:14 BRT 2023\n",
      "2023-12-04 10:24:14,014 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.19.0.5:8032\n",
      "2023-12-04 10:24:14,014 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.19.0.5:10200\n",
      "2023-12-04 10:24:14,177 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701696043891_0001\n",
      "2023-12-04 10:24:15,204 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2023-12-04 10:24:15,356 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701696043891_0001\n",
      "2023-12-04 10:24:15,356 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-12-04 10:24:15,496 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-12-04 10:24:15,497 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-12-04 10:24:15,869 INFO impl.YarnClientImpl: Submitted application application_1701696043891_0001\n",
      "2023-12-04 10:24:15,900 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701696043891_0001/\n",
      "2023-12-04 10:24:15,901 INFO mapreduce.Job: Running job: job_1701696043891_0001\n",
      "2023-12-04 10:24:22,000 INFO mapreduce.Job: Job job_1701696043891_0001 running in uber mode : false\n",
      "2023-12-04 10:24:22,001 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-12-04 10:24:30,229 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2023-12-04 10:24:31,233 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-12-04 10:24:32,241 INFO mapreduce.Job: Job job_1701696043891_0001 completed successfully\n",
      "2023-12-04 10:24:32,307 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=828729\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=360\n",
      "\t\tHDFS: Number of bytes written=157766621\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tOther local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36442\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=18221\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18221\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4664576\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=239763\n",
      "\t\tInput split bytes=360\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=494\n",
      "\t\tCPU time spent (ms)=4390\n",
      "\t\tPhysical memory (bytes) snapshot=570875904\n",
      "\t\tVirtual memory (bytes) snapshot=5638848512\n",
      "\t\tTotal committed heap usage (bytes)=271056896\n",
      "\t\tPeak Map Physical memory (bytes)=192499712\n",
      "\t\tPeak Map Virtual memory (bytes)=1880588288\n",
      "\torg.apache.hadoop.examples.RandomTextWriter$Counters\n",
      "\t\tBYTES_WRITTEN=157287095\n",
      "\t\tRECORDS_WRITTEN=239763\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=157766621\n",
      "Job ended: Mon Dec 04 10:24:32 BRT 2023\n",
      "The job took 18 seconds.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# deletes randomtext folder if it exists\n",
    "hdfs dfs -test -d /user/hadoop/randomtext && hdfs dfs -rm -r /user/hadoop/randomtext\n",
    "\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar randomtextwriter \\\n",
    "  -D mapreduce.randomtextwriter.totalbytes=157286400 \\\n",
    "  -D mapreduce.randomtextwriter.bytespermap=52428800 \\\n",
    "  -D mapreduce.output.fileoutputformat.compress=false \\\n",
    "  -outFormat org.apache.hadoop.mapreduce.lib.output.TextOutputFormat \\\n",
    "  randomtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List folder block location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://hadoop:9870/fsck?ugi=hadoop&files=1&blocks=1&locations=1&path=%2Fuser%2Fhadoop%2Frandomtext\n",
      "FSCK started by hadoop (auth:SIMPLE) from /172.19.0.5 for path /user/hadoop/randomtext at Mon Dec 04 10:24:45 BRT 2023\n",
      "\n",
      "/user/hadoop/randomtext <dir>\n",
      "/user/hadoop/randomtext/_SUCCESS 0 bytes, replicated: replication=2, 0 block(s):  OK\n",
      "\n",
      "/user/hadoop/randomtext/part-m-00000 52589372 bytes, replicated: replication=2, 2 block(s):  OK\n",
      "0. BP-1060887769-172.19.0.5-1701696025450:blk_1073741833_1009 len=33554432 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK]]\n",
      "1. BP-1060887769-172.19.0.5-1701696025450:blk_1073741835_1011 len=19034940 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK]]\n",
      "\n",
      "/user/hadoop/randomtext/part-m-00001 52588584 bytes, replicated: replication=2, 2 block(s):  OK\n",
      "0. BP-1060887769-172.19.0.5-1701696025450:blk_1073741834_1010 len=33554432 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK], DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK]]\n",
      "1. BP-1060887769-172.19.0.5-1701696025450:blk_1073741836_1012 len=19034152 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK], DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK]]\n",
      "\n",
      "/user/hadoop/randomtext/part-m-00002 52588665 bytes, replicated: replication=2, 2 block(s):  OK\n",
      "0. BP-1060887769-172.19.0.5-1701696025450:blk_1073741830_1006 len=33554432 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK], DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK]]\n",
      "1. BP-1060887769-172.19.0.5-1701696025450:blk_1073741831_1007 len=19034233 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t3\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t1\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t157766621 B\n",
      " Total files:\t4\n",
      " Total blocks (validated):\t6 (avg. block size 26294436 B)\n",
      " Minimally replicated blocks:\t6 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t2.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Mon Dec 04 10:24:45 BRT 2023 in 9 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/hadoop/randomtext' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs fsck /user/hadoop/randomtext -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change replication factor of all files in directory to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication 3 set: /user/hadoop/randomtext/_SUCCESS\n",
      "Replication 3 set: /user/hadoop/randomtext/part-m-00000\n",
      "Replication 3 set: /user/hadoop/randomtext/part-m-00001\n",
      "Replication 3 set: /user/hadoop/randomtext/part-m-00002\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -setrep 3 /user/hadoop/randomtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List folder block location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://hadoop:9870/fsck?ugi=hadoop&files=1&blocks=1&locations=1&path=%2Fuser%2Fhadoop%2Frandomtext\n",
      "FSCK started by hadoop (auth:SIMPLE) from /172.19.0.5 for path /user/hadoop/randomtext at Mon Dec 04 10:25:39 BRT 2023\n",
      "\n",
      "/user/hadoop/randomtext <dir>\n",
      "/user/hadoop/randomtext/_SUCCESS 0 bytes, replicated: replication=3, 0 block(s):  OK\n",
      "\n",
      "/user/hadoop/randomtext/part-m-00000 52589372 bytes, replicated: replication=3, 2 block(s):  OK\n",
      "0. BP-1060887769-172.19.0.5-1701696025450:blk_1073741833_1009 len=33554432 Live_repl=3  [DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK], DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK]]\n",
      "1. BP-1060887769-172.19.0.5-1701696025450:blk_1073741835_1011 len=19034940 Live_repl=3  [DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK], DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK]]\n",
      "\n",
      "/user/hadoop/randomtext/part-m-00001 52588584 bytes, replicated: replication=3, 2 block(s):  OK\n",
      "0. BP-1060887769-172.19.0.5-1701696025450:blk_1073741834_1010 len=33554432 Live_repl=3  [DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK], DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK]]\n",
      "1. BP-1060887769-172.19.0.5-1701696025450:blk_1073741836_1012 len=19034152 Live_repl=3  [DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK], DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK]]\n",
      "\n",
      "/user/hadoop/randomtext/part-m-00002 52588665 bytes, replicated: replication=3, 2 block(s):  OK\n",
      "0. BP-1060887769-172.19.0.5-1701696025450:blk_1073741830_1006 len=33554432 Live_repl=3  [DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK], DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK]]\n",
      "1. BP-1060887769-172.19.0.5-1701696025450:blk_1073741831_1007 len=19034233 Live_repl=3  [DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK], DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t3\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t1\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t157766621 B\n",
      " Total files:\t4\n",
      " Total blocks (validated):\t6 (avg. block size 26294436 B)\n",
      " Minimally replicated blocks:\t6 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t3.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Mon Dec 04 10:25:39 BRT 2023 in 2 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/hadoop/randomtext' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs fsck /user/hadoop/randomtext -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change replication factor back to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication 2 set: /user/hadoop/randomtext/_SUCCESS\n",
      "Replication 2 set: /user/hadoop/randomtext/part-m-00000\n",
      "Replication 2 set: /user/hadoop/randomtext/part-m-00001\n",
      "Replication 2 set: /user/hadoop/randomtext/part-m-00002\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -setrep 2 /user/hadoop/randomtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List folder block location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://hadoop:9870/fsck?ugi=hadoop&files=1&blocks=1&locations=1&path=%2Fuser%2Fhadoop%2Frandomtext\n",
      "FSCK started by hadoop (auth:SIMPLE) from /172.19.0.5 for path /user/hadoop/randomtext at Mon Dec 04 10:26:27 BRT 2023\n",
      "\n",
      "/user/hadoop/randomtext <dir>\n",
      "/user/hadoop/randomtext/_SUCCESS 0 bytes, replicated: replication=2, 0 block(s):  OK\n",
      "\n",
      "/user/hadoop/randomtext/part-m-00000 52589372 bytes, replicated: replication=2, 2 block(s):  OK\n",
      "0. BP-1060887769-172.19.0.5-1701696025450:blk_1073741833_1009 len=33554432 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK]]\n",
      "1. BP-1060887769-172.19.0.5-1701696025450:blk_1073741835_1011 len=19034940 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK]]\n",
      "\n",
      "/user/hadoop/randomtext/part-m-00001 52588584 bytes, replicated: replication=2, 2 block(s):  OK\n",
      "0. BP-1060887769-172.19.0.5-1701696025450:blk_1073741834_1010 len=33554432 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK], DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK]]\n",
      "1. BP-1060887769-172.19.0.5-1701696025450:blk_1073741836_1012 len=19034152 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK], DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK]]\n",
      "\n",
      "/user/hadoop/randomtext/part-m-00002 52588665 bytes, replicated: replication=2, 2 block(s):  OK\n",
      "0. BP-1060887769-172.19.0.5-1701696025450:blk_1073741830_1006 len=33554432 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK], DatanodeInfoWithStorage[172.19.0.2:9866,DS-dd925961-0131-4e0a-acdb-9557d8dd9e52,DISK]]\n",
      "1. BP-1060887769-172.19.0.5-1701696025450:blk_1073741831_1007 len=19034233 Live_repl=2  [DatanodeInfoWithStorage[172.19.0.3:9866,DS-9f58d647-656a-43c3-a8cf-2db68edaa6cf,DISK], DatanodeInfoWithStorage[172.19.0.4:9866,DS-1e865565-6bed-481f-ae3b-d9f778884cf4,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t3\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t1\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t157766621 B\n",
      " Total files:\t4\n",
      " Total blocks (validated):\t6 (avg. block size 26294436 B)\n",
      " Minimally replicated blocks:\t6 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t2.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Mon Dec 04 10:26:27 BRT 2023 in 3 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/hadoop/randomtext' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs fsck /user/hadoop/randomtext -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomission nodes\n",
    "\n",
    "- dfs.hosts.exclude in hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh nodes successful\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Decomissioning hadoop1\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "hadoop1\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Namenode:** http://localhost:9870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report HDFS Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 67272048640 (62.65 GB)\n",
      "Present Capacity: 28248800032 (26.31 GB)\n",
      "DFS Remaining: 27929780224 (26.01 GB)\n",
      "DFS Used: 319019808 (304.24 MB)\n",
      "DFS Used%: 1.13%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.19.0.2:9866 (hadoop2.docker_hadoopnet)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 159509904 (152.12 MB)\n",
      "Non DFS Used: 17776865904 (16.56 GB)\n",
      "DFS Remaining: 13964890112 (13.01 GB)\n",
      "DFS Used%: 0.47%\n",
      "DFS Remaining%: 41.52%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:27:53 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:21:23 BRT 2023\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n",
      "Name: 172.19.0.3:9866 (hadoop3.docker_hadoopnet)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 159509904 (152.12 MB)\n",
      "Non DFS Used: 17776865904 (16.56 GB)\n",
      "DFS Remaining: 13964890112 (13.01 GB)\n",
      "DFS Used%: 0.47%\n",
      "DFS Remaining%: 41.52%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:27:53 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:20:35 BRT 2023\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n",
      "Name: 172.19.0.4:9866 (hadoop1.docker_hadoopnet)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Decommissioned\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 159509904 (152.12 MB)\n",
      "Non DFS Used: 17776865904 (16.56 GB)\n",
      "DFS Remaining: 13964890112 (13.01 GB)\n",
      "DFS Used%: 0.47%\n",
      "DFS Remaining%: 41.52%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:27:53 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:20:35 BRT 2023\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recomission all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh nodes successful\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report HDFS status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 100908072960 (93.98 GB)\n",
      "Present Capacity: 42691582752 (39.76 GB)\n",
      "DFS Remaining: 42372538368 (39.46 GB)\n",
      "DFS Used: 319044384 (304.26 MB)\n",
      "DFS Used%: 0.75%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.19.0.2:9866 (hadoop2.docker_hadoopnet)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 106197705 (101.28 MB)\n",
      "Non DFS Used: 17670888759 (16.46 GB)\n",
      "DFS Remaining: 14124179456 (13.15 GB)\n",
      "DFS Used%: 0.32%\n",
      "DFS Remaining%: 41.99%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:28:50 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:21:23 BRT 2023\n",
      "Num of Blocks: 7\n",
      "\n",
      "\n",
      "Name: 172.19.0.3:9866 (hadoop3.docker_hadoopnet)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 140290333 (133.79 MB)\n",
      "Non DFS Used: 17636796131 (16.43 GB)\n",
      "DFS Remaining: 14124179456 (13.15 GB)\n",
      "DFS Used%: 0.42%\n",
      "DFS Remaining%: 41.99%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:28:50 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:20:35 BRT 2023\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n",
      "Name: 172.19.0.4:9866 (hadoop1.docker_hadoopnet)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 72556346 (69.20 MB)\n",
      "Non DFS Used: 17704530118 (16.49 GB)\n",
      "DFS Remaining: 14124179456 (13.15 GB)\n",
      "DFS Used%: 0.22%\n",
      "DFS Remaining%: 41.99%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:28:50 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:20:35 BRT 2023\n",
      "Num of Blocks: 6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling datanode failures\n",
    "\n",
    "- timeouts defined in hdfs-site.xml \n",
    "    - dfs.namenode.heartbeat.recheck-interval = 10000 (10 seconds)\n",
    "    - dfs.heartbeat.interval = 3 seconds\n",
    "- timeout = 2 x recheck-interval + 10 x heartbeat.interval\n",
    "    - timeout = 50 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# get dfs.namenode.heartbeat.recheck-interval\n",
    "hdfs getconf -confKey dfs.namenode.heartbeat.recheck-interval\n",
    "\n",
    "# get dfs.heartbeat.interval\n",
    "hdfs getconf -confKey dfs.heartbeat.interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate node fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.19.0.4' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "ssh hadoop1 'kill -9 $(cat /tmp/hadoop-hadoop-datanode.pid)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Namenode:** http://localhost:9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 67272048640 (62.65 GB)\n",
      "Present Capacity: 28494179061 (26.54 GB)\n",
      "DFS Remaining: 28247654400 (26.31 GB)\n",
      "DFS Used: 246524661 (235.10 MB)\n",
      "DFS Used%: 0.87%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 2\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 2\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (2):\n",
      "\n",
      "Name: 172.19.0.2:9866 (hadoop2.docker_hadoopnet)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 106197705 (101.28 MB)\n",
      "Non DFS Used: 17671220535 (16.46 GB)\n",
      "DFS Remaining: 14123847680 (13.15 GB)\n",
      "DFS Used%: 0.32%\n",
      "DFS Remaining%: 41.99%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:30:26 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:21:23 BRT 2023\n",
      "Num of Blocks: 9\n",
      "\n",
      "\n",
      "Name: 172.19.0.3:9866 (hadoop3.docker_hadoopnet)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 140326956 (133.83 MB)\n",
      "Non DFS Used: 17637132244 (16.43 GB)\n",
      "DFS Remaining: 14123806720 (13.15 GB)\n",
      "DFS Used%: 0.42%\n",
      "DFS Remaining%: 41.99%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:30:26 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:20:35 BRT 2023\n",
      "Num of Blocks: 10\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.19.0.4:9866 (hadoop1.docker_hadoopnet)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 72556346 (69.20 MB)\n",
      "Non DFS Used: 17704710342 (16.49 GB)\n",
      "DFS Remaining: 14123999232 (13.15 GB)\n",
      "DFS Used%: 0.22%\n",
      "DFS Remaining%: 41.99%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:29:20 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:20:35 BRT 2023\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart nodemanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.19.0.4' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "ssh hadoop1 /opt/hadoop/bin/hdfs --daemon start datanode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refresh nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh nodes successful\n",
      "Configured Capacity: 100908072960 (93.98 GB)\n",
      "Present Capacity: 42689764752 (39.76 GB)\n",
      "DFS Remaining: 42298081280 (39.39 GB)\n",
      "DFS Used: 391683472 (373.54 MB)\n",
      "DFS Used%: 0.92%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.19.0.2:9866 (hadoop2.docker_hadoopnet)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 159576064 (152.18 MB)\n",
      "Non DFS Used: 17618132992 (16.41 GB)\n",
      "DFS Remaining: 14123556864 (13.15 GB)\n",
      "DFS Used%: 0.47%\n",
      "DFS Remaining%: 41.99%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:30:50 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:21:23 BRT 2023\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n",
      "Name: 172.19.0.3:9866 (hadoop3.docker_hadoopnet)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 159509904 (152.12 MB)\n",
      "Non DFS Used: 17618199152 (16.41 GB)\n",
      "DFS Remaining: 14123556864 (13.15 GB)\n",
      "DFS Used%: 0.47%\n",
      "DFS Remaining%: 41.99%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:30:50 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:20:35 BRT 2023\n",
      "Num of Blocks: 11\n",
      "\n",
      "\n",
      "Name: 172.19.0.4:9866 (hadoop1.docker_hadoopnet)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 33636024320 (31.33 GB)\n",
      "DFS Used: 72597504 (69.23 MB)\n",
      "Non DFS Used: 17777700864 (16.56 GB)\n",
      "DFS Remaining: 14050967552 (13.09 GB)\n",
      "DFS Used%: 0.22%\n",
      "DFS Remaining%: 41.77%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Dec 04 10:30:49 BRT 2023\n",
      "Last Block Report: Mon Dec 04 10:30:43 BRT 2023\n",
      "Num of Blocks: 6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfsadmin -refreshNodes\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
