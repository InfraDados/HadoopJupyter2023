{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - Web Interface\n",
    "\n",
    "- Master node\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "- Worker node\n",
    "    - hadoop1\n",
    "        - DataNode: http://localhost:9864\n",
    "    - hadoop2\n",
    "        - DataNode: http://localhost:9865\n",
    "    - hadoop3\n",
    "        - DataNode: http://localhost:9866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem Basic Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "Download books from Gutenberg project (http://www.gutenberg.org/)\n",
    "\n",
    "- Moby Dick; Or, The Whale by Herman Melville\n",
    "- Pride and Prejudice by Jane Austen\n",
    "- Dracula by Bram Stoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "mkdir /opt/datasets\n",
    "cd /opt/datasets\n",
    "\n",
    "wget -qc http://www.gutenberg.org/files/2701/2701-0.txt -O mobydick.txt\n",
    "wget -qc http://www.gutenberg.org/files/1342/1342-0.txt -O prideandprejudice.txt\n",
    "wget -qc http://www.gutenberg.org/cache/epub/345/pg345.txt -O dracula.txt\n",
    "\n",
    "ls /opt/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create gutenberg folder in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy books to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -put * /user/hadoop/gutenberg\n",
    "# hdfs dfs -copyFromLocal * /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -ls /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show first/last KB of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "# hdfs dfs -head /user/hadoop/gutenberg/mobydick.txt\n",
    "hdfs dfs -tail /user/hadoop/gutenberg/prideandprejudice.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show whole file - CAREFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -cat /user/hadoop/gutenberg/dracula.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append file contents to a file in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -appendToFile mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/allbooks.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy allbooks.txt (in HDFS) to gutenberg directory (in HDFS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "#hdfs dfs -cp allbooks.txt /user/hadoop/gutenberg\n",
    "hdfs dfs -ls -h -R /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy allbooks.txt to local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -get allbooks.txt .\n",
    "# hdfs dfs -copyToLocal /user/hadoop/allbooks.txt .\n",
    "ls -l allbooks.txt\n",
    "rm allbooks.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove file in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -rm allbooks.txt\n",
    "# hdfs dfs -rm /user/hadoop/allbooks.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move file in HDFS (also used for renaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -mv gutenberg/allbooks.txt gutenberg/books.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print statistics on folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "printf \"name\\ttype\\tsize\\treps\\n\"\n",
    "hdfs dfs -stat \"%n %F %b %r\" /user/hadoop/gutenberg/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get several files from HDFS and merge to a single local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -getmerge /user/hadoop/gutenberg mergebooks.txt\n",
    "ls -l mergebooks.txt\n",
    "rm mergebooks.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove directory and files (-R recursive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "# hdfs dfs -rm -R /user/hadoop/gutenberg\n",
    "hdfs dfs -ls /user/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilization in a MapReduce job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy files to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/gutenberg\n",
    "hdfs dfs -put mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MapReduce application specifying HDFS folders for input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar wordcount \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output\n",
    "hdfs dfs -head /user/hadoop/gutenberg-output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy HDFS files to local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /tmp\n",
    "\n",
    "hdfs dfs -get /user/hadoop/gutenberg-output/part-r-00000 gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove output folder on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running MapReduce with 2 reduce tasks (-Dmapreduce.job.reduces=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application with 2 reducers\n",
    "hadoop jar ./hadoop-mapreduce-examples-$HADOOP_VERSION.jar wordcount \\\n",
    "-Dmapreduce.job.reduces=2 \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List output folder contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy HDFS file to local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /tmp\n",
    "\n",
    "hdfs dfs -getmerge /user/hadoop/gutenberg-output gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt\n",
    "\n",
    "# remove output folder\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebHDFS\n",
    "\n",
    "- hdfs-site.xml\n",
    "    dfs.webhdfs.enabled = true\n",
    "\n",
    "https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LISTSTATUS** - List a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "curl -s http://localhost:9870/webhdfs/v1/tmp?op=LISTSTATUS | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE** - Create and Write to a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "# -L option -> follows redirects\n",
    "curl -s -X PUT -T mobydick.txt -L \"http://localhost:9870/webhdfs/v1/tmp/mobydick.txt?op=CREATE&user.name=hadoop\"\n",
    "curl -s \"http://localhost:9870/webhdfs/v1/tmp?op=LISTSTATUS\" | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GETFILESTATUS** - Status of a File/Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "curl -s \"http://localhost:9870/webhdfs/v1/tmp/mobydick.txt?op=GETFILESTATUS\" | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GETFILEBLOCKLOCATIONS** - Get File Block Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "curl -s \"http://localhost:9870/webhdfs/v1/tmp/mobydick.txt?op=GETFILEBLOCKLOCATIONS\" | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DELETE** - Delete a File/Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "curl -s -X DELETE \"http://localhost:9870/webhdfs/v1/tmp/mobydick.txt?op=DELETE\"\n",
    "curl -s \"http://localhost:9870/webhdfs/v1/tmp?op=LISTSTATUS\" | python3 -m json.tool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
