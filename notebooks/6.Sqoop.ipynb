{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sqoop\n",
    "![Sqoop](https://sqoop.apache.org/images/sqoop-logo.png)\n",
    "\n",
    "- https://sqoop.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- download from https://archive.apache.org/dist/sqoop/1.4.7/\n",
    "- version 1.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
      "export PDSH_RCMD_TYPE=ssh\n",
      "export HADOOP_HOME=/opt/hadoop\n",
      "export HADOOP_VERSION=3.3.6\n",
      "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
      "export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
      "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
      "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
      "export HADOOP_YARN_HOME=${HADOOP_HOME}\n",
      "export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin\n",
      "# Sqoop\n",
      "export SQOOP_HOME=/opt/sqoop\n",
      "export PATH=${PATH}:${SQOOP_HOME}/bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Download package\n",
    "mkdir /opt/pkgs\n",
    "cd /opt/pkgs\n",
    "# wget -q -c https://downloads.apache.org/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz\n",
    "wget -q -c http://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz\n",
    "    \n",
    "# unpack file and create link\n",
    "tar -zxf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /opt\n",
    "ln -s /opt/sqoop-1.4.7.bin__hadoop-2.6.0 /opt/sqoop\n",
    "\n",
    "# update commons-lang\n",
    "rm /opt/sqoop/lib/commons-lang3-3.4.jar\n",
    "cp /opt/hadoop/share/hadoop/yarn/timelineservice/lib/commons-lang-2.6.jar /opt/sqoop/lib\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Sqoop\n",
    "export SQOOP_HOME=/opt/sqoop\n",
    "export PATH=\\${PATH}:\\${SQOOP_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mysql-connector\n",
    "\n",
    "- https://dev.mysql.com/downloads/connector/j/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package mysql-connector-j.\n",
      "(Reading database ... 42423 files and directories currently installed.)\n",
      "Preparing to unpack mysql-connector-j_8.0.33-1ubuntu20.04_all.deb ...\n",
      "Unpacking mysql-connector-j (8.0.33-1ubuntu20.04) ...\n",
      "Setting up mysql-connector-j (8.0.33-1ubuntu20.04) ...\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# Download package\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-j_8.0.33-1ubuntu20.04_all.deb\n",
    "    \n",
    "sudo dpkg -i mysql-connector-j_8.0.33-1ubuntu20.04_all.deb\n",
    "\n",
    "cp /usr/share/java/mysql-connector-java-8.0.33.jar /opt/sqoop/lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mysql installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Stopping MySQL database server mysqld\n",
      "   ...done.\n",
      " * Starting MySQL database server mysqld\n",
      "su: warning: cannot change directory to /nonexistent: No such file or directory\n",
      "   ...done.\n",
      " * /usr/bin/mysqladmin  Ver 8.0.35-0ubuntu0.20.04.1 for Linux on x86_64 ((Ubuntu))\n",
      "Copyright (c) 2000, 2023, Oracle and/or its affiliates.\n",
      "\n",
      "Oracle is a registered trademark of Oracle Corporation and/or its\n",
      "affiliates. Other names may be trademarks of their respective\n",
      "owners.\n",
      "\n",
      "Server version\t\t8.0.35-0ubuntu0.20.04.1\n",
      "Protocol version\t10\n",
      "Connection\t\tLocalhost via UNIX socket\n",
      "UNIX socket\t\t/var/run/mysqld/mysqld.sock\n",
      "Uptime:\t\t\t2 sec\n",
      "\n",
      "Threads: 2  Questions: 8  Slow queries: 0  Opens: 119  Flush tables: 3  Open tables: 38  Queries per second avg: 4.000\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# sudo apt update\n",
    "# sudo apt install -qq -y mysql-server unzip >> /tmp/install.log 2>&1\n",
    "\n",
    "# Enable external access (from worker nodes)\n",
    "sudo sed -i \"s/^bind-address/#bind-address/g\" /etc/mysql/mysql.conf.d/mysqld.cnf \n",
    "\n",
    "sudo service mysql restart\n",
    "sudo service mysql status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# create hadoop user\n",
    "sudo mysql -e \"create user 'hadoop'@'%' IDENTIFIED BY ''\"\n",
    "sudo mysql -e \"grant all privileges on *.* to 'hadoop'@'%' WITH GRANT OPTION\"\n",
    "sudo mysql -e \"flush privileges\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employees database setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# Download EmployeesDB sample database\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://github.com/datacharmer/test_db/archive/master.zip\n",
    "\n",
    "unzip master.zip\n",
    "\n",
    "cd test_db-master\n",
    "\n",
    "mysql -u hadoop -h hadoop < employees.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database\n",
      "employees\n",
      "information_schema\n",
      "mysql\n",
      "performance_schema\n",
      "sys\n",
      "\n",
      "========================================\n",
      "\n",
      "Tables_in_employees\n",
      "current_dept_emp\n",
      "departments\n",
      "dept_emp\n",
      "dept_emp_latest_date\n",
      "dept_manager\n",
      "employees\n",
      "salaries\n",
      "titles\n",
      "\n",
      "========================================\n",
      "\n",
      "Field\tType\tNull\tKey\tDefault\tExtra\n",
      "emp_no\tint\tNO\tPRI\tNULL\t\n",
      "birth_date\tdate\tNO\t\tNULL\t\n",
      "first_name\tvarchar(14)\tNO\t\tNULL\t\n",
      "last_name\tvarchar(16)\tNO\t\tNULL\t\n",
      "gender\tenum('M','F')\tNO\t\tNULL\t\n",
      "hire_date\tdate\tNO\t\tNULL\t\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "mysql -u hadoop -h hadoop -e 'show databases'\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" | tr ' ' '='\n",
    "\n",
    "mysql -u hadoop -h hadoop -D employees -e 'show tables'\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" | tr ' ' '='\n",
    "\n",
    "mysql -u hadoop -h hadoop -D employees -e 'describe employees'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sqoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/sqoop/../hbase does not exist! HBase imports will fail.\n",
      "Please set $HBASE_HOME to the root of your HBase installation.\n",
      "Warning: /opt/sqoop/../hcatalog does not exist! HCatalog jobs will fail.\n",
      "Please set $HCAT_HOME to the root of your HCatalog installation.\n",
      "Warning: /opt/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "Warning: /opt/sqoop/../zookeeper does not exist! Accumulo imports will fail.\n",
      "Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.\n",
      "2023-12-06 14:34:05,760 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7\n",
      "2023-12-06 14:34:05,884 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "mysql\n",
      "information_schema\n",
      "performance_schema\n",
      "sys\n",
      "employees\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "sqoop list-databases --connect jdbc:mysql://hadoop --username hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/sqoop/../hbase does not exist! HBase imports will fail.\n",
      "Please set $HBASE_HOME to the root of your HBase installation.\n",
      "Warning: /opt/sqoop/../hcatalog does not exist! HCatalog jobs will fail.\n",
      "Please set $HCAT_HOME to the root of your HCatalog installation.\n",
      "Warning: /opt/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "Warning: /opt/sqoop/../zookeeper does not exist! Accumulo imports will fail.\n",
      "Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.\n",
      "current_dept_emp\n",
      "departments\n",
      "dept_emp\n",
      "dept_emp_latest_date\n",
      "dept_manager\n",
      "employees\n",
      "salaries\n",
      "titles\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "sqoop list-tables --connect jdbc:mysql://hadoop/employees --username hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/sqoop/../hbase does not exist! HBase imports will fail.\n",
      "Please set $HBASE_HOME to the root of your HBase installation.\n",
      "Warning: /opt/sqoop/../hcatalog does not exist! HCatalog jobs will fail.\n",
      "Please set $HCAT_HOME to the root of your HCatalog installation.\n",
      "Warning: /opt/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "Warning: /opt/sqoop/../zookeeper does not exist! Accumulo imports will fail.\n",
      "Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.\n",
      "2023-12-06 14:38:42,416 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7\n",
      "2023-12-06 14:38:42,524 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "2023-12-06 14:38:42,524 INFO tool.CodeGenTool: Beginning code generation\n",
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "2023-12-06 14:38:42,997 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `employees` AS t LIMIT 1\n",
      "2023-12-06 14:38:43,040 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `employees` AS t LIMIT 1\n",
      "2023-12-06 14:38:43,056 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop\n",
      "Note: /tmp/sqoop-hadoop/compile/bb5e8218cfc62f251ced40b8ce7891d2/employees.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "2023-12-06 14:38:49,317 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/bb5e8218cfc62f251ced40b8ce7891d2/employees.jar\n",
      "2023-12-06 14:38:49,342 WARN manager.MySQLManager: It looks like you are importing from mysql.\n",
      "2023-12-06 14:38:49,342 WARN manager.MySQLManager: This transfer can be faster! Use the --direct\n",
      "2023-12-06 14:38:49,342 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.\n",
      "2023-12-06 14:38:49,342 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)\n",
      "2023-12-06 14:38:49,348 INFO mapreduce.ImportJobBase: Beginning import of employees\n",
      "2023-12-06 14:38:49,349 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2023-12-06 14:38:49,467 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "2023-12-06 14:38:49,951 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2023-12-06 14:38:50,242 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop/172.18.0.5:8032\n",
      "2023-12-06 14:38:50,487 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.18.0.5:10200\n",
      "2023-12-06 14:38:50,953 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1701881222785_0001\n",
      "2023-12-06 14:38:54,866 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "2023-12-06 14:38:54,866 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`emp_no`), MAX(`emp_no`) FROM `employees`\n",
      "2023-12-06 14:38:54,869 INFO db.IntegerSplitter: Split size: 122499; Num splits: 4 from: 10001 to: 499999\n",
      "2023-12-06 14:38:54,976 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "2023-12-06 14:38:55,328 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1701881222785_0001\n",
      "2023-12-06 14:38:55,328 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-12-06 14:38:55,527 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-12-06 14:38:55,527 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-12-06 14:38:56,089 INFO impl.YarnClientImpl: Submitted application application_1701881222785_0001\n",
      "2023-12-06 14:38:56,169 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1701881222785_0001/\n",
      "2023-12-06 14:38:56,169 INFO mapreduce.Job: Running job: job_1701881222785_0001\n",
      "2023-12-06 14:39:04,250 INFO mapreduce.Job: Job job_1701881222785_0001 running in uber mode : false\n",
      "2023-12-06 14:39:04,251 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-12-06 14:39:14,357 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2023-12-06 14:39:18,381 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2023-12-06 14:39:19,384 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-12-06 14:39:19,390 INFO mapreduce.Job: Job job_1701881222785_0001 completed successfully\n",
      "2023-12-06 14:39:19,456 INFO mapreduce.Job: Counters: 34\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=1139296\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=464\n",
      "\t\tHDFS: Number of bytes written=13821993\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=86260\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=43130\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=43130\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11041280\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=300024\n",
      "\t\tMap output records=300024\n",
      "\t\tInput split bytes=464\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=1479\n",
      "\t\tCPU time spent (ms)=18340\n",
      "\t\tPhysical memory (bytes) snapshot=953106432\n",
      "\t\tVirtual memory (bytes) snapshot=7608328192\n",
      "\t\tTotal committed heap usage (bytes)=390070272\n",
      "\t\tPeak Map Physical memory (bytes)=249802752\n",
      "\t\tPeak Map Virtual memory (bytes)=1909407744\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13821993\n",
      "2023-12-06 14:39:19,461 INFO mapreduce.ImportJobBase: Transferred 13.1817 MB in 29.4993 seconds (457.571 KB/sec)\n",
      "2023-12-06 14:39:19,464 INFO mapreduce.ImportJobBase: Retrieved 300024 records.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "sqoop import --connect jdbc:mysql://hadoop/employees --username hadoop --table employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2023-12-06 14:39 employees/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop      4.3 M 2023-12-06 14:39 employees/part-m-00000\n",
      "-rw-r--r--   2 hadoop hadoop      2.4 M 2023-12-06 14:39 employees/part-m-00001\n",
      "-rw-r--r--   2 hadoop hadoop      2.0 M 2023-12-06 14:39 employees/part-m-00002\n",
      "-rw-r--r--   2 hadoop hadoop      4.4 M 2023-12-06 14:39 employees/part-m-00003\n",
      "1953-09-02,10001,Georgi,M,1986-06-26,Facello\n",
      "1964-06-02,10002,Bezalel,F,1985-11-21,Simmel\n",
      "1959-12-03,10003,Parto,M,1986-08-28,Bamford\n",
      "1954-05-01,10004,Chirstian,M,1986-12-01,Koblick\n",
      "1955-01-21,10005,Kyoichi,M,1989-09-12,Maliniak\n",
      "1953-04-20,10006,Anneke,F,1989-06-02,Preusig\n",
      "1957-05-23,10007,Tzvetan,F,1989-02-10,Zielinski\n",
      "1958-02-19,10008,Saniya,M,1994-09-15,Kalloufi\n",
      "1952-04-19,10009,Sumant,F,1985-02-18,Peac\n",
      "1963-06-01,10010,Duangkaew,F,1989-08-24,Piveteau\n",
      "1953-11-07,10011,Mary,F,1990-01-22,Sluis\n",
      "1960-10-04,10012,Patricio,M,1992-12-18,Bridgland\n",
      "1963-06-07,10013,Eberhardt,M,1985-10-20,Terkki\n",
      "1956-02-12,10014,Berni,M,1987-03-11,Genin\n",
      "1959-08-19,10015,Guoxiang,M,1987-07-02,Nooteboom\n",
      "1961-05-02,10016,Kazuhito,M,1995-01-27,Cappelletti\n",
      "1958-07-06,10017,Cristinel,F,1993-08-03,Bouloucos\n",
      "1954-06-19,10018,Kazuhide,F,1987-04-03,Peha\n",
      "1953-01-23,10019,Lillian,M,1999-04-30,Haddadi\n",
      "1952-12-24,10020,Mayuko,M,1991-01-26,Warwick\n",
      "1960-02-20,10021,Ramzi,M,1988-02-10,Erde\n",
      "1952-07-08,10022,Shahaf,M,1995-08-22,Famili\n",
      "1953-09-29,10023"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -ls -h employees\n",
    "\n",
    "hdfs dfs -head employees/part-m-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Stopping MySQL database server mysqld\n",
      "   ...done.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "rm employees.java\n",
    "\n",
    "# Stopping mysql\n",
    "sudo service mysql stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
