{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka\n",
    "<img src=\"https://kafka.apache.org/images/apache-kafka.png\" alt=\"Kafka\" width=\"150\"/>\n",
    "\n",
    "- https://kafka.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- download from https://dlcdn.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz\n",
    "- version 3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Download package\n",
    "mkdir /opt/pkgs\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://dlcdn.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz\n",
    "    \n",
    "# unpack file and create link\n",
    "tar -zxf kafka_2.13-3.6.0.tgz -C /opt\n",
    "ln -s /opt/kafka_2.13-3.6.0 /opt/kafka\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Kafka\n",
    "export KAFKA_HOME=/opt/kafka\n",
    "export PATH=\\${PATH}:\\${KAFKA_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Kafka with ZooKeeper\n",
    "\n",
    "# Start ZooKeeper service\n",
    "zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties > /tmp/zookeeper.output 2>&1 &\n",
    "echo $! > /tmp/zookeeper.pid\n",
    "\n",
    "ps -fp $(cat /tmp/zookeeper.pid)\n",
    "\n",
    "# Start Kafka broker service\n",
    "kafka-server-start.sh $KAFKA_HOME/config/server.properties > /tmp/kafka-server.output 2>&1 &\n",
    "echo $! > /tmp/kafka-server.pid\n",
    "\n",
    "ps -fp $(cat /tmp/kafka-server.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Create a topic to store events\n",
    "\n",
    "kafka-topics.sh --create --topic mytopic --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --topic mytopic --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Write events to topic\n",
    "echo \"event 1\" | kafka-console-producer.sh --topic mytopic --bootstrap-server localhost:9092\n",
    "echo \"event 2\" | kafka-console-producer.sh --topic mytopic --bootstrap-server localhost:9092\n",
    "echo \"event 3\" | kafka-console-producer.sh --topic mytopic --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Read events from topic\n",
    "# --from-beginning - starts consuming from the beginning of the topic's partition, otherwise, starts consuming from most recent messages\n",
    "# --timeout-ms - specifies the duration in miliseconds the consumer runs waiting for events\n",
    "kafka-console-consumer.sh --topic mytopic --from-beginning --timeout-ms 2000 --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Kafka and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd $KAFKA_HOME/libs\n",
    "\n",
    "# Download and install Kafka Connect HDFS\n",
    "# https://www.confluent.io/hub/#hdfs\n",
    "# https://www.confluent.io/hub/confluentinc/kafka-connect-hdfs3\n",
    "wget -qq -c https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-hdfs3/versions/1.1.27/confluentinc-kafka-connect-hdfs3-1.1.27.zip\n",
    "\n",
    "unzip -qq confluentinc-kafka-connect-hdfs3-1.1.27.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Create topic monitor\n",
    "kafka-topics.sh --create --topic monitor --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# https://docs.confluent.io/kafka-connectors/hdfs3-sink/current/overview.html\n",
    "# Create configuration file for sink\n",
    "cat <<EOF > $KAFKA_HOME/config/connect-hdfs.properties\n",
    "name=hdfs-sink\n",
    "bootstrap.servers=localhost:9092\n",
    "connector.class=io.confluent.connect.hdfs3.Hdfs3SinkConnector\n",
    "tasks.max=1\n",
    "topics=monitor\n",
    "hdfs.url=hdfs://hadoop:9000\n",
    "format.class=io.confluent.connect.hdfs3.avro.AvroFormat\n",
    "flush.size=3\n",
    "confluent.license=\n",
    "confluent.topic.bootstrap.servers=localhost:9092\n",
    "hadoop.conf.dir=/opt/hadoop/etc/hadoop\n",
    "hadoop.home=/opt/hadoop\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Create configuration file for kafka-connect\n",
    "cat <<EOF > $KAFKA_HOME/config/myconnect.properties\n",
    "bootstrap.servers=localhost:9092\n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "value.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "key.converter.schemas.enable=true\n",
    "value.converter.schemas.enable=true\n",
    "offset.storage.file.filename=/tmp/connect.offsets\n",
    "offset.flush.interval.ms=10000\n",
    "plugin.path=/opt/kafka/libs\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Start Connector\n",
    "connect-standalone.sh $KAFKA_HOME/config/myconnect.properties $KAFKA_HOME/config/connect-hdfs.properties > /tmp/connect-standalone.output 2>&1 &\n",
    "echo $! > /tmp/connect-standalone.pid\n",
    "\n",
    "ps -fp $(cat /tmp/connect-standalone.pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Create system monitor script\n",
    "cat > /tmp/monitor.sh << EOF\n",
    "while true\n",
    "do\n",
    "    DATE=\\$(date)\n",
    "    CPU=\\$(top -bn1 | grep 'Cpu(s)' | awk '{print \\$2 + \\$4}')\n",
    "    MEM=\\$(free -m | awk '/Mem:/ {print \\$3/\\$2 * 100.0}')\n",
    "    echo -n \"Date: \\$DATE - CPU Utilization: \\$CPU% - Memory Utilization: \\$MEM%\" | \\\n",
    "        kafka-console-producer.sh --topic monitor --bootstrap-server localhost:9092\n",
    "    sleep 3\n",
    "done\n",
    "EOF\n",
    "\n",
    "chmod +x /tmp/monitor.sh\n",
    "/tmp/monitor.sh > /dev/null 2>&1 &\n",
    "echo $! > /tmp/monitor.pid\n",
    "\n",
    "ps -fp $(cat /tmp/monitor.pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# List created files in HDFS\n",
    "hdfs dfs -ls -R /topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# Download Apache Avro\n",
    "# https://avro.apache.org/\n",
    "cd /opt/pkgs\n",
    "wget -qq -c https://dlcdn.apache.org/avro/stable/java/avro-tools-1.11.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "mkdir /tmp/monitor\n",
    "cd /tmp/monitor\n",
    "\n",
    "# Get files from HDFS\n",
    "hdfs dfs -get /topics/monitor/partition=0/* .\n",
    "ls *.avro\n",
    "\n",
    "# Print files content\n",
    "for FILE in $(ls -1 *.avro); do\n",
    "    echo $FILE\n",
    "    java -jar /opt/pkgs/avro-tools-1.11.3.jar tojson $FILE\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Kill services\n",
    "kill $(cat /tmp/monitor.pid)\n",
    "kill $(cat /tmp/connect-standalone.pid)\n",
    "\n",
    "kafka-server-stop.sh\n",
    "zookeeper-server-stop.sh\n",
    "\n",
    "ps -fp $(cat /tmp/monitor.pid)\n",
    "ps -fp $(cat /tmp/connect-standalone.pid)\n",
    "ps -fp $(cat /tmp/kafka-server.pid)\n",
    "ps -fp $(cat /tmp/zookeeper.pid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
