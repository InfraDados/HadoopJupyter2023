{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRJob\n",
    "\n",
    "- https://github.com/Yelp/mrjob\n",
    "- https://mrjob.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# local install\n",
    "pip3 install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "mkdir -p /opt/src/mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/mrwordcount.py\n",
    "\n",
    "import re\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            word = word.lower()\n",
    "            yield word,1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# download book \"The History of Don Quixote by Miguel de Cervantes\" from Gutenberg Project\n",
    "wget -q -c http://www.gutenberg.org/files/996/996-0.txt -O donquixote.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# inline runner (default)\n",
    "# python3 /opt/src/mrjob/mrwordcount.py donquixote.txt > donquixote-output.txt\n",
    "\n",
    "# local runner\n",
    "python3 /opt/src/mrjob/mrwordcount.py -r local donquixote.txt > donquixote-output.txt\n",
    "\n",
    "# head output\n",
    "head donquixote-output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# install in all hadoop nodes\n",
    "pdsh -w hadoop1,hadoop2,hadoop3 pip3 install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# create directory in HDFS and send file\n",
    "hdfs dfs -mkdir donquixote\n",
    "hdfs dfs -put donquixote.txt donquixote\n",
    "\n",
    "# run in hadoop\n",
    "python3 /opt/src/mrjob/mrwordcount.py -r hadoop --output-dir donquixote-output hdfs:///user/hadoop/donquixote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# get output\n",
    "hdfs dfs -getmerge donquixote-output donquixote-output.txt\n",
    "\n",
    "# head output\n",
    "head donquixote-output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "- weblog.csv\n",
    "- books from Gutenberg project\n",
    "- departments.csv and employees.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# copy datasets used by mrjob examples to hadoop container\n",
    "docker cp mrjobdataset.tgz hadoop:/opt/src/mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "# unpack dataset\n",
    "tar -zxf mrjobdataset.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/1_count_weblog.py\n",
    "\n",
    "#Total number of times each page is visited\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRURLCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        #Extract site\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        #Emit url and 1\n",
    "        yield url, 1\n",
    "\n",
    "    def reducer(self, key, list_of_values):\n",
    "        yield key,sum(list_of_values)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRURLCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 1_count_weblog.py weblog.csv 2> /dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/2_max_weblog.py\n",
    "\n",
    "# Return most visited URL\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRURLMax(MRJob) :\n",
    "\n",
    "    def mapper1(self, _, line) :\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        #Extract site\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        #Emit url and 1\n",
    "        yield url, 1\n",
    "\n",
    "    def reducer1(self, key, list_of_values) :\n",
    "        yield None, (sum(list_of_values), key)\n",
    "\n",
    "    def reducer2(self, key, list_of_values) :\n",
    "        yield max(list_of_values)\n",
    "\n",
    "    def steps(self) :\n",
    "        return [MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "        MRStep(reducer=self.reducer2)]\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    MRURLMax.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 2_max_weblog.py weblog.csv 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/3_average_weblog.py\n",
    "\n",
    "#Average visit time\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRAvgVisitTime(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = float(data[4].strip())\n",
    "\n",
    "        #Extract site\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        #Emit url and visit time\n",
    "        yield url, visit\n",
    "\n",
    "    def reducer(self, key, list_of_values):\n",
    "        count = 0\n",
    "        total = 0.0\n",
    "        for x in list_of_values:\n",
    "            total = total + x\n",
    "            count = count + 1\n",
    "\n",
    "        avglen = (\"%.2f\" % (total/count))\n",
    "        yield key,avglen\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRAvgVisitTime.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 3_average_weblog.py weblog.csv 2> /dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/4_topn_weblog.py\n",
    "\n",
    "#Top 3 visited pages\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRTopN(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        #Extract url\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        #Emit url and 1\n",
    "        yield url, 1\n",
    "\n",
    "    def reducer1(self, key, list_of_values):\n",
    "        total_count = sum(list_of_values)\n",
    "        yield None, (total_count, key)\n",
    "\n",
    "    def reducer2(self, _, list_of_values):\n",
    "        N=3\n",
    "        list_of_values = sorted(list(list_of_values), reverse=True)\n",
    "        return list_of_values[:N]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, reducer=self.reducer1),\n",
    "        MRStep(reducer=self.reducer2)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRTopN.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 4_topn_weblog.py weblog.csv 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/5_filter_weblog.py\n",
    "\n",
    "#Filter accesses to \"/login.php?value=fail\" on Feb/2018\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRFilter(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        #Extract site\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        #Extract month/year\n",
    "        date = time[4:12]\n",
    "\n",
    "        #Filter access to \"/login.php?value=fail\" on Feb/2018\n",
    "        if url == \"/login.php?value=fail\" and date == \"Feb/2018\" :\n",
    "            yield url, (time, ip, visit)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRFilter.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 5_filter_weblog.py weblog.csv 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/6_distinct_weblog.py\n",
    "\n",
    "#Distinct IPs\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRDistinct(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        yield ip, None\n",
    "\n",
    "    def reducer(self, key, list_of_values) :\n",
    "        yield key, None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRDistinct.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 6_distinct_weblog.py weblog.csv 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/7_binning_weblog.py\n",
    "\n",
    "#Create bins for different status codes for 20/Feb/2018\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRBinning(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "        \n",
    "        #Extract month/year\n",
    "        date = time[1:12]\n",
    "\n",
    "        #Filter accesses on 20/Feb/2018\n",
    "        if date == \"20/Feb/2018\" :\n",
    "            yield status, (time, request, ip)\n",
    "\n",
    "    def reducer(self, key, list_of_values):\n",
    "        yield key, (list(list_of_values))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRBinning.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 7_binning_weblog.py weblog.csv 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/8_invertedindex_books.py\n",
    "\n",
    "#Inverted Index\n",
    "from mrjob.job import MRJob\n",
    "import os\n",
    "\n",
    "class MRInvertedIndex(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        fileName = os.environ['mapreduce_map_input_file']\n",
    "\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            yield word, fileName\n",
    "\n",
    "    def reducer(self, key, list_of_values):\n",
    "        docs = set()\n",
    "        for x in list_of_values :\n",
    "            docs.add(x)\n",
    "        yield key,list(docs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRInvertedIndex.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 8_invertedindex_books.py books 2> /dev/null | head -n 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/9_sort_weblog.py\n",
    "\n",
    "# Sort visit times in descending order\n",
    "from mrjob.job import MRJob\n",
    "class MRSortVisit(MRJob) :\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        #Extract site\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        yield None, (visit, (time, url, ip))\n",
    "\n",
    "    def reducer(self, key, list_of_values):\n",
    "        l = [(float(v), content) for v, content in list_of_values]\n",
    "        l.sort(reverse=True)\n",
    "        return l\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRSortVisit.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 9_sort_weblog.py weblog.csv 2> /dev/null | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InnerJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/10_innerjoin_db.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import os\n",
    "\n",
    "class MRInnerJoin(MRJob) :\n",
    "    def mapper(self, _, line):\n",
    "        data = line.split(',')\n",
    "\n",
    "        filename = os.environ['mapreduce_map_input_file']\n",
    "\n",
    "        if 'employees.csv' in filename :\n",
    "            dep_no = data[2]\n",
    "            yield dep_no, ('Employee', data)\n",
    "        elif 'departments.csv' in filename:\n",
    "            dep_no = data[0]\n",
    "            yield dep_no, ('Department', data)\n",
    "\n",
    "    def reducer(self, key, list_of_values) :\n",
    "        values = list(list_of_values)\n",
    "        employees = []\n",
    "        departments = []\n",
    "        for v in values:\n",
    "            if v[0] == 'Employee' :\n",
    "                employees.append(v)\n",
    "            elif v[0] == 'Department' :\n",
    "                departments.append(v)\n",
    "\n",
    "        # Inner Join\n",
    "        for e in employees :\n",
    "            for d in departments :\n",
    "                yield key, (e+d)\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    MRInnerJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 10_innerjoin_db.py employees.csv departments.csv 2> /dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeftOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/11_leftouterjoin_db.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import os\n",
    "\n",
    "class MRLeftOuterJoin(MRJob) :\n",
    "    def mapper(self, _, line):\n",
    "        data = line.split(',')\n",
    "\n",
    "        filename = os.environ['mapreduce_map_input_file']\n",
    "\n",
    "        if 'employees.csv' in filename :\n",
    "            dep_no = data[2]\n",
    "            yield dep_no, ('Employee', data)\n",
    "        elif 'departments.csv' in filename:\n",
    "            dep_no = data[0]\n",
    "            yield dep_no, ('Department', data)\n",
    "\n",
    "    def reducer(self, key, list_of_values) :\n",
    "        # yield None, list(list_of_values)\n",
    "        values = list(list_of_values)\n",
    "        employees = []\n",
    "        departments = []\n",
    "        for v in values:\n",
    "            if v[0] == 'Employee' :\n",
    "                employees.append(v)\n",
    "            elif v[0] == 'Department' :\n",
    "                departments.append(v)\n",
    "\n",
    "        # Left Outer Join\n",
    "        for e in employees :\n",
    "            if len(departments) > 0 :\n",
    "                for d in departments :\n",
    "                    yield key, (e+d)\n",
    "            else :\n",
    "                yield key, (e)\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    MRLeftOuterJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 11_leftouterjoin_db.py employees.csv departments.csv 2> /dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RightOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/12_rightouterjoin_db.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import os\n",
    "\n",
    "class MRRightOuterJoin(MRJob) :\n",
    "    def mapper(self, _, line):\n",
    "        data = line.split(',')\n",
    "\n",
    "        filename = os.environ['mapreduce_map_input_file']\n",
    "        \n",
    "        if 'employees.csv' in filename :\n",
    "            dep_no = data[2]\n",
    "            yield dep_no, ('Employee', data)\n",
    "        elif 'departments.csv' in filename:\n",
    "            dep_no = data[0]\n",
    "            yield dep_no, ('Department', data)\n",
    "\n",
    "    def reducer(self, key, list_of_values) :\n",
    "        # yield None, list(list_of_values)\n",
    "        values = list(list_of_values)\n",
    "        employees = []\n",
    "        departments = []\n",
    "        for v in values:\n",
    "            if v[0] == 'Employee' :\n",
    "                employees.append(v)\n",
    "            elif v[0] == 'Department' :\n",
    "                departments.append(v)\n",
    "\n",
    "        # Right Outer Join\n",
    "        for d in departments :\n",
    "            if len(employees) > 0 :\n",
    "                for e in employees :\n",
    "                    yield key, (e+d)\n",
    "            else :\n",
    "                yield key, (d)\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    MRRightOuterJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 12_rightouterjoin_db.py employees.csv departments.csv 2> /dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FullOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerwrite hadoop /opt/src/mrjob/13_fullouterjoin_db.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import os\n",
    "\n",
    "class MRFullOuterJoin(MRJob) :\n",
    "    def mapper(self, _, line):\n",
    "        data = line.split(',')\n",
    "\n",
    "        filename = os.environ['mapreduce_map_input_file']\n",
    "\n",
    "        if 'employees.csv' in filename :\n",
    "            dep_no = data[2]\n",
    "            yield dep_no, ('Employee', data)\n",
    "        elif 'departments.csv' in filename:\n",
    "            dep_no = data[0]\n",
    "            yield dep_no, ('Department', data)\n",
    "\n",
    "    def reducer(self, key, list_of_values) :\n",
    "        values = list(list_of_values)\n",
    "        employees = []\n",
    "        departments = []\n",
    "        for v in values:\n",
    "            if v[0] == 'Employee' :\n",
    "                employees.append(v)\n",
    "            elif v[0] == 'Department' :\n",
    "                departments.append(v)\n",
    "\n",
    "        # Full Outer Join\n",
    "        if len(employees) > 0 :\n",
    "            for e in employees :\n",
    "                if len(departments) > 0 :\n",
    "                    for d in departments :\n",
    "                        yield key, (e+d)\n",
    "                else :\n",
    "                    yield key, (e)\n",
    "        else :\n",
    "            yield None, (d)\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    MRFullOuterJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "cd /opt/src/mrjob\n",
    "\n",
    "python3 13_fullouterjoin_db.py employees.csv departments.csv 2> /dev/null | head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
